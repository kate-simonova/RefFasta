{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kate-simonova/RefFasta/blob/main/03_baseline_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Yfm4EYcswVG"
      },
      "source": [
        "# Implementation of a baseline and autoencoder-based models\n",
        "\n",
        "The goal of these notebook is to implement  dimensionality reduction models that can be used for comparison with the main models a standard autoencoder and varia.\n",
        "\n",
        "**I decided to try some linear and non-linear models such as:**\n",
        "\n",
        "* no compression\n",
        "* principal component analysis (PCA)\n",
        "* kernel PCA (sigmoid and poly kernel)\n",
        "* UMAP\n",
        "* AE\n",
        "* VAE\n",
        "\n",
        "**Three metrics were selected for measuring the quality of the baseline model:**\n",
        "\n",
        "* Sihouette score - the better, the value is closer to 1\n",
        "* Davins-Bouldin  - the better, the value is closer to 0\n",
        "* Adjusted rand (ARI) index - the better, the value is closer to 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Data Loading and installation of packages"
      ],
      "metadata": {
        "id": "lsonb89Jfn0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn\n",
        "!pip install imblearn\n",
        "!pip install scikit-posthocs"
      ],
      "metadata": {
        "id": "RJBnqsCvRn_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVJ81ITZtoC5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "e6668396-934a-4d2e-f087-618bd32872a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8f6b81a5ea81>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshapiro\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfriedmanchisquare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscikit_posthocs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mumap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUMAP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scikit_posthocs'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# importing necessary packages\n",
        "#path = \"/home/simonova/merged/split_circle\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') \n",
        "path = \"/content/drive/MyDrive/MY_DATA/merged/split_circle\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from scipy.stats import shapiro, friedmanchisquare\n",
        "import scikit_posthocs as sp\n",
        "\n",
        "from umap import UMAP\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data "
      ],
      "metadata": {
        "id": "vjlncEi4hZW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same folder also contains the testing data under the name X_test_A_merged_CRC_BRCA.csv.tar.gz and y_test_A_merged_CRC_BRCA.csv.tar.gz."
      ],
      "metadata": {
        "id": "yog-XQRku6t-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(f\"{path}/X_train_A_merged_CRC_BRCA.csv.tar.gz\", compression=\"gzip\", index_col = 0).T \n",
        "train_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3d97pXzSk5h",
        "outputId": "e294c08f-e6bf-4dda-9e4e-407b0533021e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11544, 22596)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = pd.read_csv(f\"{path}/y_train_A_merged_CRC_BRCA.csv.tar.gz\", compression=\"gzip\", index_col = 0)\n",
        "y_train = y_train.set_index(\"Sample ID\")\n",
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-tlXn78q235",
        "outputId": "3dcd585d-6a7a-48da-83c8-c956aa833a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11544, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=5, shuffle=True, random_state=811)"
      ],
      "metadata": {
        "id": "Vbx4FIeyVevl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Silhouette score of data without compression (Baseline)"
      ],
      "metadata": {
        "id": "AhiQBz3hUFLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cross-validation for statistics\n",
        "scores_no_comp = []\n",
        "\n",
        "for train_index, val_index in kf.split(train_df.index):\n",
        "  X_val = train_df.iloc[val_index, :-1]\n",
        "  y_val = y_train.iloc[val_index, :]\n",
        "\n",
        "  km = KMeans(n_clusters=len(set(y_val[\"Label\"])), random_state=42)\n",
        "  y_pred = km.fit_predict(X_val)\n",
        "  ari = adjusted_rand_score(y_pred, y_val[\"Label\"])\n",
        "  silhouetteScore = silhouette_score(X_val, y_val[\"Label\"], metric=\"euclidean\")\n",
        "  davies_bouldinScore = davies_bouldin_score(X_val, y_val[\"Label\"])\n",
        "  print((silhouetteScore, ari, davies_bouldinScore))\n",
        "  scores_no_comp.append((silhouetteScore, ari, davies_bouldinScore))"
      ],
      "metadata": {
        "id": "Q0kDaQ4QvaUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating df and saving results\n",
        "compr_df = pd.DataFrame.from_records(scores_no_comp, columns=[\"Silhouette\", \"ARI\", \"DB score\"])\n",
        "compr_df[\"Method\"] = \"No compression\"\n",
        "compr_df.describe().loc[[\"mean\", \"std\"],]\n",
        "compr_df"
      ],
      "metadata": {
        "id": "khdXYZe7yHDh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "fae9ad1b-962d-4c50-c07f-aff990117e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Silhouette       ARI  DB score          Method\n",
              "0   -0.013697  0.135645  5.279731  No compression\n",
              "1   -0.011818  0.141451  5.299576  No compression\n",
              "2   -0.009440  0.128866  5.309149  No compression\n",
              "3   -0.011166  0.131910  5.404553  No compression\n",
              "4   -0.010869  0.118869  5.110581  No compression"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1badf344-15c4-48b4-89fb-392d1f4beeac\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Silhouette</th>\n",
              "      <th>ARI</th>\n",
              "      <th>DB score</th>\n",
              "      <th>Method</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.013697</td>\n",
              "      <td>0.135645</td>\n",
              "      <td>5.279731</td>\n",
              "      <td>No compression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.011818</td>\n",
              "      <td>0.141451</td>\n",
              "      <td>5.299576</td>\n",
              "      <td>No compression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.009440</td>\n",
              "      <td>0.128866</td>\n",
              "      <td>5.309149</td>\n",
              "      <td>No compression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.011166</td>\n",
              "      <td>0.131910</td>\n",
              "      <td>5.404553</td>\n",
              "      <td>No compression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.010869</td>\n",
              "      <td>0.118869</td>\n",
              "      <td>5.110581</td>\n",
              "      <td>No compression</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1badf344-15c4-48b4-89fb-392d1f4beeac')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1badf344-15c4-48b4-89fb-392d1f4beeac button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1badf344-15c4-48b4-89fb-392d1f4beeac');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Silhouette score of data with reduced dimensionality by PCA"
      ],
      "metadata": {
        "id": "GeeBC4wbxQXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As hyperparameters for PCA I can use n_components."
      ],
      "metadata": {
        "id": "k2jGZZ4H3cEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores_pca = []\n",
        "\n",
        "for train_index, val_index in kf.split(train_df.index):\n",
        "  X_val = train_df.iloc[val_index, :-1]\n",
        "  y_val = y_train.iloc[val_index, :]\n",
        "  X_train = train_df.iloc[train_index, :-1]\n",
        "  \n",
        "  pca = PCA(0.90).fit(X_train)\n",
        "  X_val = pca.transform(X_val)\n",
        "\n",
        "  km = KMeans(n_clusters=len(set(y_val[\"Label\"])), random_state=42)\n",
        "  y_pred = km.fit_predict(X_val)\n",
        "  ari = adjusted_rand_score(y_pred, y_val[\"Label\"])\n",
        "  silhouetteScore = silhouette_score(X_val, y_val[\"Label\"], metric=\"euclidean\")\n",
        "  davies_bouldinScore = davies_bouldin_score(X_val, y_val[\"Label\"])\n",
        "\n",
        "  scores_pca.append((silhouetteScore, ari, davies_bouldinScore))"
      ],
      "metadata": {
        "id": "6C2u4jYcxc7q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1614e50-1366-4878-c367-ba42f4a7d587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca_df = pd.DataFrame.from_records(scores_pca, columns=[\"Silhouette\", \"ARI\", \"DB score\"])\n",
        "pca_df[\"Method\"] = \"PCA\"\n",
        "pca_df.describe().loc[[\"mean\", \"std\"],]\n",
        "pca_df"
      ],
      "metadata": {
        "id": "14qOAYJYtjoH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "252641e6-8f2c-4504-c9b5-a979b6c39230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Silhouette       ARI  DB score Method\n",
              "0   -0.014522  0.128812  4.722823    PCA\n",
              "1   -0.010497  0.131232  4.756356    PCA\n",
              "2   -0.009263  0.123715  4.753019    PCA\n",
              "3   -0.011931  0.130932  4.827689    PCA\n",
              "4   -0.010046  0.117388  4.570650    PCA"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-233bb2e5-0ad6-4691-9f9f-e37d19dc0119\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Silhouette</th>\n",
              "      <th>ARI</th>\n",
              "      <th>DB score</th>\n",
              "      <th>Method</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.014522</td>\n",
              "      <td>0.128812</td>\n",
              "      <td>4.722823</td>\n",
              "      <td>PCA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.010497</td>\n",
              "      <td>0.131232</td>\n",
              "      <td>4.756356</td>\n",
              "      <td>PCA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.009263</td>\n",
              "      <td>0.123715</td>\n",
              "      <td>4.753019</td>\n",
              "      <td>PCA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.011931</td>\n",
              "      <td>0.130932</td>\n",
              "      <td>4.827689</td>\n",
              "      <td>PCA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.010046</td>\n",
              "      <td>0.117388</td>\n",
              "      <td>4.570650</td>\n",
              "      <td>PCA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-233bb2e5-0ad6-4691-9f9f-e37d19dc0119')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-233bb2e5-0ad6-4691-9f9f-e37d19dc0119 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-233bb2e5-0ad6-4691-9f9f-e37d19dc0119');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(pca.explained_variance_ratio_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DW6CG8a2AhSK",
        "outputId": "f0f89ec5-d2a8-443f-c59a-df52bb386898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Silhouette score of data with reduced dimensionality by KernelPCA (Poly kernel)"
      ],
      "metadata": {
        "id": "b3UU1oTyyZ-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As hyperparameters for kPCA I can use kernels and n_components."
      ],
      "metadata": {
        "id": "Ea4p6Rqq3150"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores_poly = []\n",
        "\n",
        "for train_index, val_index in kf.split(train_df.index):\n",
        "  X_val = train_df.iloc[val_index, :-1]\n",
        "  y_val = y_train.iloc[val_index, :]\n",
        "  X_train = train_df.iloc[train_index, :-1]\n",
        "  \n",
        "  pca = KernelPCA(n_components=2408, kernel=\"poly\").fit(X_train)\n",
        "  X_val = pca.transform(X_val)\n",
        "\n",
        "  km = KMeans(n_clusters=len(set(y_val[\"Label\"])), random_state=42)\n",
        "  y_pred = km.fit_predict(X_val)\n",
        "  ari = adjusted_rand_score(y_pred, y_val[\"Label\"])\n",
        "  silhouetteScore = silhouette_score(X_val, y_val[\"Label\"], metric=\"euclidean\")\n",
        "  davies_bouldinScore = davies_bouldin_score(X_val, y_val[\"Label\"])\n",
        "\n",
        "  scores_poly.append((silhouetteScore, ari, davies_bouldinScore))"
      ],
      "metadata": {
        "id": "g05RzgD0yYvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cc03947-f377-466c-8026-d49581cc6af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poly_ipca_df = pd.DataFrame.from_records(scores_poly, columns=[\"Silhouette\", \"ARI\", \"DB score\"])\n",
        "poly_ipca_df[\"Method\"] = \"kernel PCA (poly)\"\n",
        "poly_ipca_df.describe().loc[[\"mean\", \"std\"],]\n",
        "poly_ipca_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6JEwR7G84arA",
        "outputId": "59be1164-fef5-493a-ff67-125c99c21dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Silhouette       ARI  DB score             Method\n",
              "0   -0.012303  0.124941  4.696901  kernel PCA (poly)\n",
              "1   -0.007411  0.133891  4.709202  kernel PCA (poly)\n",
              "2   -0.006155  0.122502  4.708337  kernel PCA (poly)\n",
              "3   -0.009912  0.127455  4.765225  kernel PCA (poly)\n",
              "4   -0.006701  0.125809  4.539756  kernel PCA (poly)"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8d25db82-87d3-40ee-9201-6ae46042d996\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Silhouette</th>\n",
              "      <th>ARI</th>\n",
              "      <th>DB score</th>\n",
              "      <th>Method</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.012303</td>\n",
              "      <td>0.124941</td>\n",
              "      <td>4.696901</td>\n",
              "      <td>kernel PCA (poly)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.007411</td>\n",
              "      <td>0.133891</td>\n",
              "      <td>4.709202</td>\n",
              "      <td>kernel PCA (poly)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.006155</td>\n",
              "      <td>0.122502</td>\n",
              "      <td>4.708337</td>\n",
              "      <td>kernel PCA (poly)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.009912</td>\n",
              "      <td>0.127455</td>\n",
              "      <td>4.765225</td>\n",
              "      <td>kernel PCA (poly)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.006701</td>\n",
              "      <td>0.125809</td>\n",
              "      <td>4.539756</td>\n",
              "      <td>kernel PCA (poly)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d25db82-87d3-40ee-9201-6ae46042d996')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8d25db82-87d3-40ee-9201-6ae46042d996 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8d25db82-87d3-40ee-9201-6ae46042d996');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Silhouette score of data with reduced dimensionality by KernelPCA (Sigmoid kernel)"
      ],
      "metadata": {
        "id": "3BQ6GmdlDqke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores_sigmoid = []\n",
        "\n",
        "for train_index, val_index in kf.split(train_df.index):\n",
        "  X_val = train_df.iloc[val_index, :-1]\n",
        "  y_val = y_train.iloc[val_index, :]\n",
        "  X_train = train_df.iloc[train_index, :-1]\n",
        "  \n",
        "  pca = KernelPCA(n_components=2408, kernel=\"sigmoid\").fit(X_train)\n",
        "  X_val = pca.transform(X_val)\n",
        "\n",
        "  km = KMeans(n_clusters=len(set(y_val[\"Label\"])), random_state=42)\n",
        "  y_pred = km.fit_predict(X_val)\n",
        "  ari = adjusted_rand_score(y_pred, y_val[\"Label\"])\n",
        "  silhouetteScore = silhouette_score(X_val, y_val[\"Label\"], metric=\"euclidean\")\n",
        "  davies_bouldinScore = davies_bouldin_score(X_val, y_val[\"Label\"])\n",
        "\n",
        "  scores_sigmoid.append((silhouetteScore, ari, davies_bouldinScore))"
      ],
      "metadata": {
        "id": "4UmUQYIMD2Av",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df70b316-9067-4681-ffa7-92c098ea6402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid_ipca_df = pd.DataFrame.from_records(scores_sigmoid, columns=[\"Silhouette\",\"ARI\", \"DB score\"])\n",
        "sigmoid_ipca_df[\"Method\"] = \"kernel PCA (sigmoid)\"\n",
        "sigmoid_ipca_df.describe().loc[[\"mean\", \"std\"],]\n",
        "sigmoid_ipca_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JW0rDnUUE2VW",
        "outputId": "3c61c285-7739-4247-c0a9-599a6fe0bf3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Silhouette       ARI  DB score                Method\n",
              "0   -0.015647  0.129573  4.782981  kernel PCA (sigmoid)\n",
              "1   -0.011777  0.140573  4.823139  kernel PCA (sigmoid)\n",
              "2   -0.010621  0.137111  4.821522  kernel PCA (sigmoid)\n",
              "3   -0.013103  0.143702  4.897182  kernel PCA (sigmoid)\n",
              "4   -0.011434  0.120230  4.633937  kernel PCA (sigmoid)"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d42535bc-335e-4ab2-9e8a-088dcec4d5b8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Silhouette</th>\n",
              "      <th>ARI</th>\n",
              "      <th>DB score</th>\n",
              "      <th>Method</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.015647</td>\n",
              "      <td>0.129573</td>\n",
              "      <td>4.782981</td>\n",
              "      <td>kernel PCA (sigmoid)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.011777</td>\n",
              "      <td>0.140573</td>\n",
              "      <td>4.823139</td>\n",
              "      <td>kernel PCA (sigmoid)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.010621</td>\n",
              "      <td>0.137111</td>\n",
              "      <td>4.821522</td>\n",
              "      <td>kernel PCA (sigmoid)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.013103</td>\n",
              "      <td>0.143702</td>\n",
              "      <td>4.897182</td>\n",
              "      <td>kernel PCA (sigmoid)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.011434</td>\n",
              "      <td>0.120230</td>\n",
              "      <td>4.633937</td>\n",
              "      <td>kernel PCA (sigmoid)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d42535bc-335e-4ab2-9e8a-088dcec4d5b8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d42535bc-335e-4ab2-9e8a-088dcec4d5b8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d42535bc-335e-4ab2-9e8a-088dcec4d5b8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UMAP"
      ],
      "metadata": {
        "id": "EO_YChEOlF2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "UMAP hyperparameters - n_neighbors, min_dist and n_components."
      ],
      "metadata": {
        "id": "MFEO1vIDANmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set environment variables\n",
        "os.environ['NUMBA_NUM_THREADS'] = '88'\n",
        "\n",
        "scores_UMAP = []\n",
        "\n",
        "for train_index, val_index in kf.split(train_df.index):\n",
        "  X_val = train_df.iloc[val_index, :-1]\n",
        "  y_val = y_train.iloc[val_index, :]\n",
        "  X_train = train_df.iloc[train_index, :-1]\n",
        "\n",
        "  ump = UMAP(n_components=2408, min_dist=0.1, n_neighbors=150).fit(X_train)\n",
        "  X_val = ump.transform(X_val)\n",
        "\n",
        "  km = KMeans(n_clusters=len(set(y_val[\"Label\"])), random_state=42)\n",
        "  y_pred = km.fit_predict(X_val)\n",
        "  ari = adjusted_rand_score(y_pred, y_val[\"Label\"])\n",
        "  silhouetteScore = silhouette_score(X_val, y_val[\"Label\"], metric=\"euclidean\")\n",
        "  davies_bouldinScore = davies_bouldin_score(X_val, y_val[\"Label\"])\n",
        "  print((silhouetteScore, ari, davies_bouldinScore))\n",
        "  scores_UMAP.append((silhouetteScore, ari, davies_bouldinScore))"
      ],
      "metadata": {
        "id": "uNuxVaTmlIkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad2705a-15c0-4222-aa99-e5f77cbdeb2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(-0.06695496, 4.682331250728213)\n",
            "(-0.049814403, 6.197495981642595)\n",
            "(-0.058389537, 5.9159804146144594)\n",
            "(-0.06818275, 5.769339341907875)\n",
            "(-0.06821633, 4.830218961571739)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "umap_df = pd.DataFrame.from_records(scores_UMAP, columns=[\"Silhouette\", \"ARI\", \"DB score\"])\n",
        "umap_df[\"Method\"] = \"UMAP\"\n",
        "umap_df.describe().loc[[\"mean\", \"std\"],]\n",
        "umap_df.to_csv(f\"{path}/temp_umap.csv\")"
      ],
      "metadata": {
        "id": "A4w_NHmzwlfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "umap_df.describe().loc[[\"mean\", \"std\"],]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "RjrNjrSy1kj0",
        "outputId": "a6f0c0dd-862a-45bc-ceea-cdfd64604069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Silhouette       ARI  DB score\n",
              "mean   -0.062312  0.172180  5.479073\n",
              "std     0.008100  0.007252  0.679537"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Silhouette</th>\n",
              "      <th>ARI</th>\n",
              "      <th>DB score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-0.062312</td>\n",
              "      <td>0.172180</td>\n",
              "      <td>5.479073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.008100</td>\n",
              "      <td>0.007252</td>\n",
              "      <td>0.679537</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoencoder implentation"
      ],
      "metadata": {
        "id": "v9dNa8YPxUXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model, Sequential\n",
        "from keras.losses import mean_squared_error\n",
        "from keras.metrics import binary_crossentropy\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.callbacks import CSVLogger\n",
        "from keras.layers import Reshape, Lambda, Dropout, BatchNormalization, Dense, Input\n",
        "from keras.initializers import VarianceScaling\n",
        "from keras.optimizers import SGD\n",
        "import gc\n",
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "RmRh4NUHblMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def autoencoder(dims, act='tanh', init='glorot_uniform'):\n",
        "    n_stacks = len(dims) - 1\n",
        "    # input\n",
        "    input_data = Input(shape=(dims[0],), name='input')\n",
        "    x = input_data\n",
        "    # internal layers in encoder\n",
        "    for i in range(n_stacks-1):\n",
        "        x = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x) # Dense act and kernel_initializer\n",
        "        x = BatchNormalization(name='batch_norm_%d' % i)(x) # Add batch normalization layer after dense layer\n",
        "\n",
        "    # hidden layer\n",
        "    encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)  # hidden layer, features are extracted from here\n",
        "\n",
        "    x = encoded\n",
        "    # internal layers in decoder\n",
        "    for i in range(n_stacks-1, 0, -1):\n",
        "        x = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x) # activation=act, kernel_initializer=init Dense\n",
        "        x = BatchNormalization(name='batch_norm_%d' % (n_stacks-i))(x) # Add batch normalization layer after dense layer\n",
        "    # output\n",
        "    x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n",
        "    decoded = x\n",
        "    return Model(inputs=input_data, outputs=decoded, name='AE'), Model(inputs=input_data, outputs=encoded, name='encoder')\n"
      ],
      "metadata": {
        "id": "TtLhMk-oa4nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = SGD(learning_rate=0.2, decay=1e-5, momentum=0.9, nesterov=True)\n",
        "\n",
        "batch_size = 1024\n",
        "epochs = 1000\n",
        "\n",
        "init = VarianceScaling(scale=1. / 2., mode='fan_avg',\n",
        "                      distribution='uniform',\n",
        "                      seed=0)\n",
        "\n",
        "csv_logger = CSVLogger(f'{path}/logs_hyperparameter_tuning_5CV.csv', append=True, separator=';')"
      ],
      "metadata": {
        "id": "uZoA09x5bRut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model, encoder\n",
        "K.clear_session()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7-QzXKRwXa7",
        "outputId": "be1c5ab1-8f9c-4c14-8bf6-fec20b49c5f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1880"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_ae = []\n",
        "\n",
        "for train_index, val_index in kf.split(train_df.index):\n",
        "  X_train = train_df.iloc[train_index, :]\n",
        "  y_tr = y_train.iloc[train_index, :]\n",
        "  X_valid = train_df.iloc[val_index, :]\n",
        "  y_valid = y_train.iloc[val_index, :]\n",
        "\n",
        "  pca = PCA(0.95).fit(X_train)\n",
        "  x_train = pca.transform(X_train)\n",
        "  x_val = pca.transform(X_valid)\n",
        "\n",
        "  mapp = LabelEncoder()\n",
        "\n",
        "  y_train_adasyn = mapp.fit_transform(y_tr[\"Label\"]).ravel()\n",
        "\n",
        "  ada = ADASYN(random_state=0, sampling_strategy='all')\n",
        "  x_train_ada, y_train_ada = ada.fit_resample(x_train, y_train_adasyn)\n",
        "  dims = [x_train_ada.shape[1], 1024, 64]\n",
        "  model, encoder = autoencoder(dims, init=init)\n",
        "  model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "  csv_logger = CSVLogger(f'{path}/log_ae.csv', append=True, separator=';')\n",
        "\n",
        "  history = model.fit(x_train_ada, x_train_ada, batch_size=batch_size, epochs=epochs, validation_data=(x_val, x_val), callbacks=[csv_logger])\n",
        "  X_val_pred = encoder.predict(x_val, batch_size=batch_size)\n",
        "  \n",
        "  km = KMeans(n_clusters=10, random_state=42)\n",
        "  y_pred = km.fit_predict(X_val_pred)\n",
        "  ari = adjusted_rand_score(y_pred, y_valid[\"Label\"])\n",
        "\n",
        "  silhouetteScore = silhouette_score(X_val_pred, y_valid[\"Label\"], metric=\"euclidean\")\n",
        "  davies_bouldinScore = davies_bouldin_score(X_val_pred, y_valid[\"Label\"])\n",
        "  \n",
        "  scores_ae.append((silhouetteScore, ari, davies_bouldinScore))\n",
        "  print(scores_ae)\n",
        "  ncoder.save(f'{path}/vae_encoder.h5')\n",
        "  del model, encoder\n",
        "  K.clear_session()\n",
        "  gc.collect()\n",
        " "
      ],
      "metadata": {
        "id": "ldGAU1zNxTbY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "655fe1d4-7644-4749-fd09-e3d5cf607594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "33/33 [==============================] - 5s 33ms/step - loss: 0.5697 - val_loss: 0.5504\n",
            "Epoch 2/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.4128 - val_loss: 0.5121\n",
            "Epoch 3/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.3759 - val_loss: 0.4837\n",
            "Epoch 4/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.3553 - val_loss: 0.4534\n",
            "Epoch 5/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.3400 - val_loss: 0.4262\n",
            "Epoch 6/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.3276 - val_loss: 0.4030\n",
            "Epoch 7/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.3172 - val_loss: 0.3830\n",
            "Epoch 8/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.3087 - val_loss: 0.3663\n",
            "Epoch 9/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.3020 - val_loss: 0.3534\n",
            "Epoch 10/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2961 - val_loss: 0.3431\n",
            "Epoch 11/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2912 - val_loss: 0.3343\n",
            "Epoch 12/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2870 - val_loss: 0.3271\n",
            "Epoch 13/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2831 - val_loss: 0.3216\n",
            "Epoch 14/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2797 - val_loss: 0.3165\n",
            "Epoch 15/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2768 - val_loss: 0.3127\n",
            "Epoch 16/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2741 - val_loss: 0.3090\n",
            "Epoch 17/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2717 - val_loss: 0.3060\n",
            "Epoch 18/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2696 - val_loss: 0.3036\n",
            "Epoch 19/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2678 - val_loss: 0.3014\n",
            "Epoch 20/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2661 - val_loss: 0.2993\n",
            "Epoch 21/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2646 - val_loss: 0.2976\n",
            "Epoch 22/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2633 - val_loss: 0.2962\n",
            "Epoch 23/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2622 - val_loss: 0.2949\n",
            "Epoch 24/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2612 - val_loss: 0.2938\n",
            "Epoch 25/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2603 - val_loss: 0.2929\n",
            "Epoch 26/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2595 - val_loss: 0.2920\n",
            "Epoch 27/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2588 - val_loss: 0.2913\n",
            "Epoch 28/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2582 - val_loss: 0.2906\n",
            "Epoch 29/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2577 - val_loss: 0.2901\n",
            "Epoch 30/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2573 - val_loss: 0.2896\n",
            "Epoch 31/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2567 - val_loss: 0.2892\n",
            "Epoch 32/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2565 - val_loss: 0.2888\n",
            "Epoch 33/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2561 - val_loss: 0.2885\n",
            "Epoch 34/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2558 - val_loss: 0.2882\n",
            "Epoch 35/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2556 - val_loss: 0.2879\n",
            "Epoch 36/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2553 - val_loss: 0.2877\n",
            "Epoch 37/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2550 - val_loss: 0.2874\n",
            "Epoch 38/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2548 - val_loss: 0.2872\n",
            "Epoch 39/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2546 - val_loss: 0.2870\n",
            "Epoch 40/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2544 - val_loss: 0.2868\n",
            "Epoch 41/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2543 - val_loss: 0.2867\n",
            "Epoch 42/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2541 - val_loss: 0.2865\n",
            "Epoch 43/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2539 - val_loss: 0.2863\n",
            "Epoch 44/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2537 - val_loss: 0.2862\n",
            "Epoch 45/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2535 - val_loss: 0.2861\n",
            "Epoch 46/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2534 - val_loss: 0.2859\n",
            "Epoch 47/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2532 - val_loss: 0.2858\n",
            "Epoch 48/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2531 - val_loss: 0.2857\n",
            "Epoch 49/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2529 - val_loss: 0.2856\n",
            "Epoch 50/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2527 - val_loss: 0.2855\n",
            "Epoch 51/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2525 - val_loss: 0.2854\n",
            "Epoch 52/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2524 - val_loss: 0.2852\n",
            "Epoch 53/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2523 - val_loss: 0.2852\n",
            "Epoch 54/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2521 - val_loss: 0.2851\n",
            "Epoch 55/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2520 - val_loss: 0.2849\n",
            "Epoch 56/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2518 - val_loss: 0.2848\n",
            "Epoch 57/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2517 - val_loss: 0.2848\n",
            "Epoch 58/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2515 - val_loss: 0.2847\n",
            "Epoch 59/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2513 - val_loss: 0.2846\n",
            "Epoch 60/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2512 - val_loss: 0.2844\n",
            "Epoch 61/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2510 - val_loss: 0.2844\n",
            "Epoch 62/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2509 - val_loss: 0.2843\n",
            "Epoch 63/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2507 - val_loss: 0.2842\n",
            "Epoch 64/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2505 - val_loss: 0.2841\n",
            "Epoch 65/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2502 - val_loss: 0.2840\n",
            "Epoch 66/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2501 - val_loss: 0.2839\n",
            "Epoch 67/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2499 - val_loss: 0.2838\n",
            "Epoch 68/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2496 - val_loss: 0.2837\n",
            "Epoch 69/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2495 - val_loss: 0.2837\n",
            "Epoch 70/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2492 - val_loss: 0.2836\n",
            "Epoch 71/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2491 - val_loss: 0.2835\n",
            "Epoch 72/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2488 - val_loss: 0.2834\n",
            "Epoch 73/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2486 - val_loss: 0.2833\n",
            "Epoch 74/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2484 - val_loss: 0.2832\n",
            "Epoch 75/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2481 - val_loss: 0.2831\n",
            "Epoch 76/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2480 - val_loss: 0.2830\n",
            "Epoch 77/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2476 - val_loss: 0.2829\n",
            "Epoch 78/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2475 - val_loss: 0.2828\n",
            "Epoch 79/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2473 - val_loss: 0.2827\n",
            "Epoch 80/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2469 - val_loss: 0.2826\n",
            "Epoch 81/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2467 - val_loss: 0.2825\n",
            "Epoch 82/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2465 - val_loss: 0.2824\n",
            "Epoch 83/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2463 - val_loss: 0.2823\n",
            "Epoch 84/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2459 - val_loss: 0.2822\n",
            "Epoch 85/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2457 - val_loss: 0.2821\n",
            "Epoch 86/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2455 - val_loss: 0.2820\n",
            "Epoch 87/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2452 - val_loss: 0.2820\n",
            "Epoch 88/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2449 - val_loss: 0.2819\n",
            "Epoch 89/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2446 - val_loss: 0.2817\n",
            "Epoch 90/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2443 - val_loss: 0.2816\n",
            "Epoch 91/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2440 - val_loss: 0.2815\n",
            "Epoch 92/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2438 - val_loss: 0.2815\n",
            "Epoch 93/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2434 - val_loss: 0.2814\n",
            "Epoch 94/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2432 - val_loss: 0.2812\n",
            "Epoch 95/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2429 - val_loss: 0.2811\n",
            "Epoch 96/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2426 - val_loss: 0.2810\n",
            "Epoch 97/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2423 - val_loss: 0.2809\n",
            "Epoch 98/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2420 - val_loss: 0.2808\n",
            "Epoch 99/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2417 - val_loss: 0.2807\n",
            "Epoch 100/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2414 - val_loss: 0.2806\n",
            "Epoch 101/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2411 - val_loss: 0.2805\n",
            "Epoch 102/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2408 - val_loss: 0.2804\n",
            "Epoch 103/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2405 - val_loss: 0.2803\n",
            "Epoch 104/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2402 - val_loss: 0.2802\n",
            "Epoch 105/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2399 - val_loss: 0.2801\n",
            "Epoch 106/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2395 - val_loss: 0.2800\n",
            "Epoch 107/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2394 - val_loss: 0.2799\n",
            "Epoch 108/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2389 - val_loss: 0.2798\n",
            "Epoch 109/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2386 - val_loss: 0.2797\n",
            "Epoch 110/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2383 - val_loss: 0.2796\n",
            "Epoch 111/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2380 - val_loss: 0.2795\n",
            "Epoch 112/1000\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.2377 - val_loss: 0.2794\n",
            "Epoch 113/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2374 - val_loss: 0.2793\n",
            "Epoch 114/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2371 - val_loss: 0.2792\n",
            "Epoch 115/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2367 - val_loss: 0.2791\n",
            "Epoch 116/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2365 - val_loss: 0.2790\n",
            "Epoch 117/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2362 - val_loss: 0.2789\n",
            "Epoch 118/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2358 - val_loss: 0.2788\n",
            "Epoch 119/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2356 - val_loss: 0.2787\n",
            "Epoch 120/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2352 - val_loss: 0.2786\n",
            "Epoch 121/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2349 - val_loss: 0.2785\n",
            "Epoch 122/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2345 - val_loss: 0.2784\n",
            "Epoch 123/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2342 - val_loss: 0.2783\n",
            "Epoch 124/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2340 - val_loss: 0.2782\n",
            "Epoch 125/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2336 - val_loss: 0.2781\n",
            "Epoch 126/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2334 - val_loss: 0.2780\n",
            "Epoch 127/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2330 - val_loss: 0.2780\n",
            "Epoch 128/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2327 - val_loss: 0.2778\n",
            "Epoch 129/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2325 - val_loss: 0.2778\n",
            "Epoch 130/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2320 - val_loss: 0.2777\n",
            "Epoch 131/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2318 - val_loss: 0.2776\n",
            "Epoch 132/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2315 - val_loss: 0.2775\n",
            "Epoch 133/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2312 - val_loss: 0.2774\n",
            "Epoch 134/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2309 - val_loss: 0.2773\n",
            "Epoch 135/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2305 - val_loss: 0.2773\n",
            "Epoch 136/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2302 - val_loss: 0.2772\n",
            "Epoch 137/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2299 - val_loss: 0.2771\n",
            "Epoch 138/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2296 - val_loss: 0.2770\n",
            "Epoch 139/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2293 - val_loss: 0.2769\n",
            "Epoch 140/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2290 - val_loss: 0.2769\n",
            "Epoch 141/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2287 - val_loss: 0.2767\n",
            "Epoch 142/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2284 - val_loss: 0.2767\n",
            "Epoch 143/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2282 - val_loss: 0.2768\n",
            "Epoch 144/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2279 - val_loss: 0.2766\n",
            "Epoch 145/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2275 - val_loss: 0.2765\n",
            "Epoch 146/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2272 - val_loss: 0.2764\n",
            "Epoch 147/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2270 - val_loss: 0.2764\n",
            "Epoch 148/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2267 - val_loss: 0.2763\n",
            "Epoch 149/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2264 - val_loss: 0.2762\n",
            "Epoch 150/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2261 - val_loss: 0.2761\n",
            "Epoch 151/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2257 - val_loss: 0.2760\n",
            "Epoch 152/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2255 - val_loss: 0.2761\n",
            "Epoch 153/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2253 - val_loss: 0.2759\n",
            "Epoch 154/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2248 - val_loss: 0.2758\n",
            "Epoch 155/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2246 - val_loss: 0.2757\n",
            "Epoch 156/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2243 - val_loss: 0.2759\n",
            "Epoch 157/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2241 - val_loss: 0.2756\n",
            "Epoch 158/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2237 - val_loss: 0.2755\n",
            "Epoch 159/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2235 - val_loss: 0.2756\n",
            "Epoch 160/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2232 - val_loss: 0.2754\n",
            "Epoch 161/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2229 - val_loss: 0.2753\n",
            "Epoch 162/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2227 - val_loss: 0.2756\n",
            "Epoch 163/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2224 - val_loss: 0.2752\n",
            "Epoch 164/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2221 - val_loss: 0.2752\n",
            "Epoch 165/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2217 - val_loss: 0.2751\n",
            "Epoch 166/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2215 - val_loss: 0.2750\n",
            "Epoch 167/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2213 - val_loss: 0.2751\n",
            "Epoch 168/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2211 - val_loss: 0.2750\n",
            "Epoch 169/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2208 - val_loss: 0.2748\n",
            "Epoch 170/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2205 - val_loss: 0.2750\n",
            "Epoch 171/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2201 - val_loss: 0.2748\n",
            "Epoch 172/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2199 - val_loss: 0.2749\n",
            "Epoch 173/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2197 - val_loss: 0.2746\n",
            "Epoch 174/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2195 - val_loss: 0.2747\n",
            "Epoch 175/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2192 - val_loss: 0.2746\n",
            "Epoch 176/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2188 - val_loss: 0.2744\n",
            "Epoch 177/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2185 - val_loss: 0.2743\n",
            "Epoch 178/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2183 - val_loss: 0.2752\n",
            "Epoch 179/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2183 - val_loss: 0.2742\n",
            "Epoch 180/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2176 - val_loss: 0.2742\n",
            "Epoch 181/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2175 - val_loss: 0.2742\n",
            "Epoch 182/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2173 - val_loss: 0.2741\n",
            "Epoch 183/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2169 - val_loss: 0.2743\n",
            "Epoch 184/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2171 - val_loss: 0.2740\n",
            "Epoch 185/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2164 - val_loss: 0.2740\n",
            "Epoch 186/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2162 - val_loss: 0.2740\n",
            "Epoch 187/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2160 - val_loss: 0.2740\n",
            "Epoch 188/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2158 - val_loss: 0.2739\n",
            "Epoch 189/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2155 - val_loss: 0.2738\n",
            "Epoch 190/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2151 - val_loss: 0.2738\n",
            "Epoch 191/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2152 - val_loss: 0.2743\n",
            "Epoch 192/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2147 - val_loss: 0.2736\n",
            "Epoch 193/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2145 - val_loss: 0.2736\n",
            "Epoch 194/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2142 - val_loss: 0.2737\n",
            "Epoch 195/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2140 - val_loss: 0.2737\n",
            "Epoch 196/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2136 - val_loss: 0.2735\n",
            "Epoch 197/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2135 - val_loss: 0.2736\n",
            "Epoch 198/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2135 - val_loss: 0.2736\n",
            "Epoch 199/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2129 - val_loss: 0.2734\n",
            "Epoch 200/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2127 - val_loss: 0.2734\n",
            "Epoch 201/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2125 - val_loss: 0.2734\n",
            "Epoch 202/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2123 - val_loss: 0.2736\n",
            "Epoch 203/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2123 - val_loss: 0.2733\n",
            "Epoch 204/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2118 - val_loss: 0.2732\n",
            "Epoch 205/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2116 - val_loss: 0.2732\n",
            "Epoch 206/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2116 - val_loss: 0.2733\n",
            "Epoch 207/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2111 - val_loss: 0.2732\n",
            "Epoch 208/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2109 - val_loss: 0.2731\n",
            "Epoch 209/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2107 - val_loss: 0.2732\n",
            "Epoch 210/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2106 - val_loss: 0.2730\n",
            "Epoch 211/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2101 - val_loss: 0.2730\n",
            "Epoch 212/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2101 - val_loss: 0.2732\n",
            "Epoch 213/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2099 - val_loss: 0.2734\n",
            "Epoch 214/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2096 - val_loss: 0.2729\n",
            "Epoch 215/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2092 - val_loss: 0.2728\n",
            "Epoch 216/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2091 - val_loss: 0.2729\n",
            "Epoch 217/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2089 - val_loss: 0.2730\n",
            "Epoch 218/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2088 - val_loss: 0.2729\n",
            "Epoch 219/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2084 - val_loss: 0.2727\n",
            "Epoch 220/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2083 - val_loss: 0.2727\n",
            "Epoch 221/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2080 - val_loss: 0.2733\n",
            "Epoch 222/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2079 - val_loss: 0.2727\n",
            "Epoch 223/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2074 - val_loss: 0.2726\n",
            "Epoch 224/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2073 - val_loss: 0.2726\n",
            "Epoch 225/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2071 - val_loss: 0.2727\n",
            "Epoch 226/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2071 - val_loss: 0.2726\n",
            "Epoch 227/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2067 - val_loss: 0.2726\n",
            "Epoch 228/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2065 - val_loss: 0.2726\n",
            "Epoch 229/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2063 - val_loss: 0.2726\n",
            "Epoch 230/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2059 - val_loss: 0.2726\n",
            "Epoch 231/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2059 - val_loss: 0.2724\n",
            "Epoch 232/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2056 - val_loss: 0.2724\n",
            "Epoch 233/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2056 - val_loss: 0.2728\n",
            "Epoch 234/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2054 - val_loss: 0.2724\n",
            "Epoch 235/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2048 - val_loss: 0.2723\n",
            "Epoch 236/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2047 - val_loss: 0.2724\n",
            "Epoch 237/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2047 - val_loss: 0.2727\n",
            "Epoch 238/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2046 - val_loss: 0.2724\n",
            "Epoch 239/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2041 - val_loss: 0.2723\n",
            "Epoch 240/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2041 - val_loss: 0.2729\n",
            "Epoch 241/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2038 - val_loss: 0.2723\n",
            "Epoch 242/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2036 - val_loss: 0.2724\n",
            "Epoch 243/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2036 - val_loss: 0.2725\n",
            "Epoch 244/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2032 - val_loss: 0.2722\n",
            "Epoch 245/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2029 - val_loss: 0.2725\n",
            "Epoch 246/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2030 - val_loss: 0.2724\n",
            "Epoch 247/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.2026 - val_loss: 0.2722\n",
            "Epoch 248/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2024 - val_loss: 0.2724\n",
            "Epoch 249/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2022 - val_loss: 0.2722\n",
            "Epoch 250/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2021 - val_loss: 0.2723\n",
            "Epoch 251/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2018 - val_loss: 0.2722\n",
            "Epoch 252/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2016 - val_loss: 0.2724\n",
            "Epoch 253/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2018 - val_loss: 0.2722\n",
            "Epoch 254/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2012 - val_loss: 0.2721\n",
            "Epoch 255/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2011 - val_loss: 0.2722\n",
            "Epoch 256/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2010 - val_loss: 0.2721\n",
            "Epoch 257/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2010 - val_loss: 0.2726\n",
            "Epoch 258/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2006 - val_loss: 0.2721\n",
            "Epoch 259/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2004 - val_loss: 0.2721\n",
            "Epoch 260/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.2001 - val_loss: 0.2721\n",
            "Epoch 261/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.2001 - val_loss: 0.2721\n",
            "Epoch 262/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1999 - val_loss: 0.2722\n",
            "Epoch 263/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1998 - val_loss: 0.2724\n",
            "Epoch 264/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1995 - val_loss: 0.2720\n",
            "Epoch 265/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1992 - val_loss: 0.2720\n",
            "Epoch 266/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1993 - val_loss: 0.2723\n",
            "Epoch 267/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1989 - val_loss: 0.2719\n",
            "Epoch 268/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1988 - val_loss: 0.2720\n",
            "Epoch 269/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1985 - val_loss: 0.2720\n",
            "Epoch 270/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1985 - val_loss: 0.2722\n",
            "Epoch 271/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1982 - val_loss: 0.2720\n",
            "Epoch 272/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1982 - val_loss: 0.2720\n",
            "Epoch 273/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1978 - val_loss: 0.2720\n",
            "Epoch 274/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1977 - val_loss: 0.2723\n",
            "Epoch 275/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1979 - val_loss: 0.2720\n",
            "Epoch 276/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1974 - val_loss: 0.2719\n",
            "Epoch 277/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1971 - val_loss: 0.2720\n",
            "Epoch 278/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1969 - val_loss: 0.2720\n",
            "Epoch 279/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1972 - val_loss: 0.2721\n",
            "Epoch 280/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1967 - val_loss: 0.2720\n",
            "Epoch 281/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1964 - val_loss: 0.2720\n",
            "Epoch 282/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1964 - val_loss: 0.2720\n",
            "Epoch 283/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1962 - val_loss: 0.2719\n",
            "Epoch 284/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1961 - val_loss: 0.2722\n",
            "Epoch 285/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1959 - val_loss: 0.2721\n",
            "Epoch 286/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1956 - val_loss: 0.2720\n",
            "Epoch 287/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1957 - val_loss: 0.2721\n",
            "Epoch 288/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1953 - val_loss: 0.2720\n",
            "Epoch 289/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1953 - val_loss: 0.2724\n",
            "Epoch 290/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1952 - val_loss: 0.2720\n",
            "Epoch 291/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1950 - val_loss: 0.2720\n",
            "Epoch 292/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1946 - val_loss: 0.2719\n",
            "Epoch 293/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1945 - val_loss: 0.2721\n",
            "Epoch 294/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1945 - val_loss: 0.2721\n",
            "Epoch 295/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1943 - val_loss: 0.2720\n",
            "Epoch 296/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1941 - val_loss: 0.2724\n",
            "Epoch 297/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1942 - val_loss: 0.2719\n",
            "Epoch 298/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1937 - val_loss: 0.2719\n",
            "Epoch 299/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1938 - val_loss: 0.2719\n",
            "Epoch 300/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1935 - val_loss: 0.2721\n",
            "Epoch 301/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1935 - val_loss: 0.2720\n",
            "Epoch 302/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1932 - val_loss: 0.2721\n",
            "Epoch 303/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1930 - val_loss: 0.2724\n",
            "Epoch 304/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1930 - val_loss: 0.2720\n",
            "Epoch 305/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1927 - val_loss: 0.2721\n",
            "Epoch 306/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1926 - val_loss: 0.2720\n",
            "Epoch 307/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1927 - val_loss: 0.2721\n",
            "Epoch 308/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1923 - val_loss: 0.2719\n",
            "Epoch 309/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1921 - val_loss: 0.2720\n",
            "Epoch 310/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1919 - val_loss: 0.2719\n",
            "Epoch 311/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1918 - val_loss: 0.2721\n",
            "Epoch 312/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1917 - val_loss: 0.2721\n",
            "Epoch 313/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1917 - val_loss: 0.2721\n",
            "Epoch 314/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1913 - val_loss: 0.2720\n",
            "Epoch 315/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1911 - val_loss: 0.2721\n",
            "Epoch 316/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1910 - val_loss: 0.2720\n",
            "Epoch 317/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1909 - val_loss: 0.2720\n",
            "Epoch 318/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1910 - val_loss: 0.2727\n",
            "Epoch 319/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1910 - val_loss: 0.2721\n",
            "Epoch 320/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1906 - val_loss: 0.2720\n",
            "Epoch 321/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1904 - val_loss: 0.2721\n",
            "Epoch 322/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1903 - val_loss: 0.2720\n",
            "Epoch 323/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1900 - val_loss: 0.2721\n",
            "Epoch 324/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1901 - val_loss: 0.2724\n",
            "Epoch 325/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1901 - val_loss: 0.2723\n",
            "Epoch 326/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1897 - val_loss: 0.2721\n",
            "Epoch 327/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1896 - val_loss: 0.2724\n",
            "Epoch 328/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1895 - val_loss: 0.2721\n",
            "Epoch 329/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1892 - val_loss: 0.2722\n",
            "Epoch 330/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1893 - val_loss: 0.2723\n",
            "Epoch 331/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1890 - val_loss: 0.2721\n",
            "Epoch 332/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1889 - val_loss: 0.2721\n",
            "Epoch 333/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1888 - val_loss: 0.2723\n",
            "Epoch 334/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1887 - val_loss: 0.2727\n",
            "Epoch 335/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1887 - val_loss: 0.2721\n",
            "Epoch 336/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1884 - val_loss: 0.2722\n",
            "Epoch 337/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1881 - val_loss: 0.2721\n",
            "Epoch 338/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1881 - val_loss: 0.2722\n",
            "Epoch 339/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1882 - val_loss: 0.2725\n",
            "Epoch 340/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1879 - val_loss: 0.2722\n",
            "Epoch 341/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1876 - val_loss: 0.2722\n",
            "Epoch 342/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1875 - val_loss: 0.2722\n",
            "Epoch 343/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1876 - val_loss: 0.2724\n",
            "Epoch 344/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1876 - val_loss: 0.2729\n",
            "Epoch 345/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1873 - val_loss: 0.2722\n",
            "Epoch 346/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1870 - val_loss: 0.2722\n",
            "Epoch 347/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1869 - val_loss: 0.2723\n",
            "Epoch 348/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1867 - val_loss: 0.2722\n",
            "Epoch 349/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1867 - val_loss: 0.2725\n",
            "Epoch 350/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1867 - val_loss: 0.2725\n",
            "Epoch 351/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1864 - val_loss: 0.2723\n",
            "Epoch 352/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1863 - val_loss: 0.2725\n",
            "Epoch 353/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1862 - val_loss: 0.2727\n",
            "Epoch 354/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1862 - val_loss: 0.2725\n",
            "Epoch 355/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1860 - val_loss: 0.2723\n",
            "Epoch 356/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1858 - val_loss: 0.2723\n",
            "Epoch 357/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1858 - val_loss: 0.2726\n",
            "Epoch 358/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1857 - val_loss: 0.2726\n",
            "Epoch 359/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1858 - val_loss: 0.2724\n",
            "Epoch 360/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1853 - val_loss: 0.2724\n",
            "Epoch 361/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1851 - val_loss: 0.2724\n",
            "Epoch 362/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1850 - val_loss: 0.2724\n",
            "Epoch 363/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1849 - val_loss: 0.2725\n",
            "Epoch 364/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1849 - val_loss: 0.2728\n",
            "Epoch 365/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1851 - val_loss: 0.2727\n",
            "Epoch 366/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1847 - val_loss: 0.2725\n",
            "Epoch 367/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1845 - val_loss: 0.2725\n",
            "Epoch 368/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1845 - val_loss: 0.2728\n",
            "Epoch 369/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1844 - val_loss: 0.2727\n",
            "Epoch 370/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1843 - val_loss: 0.2726\n",
            "Epoch 371/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1840 - val_loss: 0.2725\n",
            "Epoch 372/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1840 - val_loss: 0.2730\n",
            "Epoch 373/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1840 - val_loss: 0.2727\n",
            "Epoch 374/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1840 - val_loss: 0.2726\n",
            "Epoch 375/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1837 - val_loss: 0.2727\n",
            "Epoch 376/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1834 - val_loss: 0.2726\n",
            "Epoch 377/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1834 - val_loss: 0.2726\n",
            "Epoch 378/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1833 - val_loss: 0.2728\n",
            "Epoch 379/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1834 - val_loss: 0.2728\n",
            "Epoch 380/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1831 - val_loss: 0.2728\n",
            "Epoch 381/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1829 - val_loss: 0.2728\n",
            "Epoch 382/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1829 - val_loss: 0.2728\n",
            "Epoch 383/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1829 - val_loss: 0.2732\n",
            "Epoch 384/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1827 - val_loss: 0.2727\n",
            "Epoch 385/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1826 - val_loss: 0.2729\n",
            "Epoch 386/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1824 - val_loss: 0.2728\n",
            "Epoch 387/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1825 - val_loss: 0.2732\n",
            "Epoch 388/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1823 - val_loss: 0.2728\n",
            "Epoch 389/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1821 - val_loss: 0.2728\n",
            "Epoch 390/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1821 - val_loss: 0.2729\n",
            "Epoch 391/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1819 - val_loss: 0.2730\n",
            "Epoch 392/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1818 - val_loss: 0.2728\n",
            "Epoch 393/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1817 - val_loss: 0.2735\n",
            "Epoch 394/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1818 - val_loss: 0.2730\n",
            "Epoch 395/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1815 - val_loss: 0.2730\n",
            "Epoch 396/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1813 - val_loss: 0.2729\n",
            "Epoch 397/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1813 - val_loss: 0.2729\n",
            "Epoch 398/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1812 - val_loss: 0.2730\n",
            "Epoch 399/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1811 - val_loss: 0.2732\n",
            "Epoch 400/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1810 - val_loss: 0.2731\n",
            "Epoch 401/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1809 - val_loss: 0.2731\n",
            "Epoch 402/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1808 - val_loss: 0.2730\n",
            "Epoch 403/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1807 - val_loss: 0.2731\n",
            "Epoch 404/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1805 - val_loss: 0.2731\n",
            "Epoch 405/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1806 - val_loss: 0.2737\n",
            "Epoch 406/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1806 - val_loss: 0.2731\n",
            "Epoch 407/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1803 - val_loss: 0.2732\n",
            "Epoch 408/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1803 - val_loss: 0.2736\n",
            "Epoch 409/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1803 - val_loss: 0.2731\n",
            "Epoch 410/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1799 - val_loss: 0.2732\n",
            "Epoch 411/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1799 - val_loss: 0.2734\n",
            "Epoch 412/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1797 - val_loss: 0.2733\n",
            "Epoch 413/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1798 - val_loss: 0.2732\n",
            "Epoch 414/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1796 - val_loss: 0.2733\n",
            "Epoch 415/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1794 - val_loss: 0.2733\n",
            "Epoch 416/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1794 - val_loss: 0.2735\n",
            "Epoch 417/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1793 - val_loss: 0.2734\n",
            "Epoch 418/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1792 - val_loss: 0.2733\n",
            "Epoch 419/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1792 - val_loss: 0.2735\n",
            "Epoch 420/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1790 - val_loss: 0.2733\n",
            "Epoch 421/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1790 - val_loss: 0.2735\n",
            "Epoch 422/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1790 - val_loss: 0.2734\n",
            "Epoch 423/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1790 - val_loss: 0.2737\n",
            "Epoch 424/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1787 - val_loss: 0.2736\n",
            "Epoch 425/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1788 - val_loss: 0.2735\n",
            "Epoch 426/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1785 - val_loss: 0.2734\n",
            "Epoch 427/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1784 - val_loss: 0.2736\n",
            "Epoch 428/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1783 - val_loss: 0.2735\n",
            "Epoch 429/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1782 - val_loss: 0.2735\n",
            "Epoch 430/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1781 - val_loss: 0.2735\n",
            "Epoch 431/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1779 - val_loss: 0.2737\n",
            "Epoch 432/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1780 - val_loss: 0.2737\n",
            "Epoch 433/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1778 - val_loss: 0.2736\n",
            "Epoch 434/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1778 - val_loss: 0.2736\n",
            "Epoch 435/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1777 - val_loss: 0.2737\n",
            "Epoch 436/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1778 - val_loss: 0.2738\n",
            "Epoch 437/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1774 - val_loss: 0.2742\n",
            "Epoch 438/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1777 - val_loss: 0.2738\n",
            "Epoch 439/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1775 - val_loss: 0.2739\n",
            "Epoch 440/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1773 - val_loss: 0.2737\n",
            "Epoch 441/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1772 - val_loss: 0.2737\n",
            "Epoch 442/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1770 - val_loss: 0.2738\n",
            "Epoch 443/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1772 - val_loss: 0.2739\n",
            "Epoch 444/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1769 - val_loss: 0.2738\n",
            "Epoch 445/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1769 - val_loss: 0.2738\n",
            "Epoch 446/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1768 - val_loss: 0.2739\n",
            "Epoch 447/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1767 - val_loss: 0.2739\n",
            "Epoch 448/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1765 - val_loss: 0.2739\n",
            "Epoch 449/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1764 - val_loss: 0.2739\n",
            "Epoch 450/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1763 - val_loss: 0.2740\n",
            "Epoch 451/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1764 - val_loss: 0.2740\n",
            "Epoch 452/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1762 - val_loss: 0.2739\n",
            "Epoch 453/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1761 - val_loss: 0.2743\n",
            "Epoch 454/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1762 - val_loss: 0.2742\n",
            "Epoch 455/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1761 - val_loss: 0.2747\n",
            "Epoch 456/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1761 - val_loss: 0.2741\n",
            "Epoch 457/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1759 - val_loss: 0.2740\n",
            "Epoch 458/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1756 - val_loss: 0.2741\n",
            "Epoch 459/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1757 - val_loss: 0.2741\n",
            "Epoch 460/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1756 - val_loss: 0.2742\n",
            "Epoch 461/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1757 - val_loss: 0.2744\n",
            "Epoch 462/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1755 - val_loss: 0.2743\n",
            "Epoch 463/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1755 - val_loss: 0.2744\n",
            "Epoch 464/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1752 - val_loss: 0.2742\n",
            "Epoch 465/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1752 - val_loss: 0.2744\n",
            "Epoch 466/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1751 - val_loss: 0.2744\n",
            "Epoch 467/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1750 - val_loss: 0.2742\n",
            "Epoch 468/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1750 - val_loss: 0.2747\n",
            "Epoch 469/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1750 - val_loss: 0.2744\n",
            "Epoch 470/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1747 - val_loss: 0.2744\n",
            "Epoch 471/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1748 - val_loss: 0.2744\n",
            "Epoch 472/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1746 - val_loss: 0.2748\n",
            "Epoch 473/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1747 - val_loss: 0.2744\n",
            "Epoch 474/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1745 - val_loss: 0.2746\n",
            "Epoch 475/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1744 - val_loss: 0.2745\n",
            "Epoch 476/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1744 - val_loss: 0.2748\n",
            "Epoch 477/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1744 - val_loss: 0.2745\n",
            "Epoch 478/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1742 - val_loss: 0.2745\n",
            "Epoch 479/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1740 - val_loss: 0.2745\n",
            "Epoch 480/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1741 - val_loss: 0.2746\n",
            "Epoch 481/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1740 - val_loss: 0.2746\n",
            "Epoch 482/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1740 - val_loss: 0.2747\n",
            "Epoch 483/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1739 - val_loss: 0.2748\n",
            "Epoch 484/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1737 - val_loss: 0.2747\n",
            "Epoch 485/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1737 - val_loss: 0.2748\n",
            "Epoch 486/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1737 - val_loss: 0.2748\n",
            "Epoch 487/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1734 - val_loss: 0.2747\n",
            "Epoch 488/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1735 - val_loss: 0.2748\n",
            "Epoch 489/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1734 - val_loss: 0.2748\n",
            "Epoch 490/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1735 - val_loss: 0.2748\n",
            "Epoch 491/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1733 - val_loss: 0.2750\n",
            "Epoch 492/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1733 - val_loss: 0.2748\n",
            "Epoch 493/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1731 - val_loss: 0.2748\n",
            "Epoch 494/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1731 - val_loss: 0.2750\n",
            "Epoch 495/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1730 - val_loss: 0.2749\n",
            "Epoch 496/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1729 - val_loss: 0.2749\n",
            "Epoch 497/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1727 - val_loss: 0.2749\n",
            "Epoch 498/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1727 - val_loss: 0.2750\n",
            "Epoch 499/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1728 - val_loss: 0.2753\n",
            "Epoch 500/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1727 - val_loss: 0.2750\n",
            "Epoch 501/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1726 - val_loss: 0.2751\n",
            "Epoch 502/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1725 - val_loss: 0.2751\n",
            "Epoch 503/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1725 - val_loss: 0.2752\n",
            "Epoch 504/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1723 - val_loss: 0.2752\n",
            "Epoch 505/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1724 - val_loss: 0.2750\n",
            "Epoch 506/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1722 - val_loss: 0.2750\n",
            "Epoch 507/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1721 - val_loss: 0.2752\n",
            "Epoch 508/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1723 - val_loss: 0.2752\n",
            "Epoch 509/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1720 - val_loss: 0.2753\n",
            "Epoch 510/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1719 - val_loss: 0.2752\n",
            "Epoch 511/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1720 - val_loss: 0.2754\n",
            "Epoch 512/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1718 - val_loss: 0.2753\n",
            "Epoch 513/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1717 - val_loss: 0.2752\n",
            "Epoch 514/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1716 - val_loss: 0.2753\n",
            "Epoch 515/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1717 - val_loss: 0.2753\n",
            "Epoch 516/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1716 - val_loss: 0.2754\n",
            "Epoch 517/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1714 - val_loss: 0.2754\n",
            "Epoch 518/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1713 - val_loss: 0.2754\n",
            "Epoch 519/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1715 - val_loss: 0.2754\n",
            "Epoch 520/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1713 - val_loss: 0.2760\n",
            "Epoch 521/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1714 - val_loss: 0.2755\n",
            "Epoch 522/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1711 - val_loss: 0.2755\n",
            "Epoch 523/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1710 - val_loss: 0.2755\n",
            "Epoch 524/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1710 - val_loss: 0.2756\n",
            "Epoch 525/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1710 - val_loss: 0.2758\n",
            "Epoch 526/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1711 - val_loss: 0.2761\n",
            "Epoch 527/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1710 - val_loss: 0.2755\n",
            "Epoch 528/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1708 - val_loss: 0.2759\n",
            "Epoch 529/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1707 - val_loss: 0.2756\n",
            "Epoch 530/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1707 - val_loss: 0.2756\n",
            "Epoch 531/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1706 - val_loss: 0.2757\n",
            "Epoch 532/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1705 - val_loss: 0.2758\n",
            "Epoch 533/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1705 - val_loss: 0.2759\n",
            "Epoch 534/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1705 - val_loss: 0.2758\n",
            "Epoch 535/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1703 - val_loss: 0.2757\n",
            "Epoch 536/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1704 - val_loss: 0.2757\n",
            "Epoch 537/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1703 - val_loss: 0.2758\n",
            "Epoch 538/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1702 - val_loss: 0.2759\n",
            "Epoch 539/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1702 - val_loss: 0.2758\n",
            "Epoch 540/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1701 - val_loss: 0.2759\n",
            "Epoch 541/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1703 - val_loss: 0.2760\n",
            "Epoch 542/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1700 - val_loss: 0.2759\n",
            "Epoch 543/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1699 - val_loss: 0.2759\n",
            "Epoch 544/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1697 - val_loss: 0.2761\n",
            "Epoch 545/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1697 - val_loss: 0.2760\n",
            "Epoch 546/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1696 - val_loss: 0.2759\n",
            "Epoch 547/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1696 - val_loss: 0.2760\n",
            "Epoch 548/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1696 - val_loss: 0.2761\n",
            "Epoch 549/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1695 - val_loss: 0.2762\n",
            "Epoch 550/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1696 - val_loss: 0.2763\n",
            "Epoch 551/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1695 - val_loss: 0.2761\n",
            "Epoch 552/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1694 - val_loss: 0.2762\n",
            "Epoch 553/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1692 - val_loss: 0.2762\n",
            "Epoch 554/1000\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.1691 - val_loss: 0.2763\n",
            "Epoch 555/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1691 - val_loss: 0.2764\n",
            "Epoch 556/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1692 - val_loss: 0.2766\n",
            "Epoch 557/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1690 - val_loss: 0.2763\n",
            "Epoch 558/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1690 - val_loss: 0.2763\n",
            "Epoch 559/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1690 - val_loss: 0.2763\n",
            "Epoch 560/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1689 - val_loss: 0.2764\n",
            "Epoch 561/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1690 - val_loss: 0.2765\n",
            "Epoch 562/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1688 - val_loss: 0.2764\n",
            "Epoch 563/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1690 - val_loss: 0.2765\n",
            "Epoch 564/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1686 - val_loss: 0.2764\n",
            "Epoch 565/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1686 - val_loss: 0.2765\n",
            "Epoch 566/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1686 - val_loss: 0.2767\n",
            "Epoch 567/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1685 - val_loss: 0.2766\n",
            "Epoch 568/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1685 - val_loss: 0.2765\n",
            "Epoch 569/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1684 - val_loss: 0.2765\n",
            "Epoch 570/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1683 - val_loss: 0.2765\n",
            "Epoch 571/1000\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.1683 - val_loss: 0.2765\n",
            "Epoch 572/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1682 - val_loss: 0.2765\n",
            "Epoch 573/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1682 - val_loss: 0.2770\n",
            "Epoch 574/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1683 - val_loss: 0.2767\n",
            "Epoch 575/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1680 - val_loss: 0.2767\n",
            "Epoch 576/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1681 - val_loss: 0.2767\n",
            "Epoch 577/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1680 - val_loss: 0.2767\n",
            "Epoch 578/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1679 - val_loss: 0.2769\n",
            "Epoch 579/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1678 - val_loss: 0.2767\n",
            "Epoch 580/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1679 - val_loss: 0.2772\n",
            "Epoch 581/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1680 - val_loss: 0.2768\n",
            "Epoch 582/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1676 - val_loss: 0.2767\n",
            "Epoch 583/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1676 - val_loss: 0.2768\n",
            "Epoch 584/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1678 - val_loss: 0.2771\n",
            "Epoch 585/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1675 - val_loss: 0.2769\n",
            "Epoch 586/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1675 - val_loss: 0.2769\n",
            "Epoch 587/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1674 - val_loss: 0.2771\n",
            "Epoch 588/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1674 - val_loss: 0.2770\n",
            "Epoch 589/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1673 - val_loss: 0.2770\n",
            "Epoch 590/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1673 - val_loss: 0.2770\n",
            "Epoch 591/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1672 - val_loss: 0.2770\n",
            "Epoch 592/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1672 - val_loss: 0.2773\n",
            "Epoch 593/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1672 - val_loss: 0.2772\n",
            "Epoch 594/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1670 - val_loss: 0.2770\n",
            "Epoch 595/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1671 - val_loss: 0.2774\n",
            "Epoch 596/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1670 - val_loss: 0.2774\n",
            "Epoch 597/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1669 - val_loss: 0.2771\n",
            "Epoch 598/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1670 - val_loss: 0.2771\n",
            "Epoch 599/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1667 - val_loss: 0.2772\n",
            "Epoch 600/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1668 - val_loss: 0.2775\n",
            "Epoch 601/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1667 - val_loss: 0.2773\n",
            "Epoch 602/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1665 - val_loss: 0.2772\n",
            "Epoch 603/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1668 - val_loss: 0.2773\n",
            "Epoch 604/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1666 - val_loss: 0.2774\n",
            "Epoch 605/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1664 - val_loss: 0.2774\n",
            "Epoch 606/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1664 - val_loss: 0.2774\n",
            "Epoch 607/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1664 - val_loss: 0.2775\n",
            "Epoch 608/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1665 - val_loss: 0.2774\n",
            "Epoch 609/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1662 - val_loss: 0.2774\n",
            "Epoch 610/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1664 - val_loss: 0.2775\n",
            "Epoch 611/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1663 - val_loss: 0.2777\n",
            "Epoch 612/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1662 - val_loss: 0.2776\n",
            "Epoch 613/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1661 - val_loss: 0.2775\n",
            "Epoch 614/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1662 - val_loss: 0.2777\n",
            "Epoch 615/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1660 - val_loss: 0.2778\n",
            "Epoch 616/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1659 - val_loss: 0.2776\n",
            "Epoch 617/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1659 - val_loss: 0.2776\n",
            "Epoch 618/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1659 - val_loss: 0.2777\n",
            "Epoch 619/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1660 - val_loss: 0.2779\n",
            "Epoch 620/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1658 - val_loss: 0.2780\n",
            "Epoch 621/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1658 - val_loss: 0.2777\n",
            "Epoch 622/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1655 - val_loss: 0.2776\n",
            "Epoch 623/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1656 - val_loss: 0.2777\n",
            "Epoch 624/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1656 - val_loss: 0.2780\n",
            "Epoch 625/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1656 - val_loss: 0.2777\n",
            "Epoch 626/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1656 - val_loss: 0.2778\n",
            "Epoch 627/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1654 - val_loss: 0.2779\n",
            "Epoch 628/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1653 - val_loss: 0.2781\n",
            "Epoch 629/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1655 - val_loss: 0.2779\n",
            "Epoch 630/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1653 - val_loss: 0.2781\n",
            "Epoch 631/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1654 - val_loss: 0.2780\n",
            "Epoch 632/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1652 - val_loss: 0.2779\n",
            "Epoch 633/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1652 - val_loss: 0.2780\n",
            "Epoch 634/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1652 - val_loss: 0.2781\n",
            "Epoch 635/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1650 - val_loss: 0.2781\n",
            "Epoch 636/1000\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.1650 - val_loss: 0.2780\n",
            "Epoch 637/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1651 - val_loss: 0.2780\n",
            "Epoch 638/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1651 - val_loss: 0.2781\n",
            "Epoch 639/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1648 - val_loss: 0.2781\n",
            "Epoch 640/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1650 - val_loss: 0.2785\n",
            "Epoch 641/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1649 - val_loss: 0.2782\n",
            "Epoch 642/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1648 - val_loss: 0.2782\n",
            "Epoch 643/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1647 - val_loss: 0.2782\n",
            "Epoch 644/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1646 - val_loss: 0.2782\n",
            "Epoch 645/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1645 - val_loss: 0.2782\n",
            "Epoch 646/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1647 - val_loss: 0.2782\n",
            "Epoch 647/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1648 - val_loss: 0.2787\n",
            "Epoch 648/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1646 - val_loss: 0.2785\n",
            "Epoch 649/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1645 - val_loss: 0.2783\n",
            "Epoch 650/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1645 - val_loss: 0.2786\n",
            "Epoch 651/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1644 - val_loss: 0.2785\n",
            "Epoch 652/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1643 - val_loss: 0.2785\n",
            "Epoch 653/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1642 - val_loss: 0.2784\n",
            "Epoch 654/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1644 - val_loss: 0.2784\n",
            "Epoch 655/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1642 - val_loss: 0.2787\n",
            "Epoch 656/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1643 - val_loss: 0.2785\n",
            "Epoch 657/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1642 - val_loss: 0.2785\n",
            "Epoch 658/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1641 - val_loss: 0.2785\n",
            "Epoch 659/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1640 - val_loss: 0.2788\n",
            "Epoch 660/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1641 - val_loss: 0.2785\n",
            "Epoch 661/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1640 - val_loss: 0.2787\n",
            "Epoch 662/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1640 - val_loss: 0.2788\n",
            "Epoch 663/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1640 - val_loss: 0.2789\n",
            "Epoch 664/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1639 - val_loss: 0.2786\n",
            "Epoch 665/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1639 - val_loss: 0.2786\n",
            "Epoch 666/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1638 - val_loss: 0.2788\n",
            "Epoch 667/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1637 - val_loss: 0.2787\n",
            "Epoch 668/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1637 - val_loss: 0.2791\n",
            "Epoch 669/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1639 - val_loss: 0.2789\n",
            "Epoch 670/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1637 - val_loss: 0.2788\n",
            "Epoch 671/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1636 - val_loss: 0.2788\n",
            "Epoch 672/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1635 - val_loss: 0.2788\n",
            "Epoch 673/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1636 - val_loss: 0.2790\n",
            "Epoch 674/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1635 - val_loss: 0.2792\n",
            "Epoch 675/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1635 - val_loss: 0.2788\n",
            "Epoch 676/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1634 - val_loss: 0.2791\n",
            "Epoch 677/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1633 - val_loss: 0.2789\n",
            "Epoch 678/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1633 - val_loss: 0.2789\n",
            "Epoch 679/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1632 - val_loss: 0.2791\n",
            "Epoch 680/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1632 - val_loss: 0.2792\n",
            "Epoch 681/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1634 - val_loss: 0.2791\n",
            "Epoch 682/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1632 - val_loss: 0.2791\n",
            "Epoch 683/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1631 - val_loss: 0.2790\n",
            "Epoch 684/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1632 - val_loss: 0.2791\n",
            "Epoch 685/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1630 - val_loss: 0.2790\n",
            "Epoch 686/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1631 - val_loss: 0.2791\n",
            "Epoch 687/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1630 - val_loss: 0.2795\n",
            "Epoch 688/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1632 - val_loss: 0.2794\n",
            "Epoch 689/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1630 - val_loss: 0.2792\n",
            "Epoch 690/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1628 - val_loss: 0.2792\n",
            "Epoch 691/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1627 - val_loss: 0.2793\n",
            "Epoch 692/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1627 - val_loss: 0.2793\n",
            "Epoch 693/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1628 - val_loss: 0.2795\n",
            "Epoch 694/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1629 - val_loss: 0.2794\n",
            "Epoch 695/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1626 - val_loss: 0.2793\n",
            "Epoch 696/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1625 - val_loss: 0.2794\n",
            "Epoch 697/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1625 - val_loss: 0.2793\n",
            "Epoch 698/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1626 - val_loss: 0.2798\n",
            "Epoch 699/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1626 - val_loss: 0.2795\n",
            "Epoch 700/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1626 - val_loss: 0.2794\n",
            "Epoch 701/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1625 - val_loss: 0.2795\n",
            "Epoch 702/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1623 - val_loss: 0.2796\n",
            "Epoch 703/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1625 - val_loss: 0.2796\n",
            "Epoch 704/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1623 - val_loss: 0.2796\n",
            "Epoch 705/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1623 - val_loss: 0.2796\n",
            "Epoch 706/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1622 - val_loss: 0.2796\n",
            "Epoch 707/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1622 - val_loss: 0.2797\n",
            "Epoch 708/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1622 - val_loss: 0.2796\n",
            "Epoch 709/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1621 - val_loss: 0.2798\n",
            "Epoch 710/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1622 - val_loss: 0.2797\n",
            "Epoch 711/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1620 - val_loss: 0.2797\n",
            "Epoch 712/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1621 - val_loss: 0.2797\n",
            "Epoch 713/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1619 - val_loss: 0.2798\n",
            "Epoch 714/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1619 - val_loss: 0.2799\n",
            "Epoch 715/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1620 - val_loss: 0.2798\n",
            "Epoch 716/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1618 - val_loss: 0.2798\n",
            "Epoch 717/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1620 - val_loss: 0.2799\n",
            "Epoch 718/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1619 - val_loss: 0.2800\n",
            "Epoch 719/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1619 - val_loss: 0.2802\n",
            "Epoch 720/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1619 - val_loss: 0.2799\n",
            "Epoch 721/1000\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.1618 - val_loss: 0.2799\n",
            "Epoch 722/1000\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.1618 - val_loss: 0.2800\n",
            "Epoch 723/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1617 - val_loss: 0.2800\n",
            "Epoch 724/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1617 - val_loss: 0.2800\n",
            "Epoch 725/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1615 - val_loss: 0.2801\n",
            "Epoch 726/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1615 - val_loss: 0.2802\n",
            "Epoch 727/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1614 - val_loss: 0.2801\n",
            "Epoch 728/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1616 - val_loss: 0.2800\n",
            "Epoch 729/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1615 - val_loss: 0.2801\n",
            "Epoch 730/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1615 - val_loss: 0.2801\n",
            "Epoch 731/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1613 - val_loss: 0.2801\n",
            "Epoch 732/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1613 - val_loss: 0.2802\n",
            "Epoch 733/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1614 - val_loss: 0.2805\n",
            "Epoch 734/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1613 - val_loss: 0.2801\n",
            "Epoch 735/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1611 - val_loss: 0.2802\n",
            "Epoch 736/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1614 - val_loss: 0.2803\n",
            "Epoch 737/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1612 - val_loss: 0.2804\n",
            "Epoch 738/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1612 - val_loss: 0.2802\n",
            "Epoch 739/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1611 - val_loss: 0.2803\n",
            "Epoch 740/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1609 - val_loss: 0.2805\n",
            "Epoch 741/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1611 - val_loss: 0.2804\n",
            "Epoch 742/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1611 - val_loss: 0.2804\n",
            "Epoch 743/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1610 - val_loss: 0.2803\n",
            "Epoch 744/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1609 - val_loss: 0.2806\n",
            "Epoch 745/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1609 - val_loss: 0.2804\n",
            "Epoch 746/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1610 - val_loss: 0.2807\n",
            "Epoch 747/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1610 - val_loss: 0.2805\n",
            "Epoch 748/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1608 - val_loss: 0.2808\n",
            "Epoch 749/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1608 - val_loss: 0.2805\n",
            "Epoch 750/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1607 - val_loss: 0.2805\n",
            "Epoch 751/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1607 - val_loss: 0.2806\n",
            "Epoch 752/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1607 - val_loss: 0.2805\n",
            "Epoch 753/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1606 - val_loss: 0.2806\n",
            "Epoch 754/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1608 - val_loss: 0.2806\n",
            "Epoch 755/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1606 - val_loss: 0.2807\n",
            "Epoch 756/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1607 - val_loss: 0.2807\n",
            "Epoch 757/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1605 - val_loss: 0.2810\n",
            "Epoch 758/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1604 - val_loss: 0.2808\n",
            "Epoch 759/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1604 - val_loss: 0.2808\n",
            "Epoch 760/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1604 - val_loss: 0.2808\n",
            "Epoch 761/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1604 - val_loss: 0.2808\n",
            "Epoch 762/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1603 - val_loss: 0.2807\n",
            "Epoch 763/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1604 - val_loss: 0.2808\n",
            "Epoch 764/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1604 - val_loss: 0.2808\n",
            "Epoch 765/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1602 - val_loss: 0.2808\n",
            "Epoch 766/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1601 - val_loss: 0.2809\n",
            "Epoch 767/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1602 - val_loss: 0.2809\n",
            "Epoch 768/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1603 - val_loss: 0.2811\n",
            "Epoch 769/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1603 - val_loss: 0.2809\n",
            "Epoch 770/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1601 - val_loss: 0.2811\n",
            "Epoch 771/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1601 - val_loss: 0.2810\n",
            "Epoch 772/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1601 - val_loss: 0.2810\n",
            "Epoch 773/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1602 - val_loss: 0.2810\n",
            "Epoch 774/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1601 - val_loss: 0.2810\n",
            "Epoch 775/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1600 - val_loss: 0.2812\n",
            "Epoch 776/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1600 - val_loss: 0.2810\n",
            "Epoch 777/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1598 - val_loss: 0.2810\n",
            "Epoch 778/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1599 - val_loss: 0.2811\n",
            "Epoch 779/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1597 - val_loss: 0.2812\n",
            "Epoch 780/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1598 - val_loss: 0.2813\n",
            "Epoch 781/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1598 - val_loss: 0.2813\n",
            "Epoch 782/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1598 - val_loss: 0.2812\n",
            "Epoch 783/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1597 - val_loss: 0.2814\n",
            "Epoch 784/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1598 - val_loss: 0.2813\n",
            "Epoch 785/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1596 - val_loss: 0.2812\n",
            "Epoch 786/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1596 - val_loss: 0.2815\n",
            "Epoch 787/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1597 - val_loss: 0.2814\n",
            "Epoch 788/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1597 - val_loss: 0.2816\n",
            "Epoch 789/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1596 - val_loss: 0.2813\n",
            "Epoch 790/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1595 - val_loss: 0.2814\n",
            "Epoch 791/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1595 - val_loss: 0.2814\n",
            "Epoch 792/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1594 - val_loss: 0.2814\n",
            "Epoch 793/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1595 - val_loss: 0.2816\n",
            "Epoch 794/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1594 - val_loss: 0.2815\n",
            "Epoch 795/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1594 - val_loss: 0.2815\n",
            "Epoch 796/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1593 - val_loss: 0.2815\n",
            "Epoch 797/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1593 - val_loss: 0.2815\n",
            "Epoch 798/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1593 - val_loss: 0.2816\n",
            "Epoch 799/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1592 - val_loss: 0.2816\n",
            "Epoch 800/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1591 - val_loss: 0.2816\n",
            "Epoch 801/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1592 - val_loss: 0.2818\n",
            "Epoch 802/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1592 - val_loss: 0.2817\n",
            "Epoch 803/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1592 - val_loss: 0.2817\n",
            "Epoch 804/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1592 - val_loss: 0.2817\n",
            "Epoch 805/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1591 - val_loss: 0.2817\n",
            "Epoch 806/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1591 - val_loss: 0.2817\n",
            "Epoch 807/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1591 - val_loss: 0.2817\n",
            "Epoch 808/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1591 - val_loss: 0.2818\n",
            "Epoch 809/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1589 - val_loss: 0.2818\n",
            "Epoch 810/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1588 - val_loss: 0.2817\n",
            "Epoch 811/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1589 - val_loss: 0.2820\n",
            "Epoch 812/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1590 - val_loss: 0.2819\n",
            "Epoch 813/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1588 - val_loss: 0.2819\n",
            "Epoch 814/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1588 - val_loss: 0.2819\n",
            "Epoch 815/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1588 - val_loss: 0.2819\n",
            "Epoch 816/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1587 - val_loss: 0.2819\n",
            "Epoch 817/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1587 - val_loss: 0.2820\n",
            "Epoch 818/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1587 - val_loss: 0.2821\n",
            "Epoch 819/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1586 - val_loss: 0.2820\n",
            "Epoch 820/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1588 - val_loss: 0.2821\n",
            "Epoch 821/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1587 - val_loss: 0.2822\n",
            "Epoch 822/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1586 - val_loss: 0.2820\n",
            "Epoch 823/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1586 - val_loss: 0.2822\n",
            "Epoch 824/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1586 - val_loss: 0.2820\n",
            "Epoch 825/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1585 - val_loss: 0.2822\n",
            "Epoch 826/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1586 - val_loss: 0.2821\n",
            "Epoch 827/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1585 - val_loss: 0.2823\n",
            "Epoch 828/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1584 - val_loss: 0.2824\n",
            "Epoch 829/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1586 - val_loss: 0.2822\n",
            "Epoch 830/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1584 - val_loss: 0.2822\n",
            "Epoch 831/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1583 - val_loss: 0.2822\n",
            "Epoch 832/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1584 - val_loss: 0.2823\n",
            "Epoch 833/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1584 - val_loss: 0.2822\n",
            "Epoch 834/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1583 - val_loss: 0.2824\n",
            "Epoch 835/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1582 - val_loss: 0.2824\n",
            "Epoch 836/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1583 - val_loss: 0.2824\n",
            "Epoch 837/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1582 - val_loss: 0.2826\n",
            "Epoch 838/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1581 - val_loss: 0.2824\n",
            "Epoch 839/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1581 - val_loss: 0.2824\n",
            "Epoch 840/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1582 - val_loss: 0.2826\n",
            "Epoch 841/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1583 - val_loss: 0.2826\n",
            "Epoch 842/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1580 - val_loss: 0.2825\n",
            "Epoch 843/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1581 - val_loss: 0.2825\n",
            "Epoch 844/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1581 - val_loss: 0.2826\n",
            "Epoch 845/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1580 - val_loss: 0.2826\n",
            "Epoch 846/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1580 - val_loss: 0.2826\n",
            "Epoch 847/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1580 - val_loss: 0.2826\n",
            "Epoch 848/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1581 - val_loss: 0.2826\n",
            "Epoch 849/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1578 - val_loss: 0.2826\n",
            "Epoch 850/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1579 - val_loss: 0.2827\n",
            "Epoch 851/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1580 - val_loss: 0.2827\n",
            "Epoch 852/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1578 - val_loss: 0.2827\n",
            "Epoch 853/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1578 - val_loss: 0.2826\n",
            "Epoch 854/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1578 - val_loss: 0.2829\n",
            "Epoch 855/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1577 - val_loss: 0.2830\n",
            "Epoch 856/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1577 - val_loss: 0.2828\n",
            "Epoch 857/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1576 - val_loss: 0.2827\n",
            "Epoch 858/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1578 - val_loss: 0.2829\n",
            "Epoch 859/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1577 - val_loss: 0.2829\n",
            "Epoch 860/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1577 - val_loss: 0.2828\n",
            "Epoch 861/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1577 - val_loss: 0.2829\n",
            "Epoch 862/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1576 - val_loss: 0.2829\n",
            "Epoch 863/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1576 - val_loss: 0.2829\n",
            "Epoch 864/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1575 - val_loss: 0.2829\n",
            "Epoch 865/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1574 - val_loss: 0.2830\n",
            "Epoch 866/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1577 - val_loss: 0.2829\n",
            "Epoch 867/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1575 - val_loss: 0.2831\n",
            "Epoch 868/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1574 - val_loss: 0.2830\n",
            "Epoch 869/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1574 - val_loss: 0.2831\n",
            "Epoch 870/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1574 - val_loss: 0.2830\n",
            "Epoch 871/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1573 - val_loss: 0.2831\n",
            "Epoch 872/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1573 - val_loss: 0.2831\n",
            "Epoch 873/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1573 - val_loss: 0.2831\n",
            "Epoch 874/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1572 - val_loss: 0.2833\n",
            "Epoch 875/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1575 - val_loss: 0.2832\n",
            "Epoch 876/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1574 - val_loss: 0.2832\n",
            "Epoch 877/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1572 - val_loss: 0.2833\n",
            "Epoch 878/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1572 - val_loss: 0.2832\n",
            "Epoch 879/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1571 - val_loss: 0.2834\n",
            "Epoch 880/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1571 - val_loss: 0.2833\n",
            "Epoch 881/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1570 - val_loss: 0.2833\n",
            "Epoch 882/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1571 - val_loss: 0.2833\n",
            "Epoch 883/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1570 - val_loss: 0.2833\n",
            "Epoch 884/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1571 - val_loss: 0.2834\n",
            "Epoch 885/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1571 - val_loss: 0.2834\n",
            "Epoch 886/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1570 - val_loss: 0.2833\n",
            "Epoch 887/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1569 - val_loss: 0.2834\n",
            "Epoch 888/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1571 - val_loss: 0.2835\n",
            "Epoch 889/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1569 - val_loss: 0.2835\n",
            "Epoch 890/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1569 - val_loss: 0.2835\n",
            "Epoch 891/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1568 - val_loss: 0.2837\n",
            "Epoch 892/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1570 - val_loss: 0.2835\n",
            "Epoch 893/1000\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.1569 - val_loss: 0.2835\n",
            "Epoch 894/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1569 - val_loss: 0.2836\n",
            "Epoch 895/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1567 - val_loss: 0.2836\n",
            "Epoch 896/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1567 - val_loss: 0.2836\n",
            "Epoch 897/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1568 - val_loss: 0.2836\n",
            "Epoch 898/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1567 - val_loss: 0.2836\n",
            "Epoch 899/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1567 - val_loss: 0.2837\n",
            "Epoch 900/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1567 - val_loss: 0.2836\n",
            "Epoch 901/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1566 - val_loss: 0.2836\n",
            "Epoch 902/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1566 - val_loss: 0.2838\n",
            "Epoch 903/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1565 - val_loss: 0.2838\n",
            "Epoch 904/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1568 - val_loss: 0.2841\n",
            "Epoch 905/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1566 - val_loss: 0.2838\n",
            "Epoch 906/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1566 - val_loss: 0.2837\n",
            "Epoch 907/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1565 - val_loss: 0.2839\n",
            "Epoch 908/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1566 - val_loss: 0.2838\n",
            "Epoch 909/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1565 - val_loss: 0.2839\n",
            "Epoch 910/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1564 - val_loss: 0.2839\n",
            "Epoch 911/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1564 - val_loss: 0.2838\n",
            "Epoch 912/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1564 - val_loss: 0.2840\n",
            "Epoch 913/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1565 - val_loss: 0.2839\n",
            "Epoch 914/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1565 - val_loss: 0.2839\n",
            "Epoch 915/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1562 - val_loss: 0.2843\n",
            "Epoch 916/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1563 - val_loss: 0.2840\n",
            "Epoch 917/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1563 - val_loss: 0.2839\n",
            "Epoch 918/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1561 - val_loss: 0.2840\n",
            "Epoch 919/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1564 - val_loss: 0.2840\n",
            "Epoch 920/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1563 - val_loss: 0.2840\n",
            "Epoch 921/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1561 - val_loss: 0.2842\n",
            "Epoch 922/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1562 - val_loss: 0.2843\n",
            "Epoch 923/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1561 - val_loss: 0.2841\n",
            "Epoch 924/1000\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.1562 - val_loss: 0.2844\n",
            "Epoch 925/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1562 - val_loss: 0.2843\n",
            "Epoch 926/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1560 - val_loss: 0.2842\n",
            "Epoch 927/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1562 - val_loss: 0.2842\n",
            "Epoch 928/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1560 - val_loss: 0.2843\n",
            "Epoch 929/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1562 - val_loss: 0.2843\n",
            "Epoch 930/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1559 - val_loss: 0.2842\n",
            "Epoch 931/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1560 - val_loss: 0.2842\n",
            "Epoch 932/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1560 - val_loss: 0.2845\n",
            "Epoch 933/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1560 - val_loss: 0.2842\n",
            "Epoch 934/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1559 - val_loss: 0.2843\n",
            "Epoch 935/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1560 - val_loss: 0.2844\n",
            "Epoch 936/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1559 - val_loss: 0.2844\n",
            "Epoch 937/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1560 - val_loss: 0.2844\n",
            "Epoch 938/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1559 - val_loss: 0.2844\n",
            "Epoch 939/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1557 - val_loss: 0.2844\n",
            "Epoch 940/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1558 - val_loss: 0.2844\n",
            "Epoch 941/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1558 - val_loss: 0.2844\n",
            "Epoch 942/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1558 - val_loss: 0.2846\n",
            "Epoch 943/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1558 - val_loss: 0.2844\n",
            "Epoch 944/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1557 - val_loss: 0.2845\n",
            "Epoch 945/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1557 - val_loss: 0.2845\n",
            "Epoch 946/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1558 - val_loss: 0.2846\n",
            "Epoch 947/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1556 - val_loss: 0.2845\n",
            "Epoch 948/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1556 - val_loss: 0.2846\n",
            "Epoch 949/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1557 - val_loss: 0.2846\n",
            "Epoch 950/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1556 - val_loss: 0.2846\n",
            "Epoch 951/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1556 - val_loss: 0.2846\n",
            "Epoch 952/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1556 - val_loss: 0.2847\n",
            "Epoch 953/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1556 - val_loss: 0.2847\n",
            "Epoch 954/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1554 - val_loss: 0.2847\n",
            "Epoch 955/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1556 - val_loss: 0.2847\n",
            "Epoch 956/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1554 - val_loss: 0.2847\n",
            "Epoch 957/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1555 - val_loss: 0.2848\n",
            "Epoch 958/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1554 - val_loss: 0.2849\n",
            "Epoch 959/1000\n",
            "33/33 [==============================] - 1s 25ms/step - loss: 0.1554 - val_loss: 0.2849\n",
            "Epoch 960/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1554 - val_loss: 0.2848\n",
            "Epoch 961/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1555 - val_loss: 0.2850\n",
            "Epoch 962/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1553 - val_loss: 0.2849\n",
            "Epoch 963/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1553 - val_loss: 0.2848\n",
            "Epoch 964/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1555 - val_loss: 0.2852\n",
            "Epoch 965/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1554 - val_loss: 0.2849\n",
            "Epoch 966/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1553 - val_loss: 0.2849\n",
            "Epoch 967/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1553 - val_loss: 0.2850\n",
            "Epoch 968/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1552 - val_loss: 0.2850\n",
            "Epoch 969/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1552 - val_loss: 0.2850\n",
            "Epoch 970/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1553 - val_loss: 0.2851\n",
            "Epoch 971/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1552 - val_loss: 0.2851\n",
            "Epoch 972/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1551 - val_loss: 0.2850\n",
            "Epoch 973/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1552 - val_loss: 0.2851\n",
            "Epoch 974/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1552 - val_loss: 0.2853\n",
            "Epoch 975/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1551 - val_loss: 0.2853\n",
            "Epoch 976/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1552 - val_loss: 0.2851\n",
            "Epoch 977/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1552 - val_loss: 0.2854\n",
            "Epoch 978/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1551 - val_loss: 0.2854\n",
            "Epoch 979/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1552 - val_loss: 0.2852\n",
            "Epoch 980/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1550 - val_loss: 0.2852\n",
            "Epoch 981/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1550 - val_loss: 0.2852\n",
            "Epoch 982/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1551 - val_loss: 0.2852\n",
            "Epoch 983/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1549 - val_loss: 0.2852\n",
            "Epoch 984/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1549 - val_loss: 0.2853\n",
            "Epoch 985/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1549 - val_loss: 0.2854\n",
            "Epoch 986/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1551 - val_loss: 0.2854\n",
            "Epoch 987/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1548 - val_loss: 0.2853\n",
            "Epoch 988/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1550 - val_loss: 0.2853\n",
            "Epoch 989/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1548 - val_loss: 0.2854\n",
            "Epoch 990/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1549 - val_loss: 0.2853\n",
            "Epoch 991/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1548 - val_loss: 0.2856\n",
            "Epoch 992/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1550 - val_loss: 0.2854\n",
            "Epoch 993/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1547 - val_loss: 0.2854\n",
            "Epoch 994/1000\n",
            "33/33 [==============================] - 1s 24ms/step - loss: 0.1547 - val_loss: 0.2855\n",
            "Epoch 995/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1547 - val_loss: 0.2856\n",
            "Epoch 996/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1547 - val_loss: 0.2855\n",
            "Epoch 997/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1547 - val_loss: 0.2854\n",
            "Epoch 998/1000\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 0.1547 - val_loss: 0.2855\n",
            "Epoch 999/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1547 - val_loss: 0.2856\n",
            "Epoch 1000/1000\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 0.1546 - val_loss: 0.2855\n",
            "3/3 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0.034874942, 0.2212694204456747, 3.6797975968037435)]\n",
            "Epoch 1/1000\n",
            "34/34 [==============================] - 2s 32ms/step - loss: 0.5850 - val_loss: 0.5477\n",
            "Epoch 2/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.4260 - val_loss: 0.5118\n",
            "Epoch 3/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.3893 - val_loss: 0.4829\n",
            "Epoch 4/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3688 - val_loss: 0.4531\n",
            "Epoch 5/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3533 - val_loss: 0.4270\n",
            "Epoch 6/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3409 - val_loss: 0.4045\n",
            "Epoch 7/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3304 - val_loss: 0.3862\n",
            "Epoch 8/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3215 - val_loss: 0.3708\n",
            "Epoch 9/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.3141 - val_loss: 0.3587\n",
            "Epoch 10/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3078 - val_loss: 0.3490\n",
            "Epoch 11/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3024 - val_loss: 0.3413\n",
            "Epoch 12/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2979 - val_loss: 0.3350\n",
            "Epoch 13/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2938 - val_loss: 0.3298\n",
            "Epoch 14/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2901 - val_loss: 0.3250\n",
            "Epoch 15/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2868 - val_loss: 0.3209\n",
            "Epoch 16/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2838 - val_loss: 0.3171\n",
            "Epoch 17/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2811 - val_loss: 0.3141\n",
            "Epoch 18/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2784 - val_loss: 0.3109\n",
            "Epoch 19/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2760 - val_loss: 0.3082\n",
            "Epoch 20/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2739 - val_loss: 0.3058\n",
            "Epoch 21/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2719 - val_loss: 0.3038\n",
            "Epoch 22/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2701 - val_loss: 0.3014\n",
            "Epoch 23/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2684 - val_loss: 0.2997\n",
            "Epoch 24/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2668 - val_loss: 0.2980\n",
            "Epoch 25/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2655 - val_loss: 0.2966\n",
            "Epoch 26/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2643 - val_loss: 0.2953\n",
            "Epoch 27/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2632 - val_loss: 0.2940\n",
            "Epoch 28/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2622 - val_loss: 0.2930\n",
            "Epoch 29/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2614 - val_loss: 0.2920\n",
            "Epoch 30/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2605 - val_loss: 0.2912\n",
            "Epoch 31/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2598 - val_loss: 0.2904\n",
            "Epoch 32/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2592 - val_loss: 0.2898\n",
            "Epoch 33/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2586 - val_loss: 0.2891\n",
            "Epoch 34/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2580 - val_loss: 0.2885\n",
            "Epoch 35/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2576 - val_loss: 0.2881\n",
            "Epoch 36/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2572 - val_loss: 0.2876\n",
            "Epoch 37/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2567 - val_loss: 0.2872\n",
            "Epoch 38/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2565 - val_loss: 0.2868\n",
            "Epoch 39/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2561 - val_loss: 0.2864\n",
            "Epoch 40/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2558 - val_loss: 0.2861\n",
            "Epoch 41/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2555 - val_loss: 0.2858\n",
            "Epoch 42/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2553 - val_loss: 0.2856\n",
            "Epoch 43/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2549 - val_loss: 0.2853\n",
            "Epoch 44/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2548 - val_loss: 0.2851\n",
            "Epoch 45/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2546 - val_loss: 0.2849\n",
            "Epoch 46/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2543 - val_loss: 0.2847\n",
            "Epoch 47/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2542 - val_loss: 0.2845\n",
            "Epoch 48/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2540 - val_loss: 0.2843\n",
            "Epoch 49/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2538 - val_loss: 0.2842\n",
            "Epoch 50/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2536 - val_loss: 0.2840\n",
            "Epoch 51/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2535 - val_loss: 0.2839\n",
            "Epoch 52/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2533 - val_loss: 0.2837\n",
            "Epoch 53/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2532 - val_loss: 0.2836\n",
            "Epoch 54/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2530 - val_loss: 0.2835\n",
            "Epoch 55/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2529 - val_loss: 0.2833\n",
            "Epoch 56/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2527 - val_loss: 0.2832\n",
            "Epoch 57/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2526 - val_loss: 0.2831\n",
            "Epoch 58/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2525 - val_loss: 0.2830\n",
            "Epoch 59/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2523 - val_loss: 0.2829\n",
            "Epoch 60/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2522 - val_loss: 0.2828\n",
            "Epoch 61/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2521 - val_loss: 0.2827\n",
            "Epoch 62/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2520 - val_loss: 0.2827\n",
            "Epoch 63/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2518 - val_loss: 0.2825\n",
            "Epoch 64/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2517 - val_loss: 0.2825\n",
            "Epoch 65/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2516 - val_loss: 0.2824\n",
            "Epoch 66/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2514 - val_loss: 0.2823\n",
            "Epoch 67/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2514 - val_loss: 0.2822\n",
            "Epoch 68/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2512 - val_loss: 0.2821\n",
            "Epoch 69/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2511 - val_loss: 0.2821\n",
            "Epoch 70/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2510 - val_loss: 0.2820\n",
            "Epoch 71/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2509 - val_loss: 0.2819\n",
            "Epoch 72/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2506 - val_loss: 0.2818\n",
            "Epoch 73/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2506 - val_loss: 0.2817\n",
            "Epoch 74/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2504 - val_loss: 0.2817\n",
            "Epoch 75/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2503 - val_loss: 0.2815\n",
            "Epoch 76/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2501 - val_loss: 0.2814\n",
            "Epoch 77/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2500 - val_loss: 0.2814\n",
            "Epoch 78/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2499 - val_loss: 0.2813\n",
            "Epoch 79/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2497 - val_loss: 0.2812\n",
            "Epoch 80/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2496 - val_loss: 0.2812\n",
            "Epoch 81/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2494 - val_loss: 0.2811\n",
            "Epoch 82/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2494 - val_loss: 0.2810\n",
            "Epoch 83/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2492 - val_loss: 0.2810\n",
            "Epoch 84/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2491 - val_loss: 0.2809\n",
            "Epoch 85/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2488 - val_loss: 0.2808\n",
            "Epoch 86/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2487 - val_loss: 0.2807\n",
            "Epoch 87/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2485 - val_loss: 0.2807\n",
            "Epoch 88/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2484 - val_loss: 0.2806\n",
            "Epoch 89/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2482 - val_loss: 0.2805\n",
            "Epoch 90/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2480 - val_loss: 0.2804\n",
            "Epoch 91/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2478 - val_loss: 0.2804\n",
            "Epoch 92/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2477 - val_loss: 0.2803\n",
            "Epoch 93/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2475 - val_loss: 0.2803\n",
            "Epoch 94/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2474 - val_loss: 0.2803\n",
            "Epoch 95/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2471 - val_loss: 0.2802\n",
            "Epoch 96/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2470 - val_loss: 0.2801\n",
            "Epoch 97/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2468 - val_loss: 0.2800\n",
            "Epoch 98/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2466 - val_loss: 0.2799\n",
            "Epoch 99/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2464 - val_loss: 0.2799\n",
            "Epoch 100/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2462 - val_loss: 0.2798\n",
            "Epoch 101/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2460 - val_loss: 0.2797\n",
            "Epoch 102/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2459 - val_loss: 0.2797\n",
            "Epoch 103/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2457 - val_loss: 0.2797\n",
            "Epoch 104/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2455 - val_loss: 0.2795\n",
            "Epoch 105/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2453 - val_loss: 0.2794\n",
            "Epoch 106/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2450 - val_loss: 0.2793\n",
            "Epoch 107/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2449 - val_loss: 0.2793\n",
            "Epoch 108/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2446 - val_loss: 0.2792\n",
            "Epoch 109/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2445 - val_loss: 0.2791\n",
            "Epoch 110/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2442 - val_loss: 0.2790\n",
            "Epoch 111/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2439 - val_loss: 0.2789\n",
            "Epoch 112/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2438 - val_loss: 0.2789\n",
            "Epoch 113/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2435 - val_loss: 0.2788\n",
            "Epoch 114/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2434 - val_loss: 0.2788\n",
            "Epoch 115/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2431 - val_loss: 0.2787\n",
            "Epoch 116/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2429 - val_loss: 0.2786\n",
            "Epoch 117/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2427 - val_loss: 0.2785\n",
            "Epoch 118/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2425 - val_loss: 0.2784\n",
            "Epoch 119/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2422 - val_loss: 0.2783\n",
            "Epoch 120/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2420 - val_loss: 0.2782\n",
            "Epoch 121/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2418 - val_loss: 0.2782\n",
            "Epoch 122/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2415 - val_loss: 0.2782\n",
            "Epoch 123/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2414 - val_loss: 0.2780\n",
            "Epoch 124/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2411 - val_loss: 0.2780\n",
            "Epoch 125/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2408 - val_loss: 0.2779\n",
            "Epoch 126/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2406 - val_loss: 0.2778\n",
            "Epoch 127/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2403 - val_loss: 0.2777\n",
            "Epoch 128/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2401 - val_loss: 0.2777\n",
            "Epoch 129/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2400 - val_loss: 0.2776\n",
            "Epoch 130/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2396 - val_loss: 0.2775\n",
            "Epoch 131/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2394 - val_loss: 0.2775\n",
            "Epoch 132/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2391 - val_loss: 0.2774\n",
            "Epoch 133/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2389 - val_loss: 0.2772\n",
            "Epoch 134/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2386 - val_loss: 0.2772\n",
            "Epoch 135/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2384 - val_loss: 0.2771\n",
            "Epoch 136/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2382 - val_loss: 0.2771\n",
            "Epoch 137/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2379 - val_loss: 0.2769\n",
            "Epoch 138/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2377 - val_loss: 0.2768\n",
            "Epoch 139/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2375 - val_loss: 0.2768\n",
            "Epoch 140/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2372 - val_loss: 0.2767\n",
            "Epoch 141/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2369 - val_loss: 0.2766\n",
            "Epoch 142/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2367 - val_loss: 0.2766\n",
            "Epoch 143/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2364 - val_loss: 0.2765\n",
            "Epoch 144/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2362 - val_loss: 0.2764\n",
            "Epoch 145/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2360 - val_loss: 0.2765\n",
            "Epoch 146/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2357 - val_loss: 0.2763\n",
            "Epoch 147/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2355 - val_loss: 0.2763\n",
            "Epoch 148/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2353 - val_loss: 0.2761\n",
            "Epoch 149/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2350 - val_loss: 0.2762\n",
            "Epoch 150/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2349 - val_loss: 0.2760\n",
            "Epoch 151/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2345 - val_loss: 0.2759\n",
            "Epoch 152/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2342 - val_loss: 0.2759\n",
            "Epoch 153/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2340 - val_loss: 0.2758\n",
            "Epoch 154/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2337 - val_loss: 0.2757\n",
            "Epoch 155/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2335 - val_loss: 0.2756\n",
            "Epoch 156/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2334 - val_loss: 0.2756\n",
            "Epoch 157/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2330 - val_loss: 0.2755\n",
            "Epoch 158/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2328 - val_loss: 0.2755\n",
            "Epoch 159/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2326 - val_loss: 0.2754\n",
            "Epoch 160/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2323 - val_loss: 0.2753\n",
            "Epoch 161/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2320 - val_loss: 0.2752\n",
            "Epoch 162/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2319 - val_loss: 0.2751\n",
            "Epoch 163/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2315 - val_loss: 0.2751\n",
            "Epoch 164/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2313 - val_loss: 0.2750\n",
            "Epoch 165/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2310 - val_loss: 0.2749\n",
            "Epoch 166/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2308 - val_loss: 0.2749\n",
            "Epoch 167/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2306 - val_loss: 0.2748\n",
            "Epoch 168/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2303 - val_loss: 0.2748\n",
            "Epoch 169/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2301 - val_loss: 0.2747\n",
            "Epoch 170/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2300 - val_loss: 0.2746\n",
            "Epoch 171/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2296 - val_loss: 0.2746\n",
            "Epoch 172/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2293 - val_loss: 0.2745\n",
            "Epoch 173/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2292 - val_loss: 0.2744\n",
            "Epoch 174/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2289 - val_loss: 0.2743\n",
            "Epoch 175/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2287 - val_loss: 0.2742\n",
            "Epoch 176/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2284 - val_loss: 0.2742\n",
            "Epoch 177/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2281 - val_loss: 0.2742\n",
            "Epoch 178/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2279 - val_loss: 0.2741\n",
            "Epoch 179/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2277 - val_loss: 0.2740\n",
            "Epoch 180/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2275 - val_loss: 0.2740\n",
            "Epoch 181/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2272 - val_loss: 0.2739\n",
            "Epoch 182/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2270 - val_loss: 0.2739\n",
            "Epoch 183/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2269 - val_loss: 0.2738\n",
            "Epoch 184/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2266 - val_loss: 0.2737\n",
            "Epoch 185/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2263 - val_loss: 0.2738\n",
            "Epoch 186/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2260 - val_loss: 0.2736\n",
            "Epoch 187/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2257 - val_loss: 0.2735\n",
            "Epoch 188/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2256 - val_loss: 0.2736\n",
            "Epoch 189/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2254 - val_loss: 0.2734\n",
            "Epoch 190/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2251 - val_loss: 0.2734\n",
            "Epoch 191/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2249 - val_loss: 0.2734\n",
            "Epoch 192/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2247 - val_loss: 0.2733\n",
            "Epoch 193/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2245 - val_loss: 0.2732\n",
            "Epoch 194/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2242 - val_loss: 0.2731\n",
            "Epoch 195/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2239 - val_loss: 0.2731\n",
            "Epoch 196/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2238 - val_loss: 0.2730\n",
            "Epoch 197/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2235 - val_loss: 0.2730\n",
            "Epoch 198/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2232 - val_loss: 0.2729\n",
            "Epoch 199/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2231 - val_loss: 0.2730\n",
            "Epoch 200/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2228 - val_loss: 0.2728\n",
            "Epoch 201/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2226 - val_loss: 0.2728\n",
            "Epoch 202/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2224 - val_loss: 0.2727\n",
            "Epoch 203/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2222 - val_loss: 0.2727\n",
            "Epoch 204/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2219 - val_loss: 0.2726\n",
            "Epoch 205/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2217 - val_loss: 0.2727\n",
            "Epoch 206/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2215 - val_loss: 0.2725\n",
            "Epoch 207/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2213 - val_loss: 0.2725\n",
            "Epoch 208/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2210 - val_loss: 0.2725\n",
            "Epoch 209/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2208 - val_loss: 0.2724\n",
            "Epoch 210/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2206 - val_loss: 0.2723\n",
            "Epoch 211/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2203 - val_loss: 0.2725\n",
            "Epoch 212/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2202 - val_loss: 0.2723\n",
            "Epoch 213/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2199 - val_loss: 0.2722\n",
            "Epoch 214/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2198 - val_loss: 0.2722\n",
            "Epoch 215/1000\n",
            "34/34 [==============================] - 1s 21ms/step - loss: 0.2195 - val_loss: 0.2722\n",
            "Epoch 216/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2193 - val_loss: 0.2721\n",
            "Epoch 217/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2191 - val_loss: 0.2721\n",
            "Epoch 218/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2189 - val_loss: 0.2722\n",
            "Epoch 219/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2187 - val_loss: 0.2721\n",
            "Epoch 220/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2185 - val_loss: 0.2721\n",
            "Epoch 221/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2183 - val_loss: 0.2718\n",
            "Epoch 222/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2180 - val_loss: 0.2718\n",
            "Epoch 223/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2178 - val_loss: 0.2718\n",
            "Epoch 224/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2176 - val_loss: 0.2718\n",
            "Epoch 225/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2174 - val_loss: 0.2717\n",
            "Epoch 226/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2173 - val_loss: 0.2717\n",
            "Epoch 227/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2171 - val_loss: 0.2719\n",
            "Epoch 228/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2168 - val_loss: 0.2715\n",
            "Epoch 229/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2166 - val_loss: 0.2716\n",
            "Epoch 230/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2163 - val_loss: 0.2715\n",
            "Epoch 231/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2162 - val_loss: 0.2717\n",
            "Epoch 232/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2160 - val_loss: 0.2715\n",
            "Epoch 233/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2157 - val_loss: 0.2715\n",
            "Epoch 234/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2155 - val_loss: 0.2716\n",
            "Epoch 235/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2154 - val_loss: 0.2713\n",
            "Epoch 236/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2151 - val_loss: 0.2713\n",
            "Epoch 237/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2150 - val_loss: 0.2713\n",
            "Epoch 238/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2149 - val_loss: 0.2713\n",
            "Epoch 239/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2146 - val_loss: 0.2713\n",
            "Epoch 240/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2143 - val_loss: 0.2712\n",
            "Epoch 241/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2142 - val_loss: 0.2712\n",
            "Epoch 242/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2140 - val_loss: 0.2712\n",
            "Epoch 243/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2138 - val_loss: 0.2711\n",
            "Epoch 244/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2136 - val_loss: 0.2711\n",
            "Epoch 245/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2134 - val_loss: 0.2712\n",
            "Epoch 246/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2134 - val_loss: 0.2710\n",
            "Epoch 247/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2130 - val_loss: 0.2709\n",
            "Epoch 248/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2128 - val_loss: 0.2709\n",
            "Epoch 249/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2127 - val_loss: 0.2710\n",
            "Epoch 250/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2124 - val_loss: 0.2708\n",
            "Epoch 251/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2122 - val_loss: 0.2708\n",
            "Epoch 252/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2121 - val_loss: 0.2708\n",
            "Epoch 253/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2120 - val_loss: 0.2709\n",
            "Epoch 254/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2116 - val_loss: 0.2708\n",
            "Epoch 255/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2115 - val_loss: 0.2708\n",
            "Epoch 256/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2113 - val_loss: 0.2707\n",
            "Epoch 257/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2111 - val_loss: 0.2706\n",
            "Epoch 258/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2108 - val_loss: 0.2706\n",
            "Epoch 259/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2107 - val_loss: 0.2705\n",
            "Epoch 260/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2106 - val_loss: 0.2706\n",
            "Epoch 261/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2105 - val_loss: 0.2707\n",
            "Epoch 262/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2102 - val_loss: 0.2707\n",
            "Epoch 263/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2101 - val_loss: 0.2705\n",
            "Epoch 264/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2098 - val_loss: 0.2704\n",
            "Epoch 265/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2096 - val_loss: 0.2705\n",
            "Epoch 266/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2096 - val_loss: 0.2704\n",
            "Epoch 267/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2092 - val_loss: 0.2705\n",
            "Epoch 268/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2090 - val_loss: 0.2707\n",
            "Epoch 269/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2090 - val_loss: 0.2704\n",
            "Epoch 270/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2086 - val_loss: 0.2703\n",
            "Epoch 271/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2085 - val_loss: 0.2705\n",
            "Epoch 272/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2085 - val_loss: 0.2709\n",
            "Epoch 273/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2082 - val_loss: 0.2703\n",
            "Epoch 274/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2080 - val_loss: 0.2704\n",
            "Epoch 275/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2078 - val_loss: 0.2703\n",
            "Epoch 276/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2076 - val_loss: 0.2704\n",
            "Epoch 277/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2075 - val_loss: 0.2705\n",
            "Epoch 278/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2074 - val_loss: 0.2702\n",
            "Epoch 279/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2071 - val_loss: 0.2702\n",
            "Epoch 280/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2069 - val_loss: 0.2703\n",
            "Epoch 281/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2069 - val_loss: 0.2700\n",
            "Epoch 282/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2066 - val_loss: 0.2701\n",
            "Epoch 283/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2065 - val_loss: 0.2702\n",
            "Epoch 284/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2063 - val_loss: 0.2701\n",
            "Epoch 285/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2062 - val_loss: 0.2703\n",
            "Epoch 286/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2060 - val_loss: 0.2702\n",
            "Epoch 287/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2057 - val_loss: 0.2700\n",
            "Epoch 288/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2056 - val_loss: 0.2702\n",
            "Epoch 289/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2057 - val_loss: 0.2701\n",
            "Epoch 290/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2052 - val_loss: 0.2699\n",
            "Epoch 291/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2051 - val_loss: 0.2699\n",
            "Epoch 292/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2051 - val_loss: 0.2700\n",
            "Epoch 293/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2047 - val_loss: 0.2699\n",
            "Epoch 294/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2045 - val_loss: 0.2698\n",
            "Epoch 295/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2044 - val_loss: 0.2699\n",
            "Epoch 296/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2044 - val_loss: 0.2700\n",
            "Epoch 297/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2043 - val_loss: 0.2698\n",
            "Epoch 298/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2039 - val_loss: 0.2701\n",
            "Epoch 299/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2038 - val_loss: 0.2700\n",
            "Epoch 300/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2038 - val_loss: 0.2698\n",
            "Epoch 301/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2034 - val_loss: 0.2699\n",
            "Epoch 302/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2033 - val_loss: 0.2698\n",
            "Epoch 303/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2032 - val_loss: 0.2698\n",
            "Epoch 304/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2030 - val_loss: 0.2697\n",
            "Epoch 305/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2030 - val_loss: 0.2703\n",
            "Epoch 306/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2028 - val_loss: 0.2697\n",
            "Epoch 307/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2025 - val_loss: 0.2697\n",
            "Epoch 308/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2023 - val_loss: 0.2701\n",
            "Epoch 309/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2023 - val_loss: 0.2699\n",
            "Epoch 310/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2021 - val_loss: 0.2698\n",
            "Epoch 311/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2020 - val_loss: 0.2697\n",
            "Epoch 312/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2018 - val_loss: 0.2698\n",
            "Epoch 313/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2015 - val_loss: 0.2698\n",
            "Epoch 314/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2017 - val_loss: 0.2698\n",
            "Epoch 315/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2013 - val_loss: 0.2696\n",
            "Epoch 316/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2010 - val_loss: 0.2697\n",
            "Epoch 317/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2010 - val_loss: 0.2696\n",
            "Epoch 318/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2008 - val_loss: 0.2696\n",
            "Epoch 319/1000\n",
            "34/34 [==============================] - 1s 21ms/step - loss: 0.2006 - val_loss: 0.2696\n",
            "Epoch 320/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2008 - val_loss: 0.2699\n",
            "Epoch 321/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2004 - val_loss: 0.2696\n",
            "Epoch 322/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2002 - val_loss: 0.2696\n",
            "Epoch 323/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2001 - val_loss: 0.2696\n",
            "Epoch 324/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1998 - val_loss: 0.2695\n",
            "Epoch 325/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1998 - val_loss: 0.2705\n",
            "Epoch 326/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1999 - val_loss: 0.2697\n",
            "Epoch 327/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1995 - val_loss: 0.2695\n",
            "Epoch 328/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1993 - val_loss: 0.2699\n",
            "Epoch 329/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1992 - val_loss: 0.2696\n",
            "Epoch 330/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1990 - val_loss: 0.2695\n",
            "Epoch 331/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1990 - val_loss: 0.2698\n",
            "Epoch 332/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1988 - val_loss: 0.2697\n",
            "Epoch 333/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1987 - val_loss: 0.2696\n",
            "Epoch 334/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1985 - val_loss: 0.2698\n",
            "Epoch 335/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1986 - val_loss: 0.2696\n",
            "Epoch 336/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1982 - val_loss: 0.2695\n",
            "Epoch 337/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1979 - val_loss: 0.2697\n",
            "Epoch 338/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1980 - val_loss: 0.2696\n",
            "Epoch 339/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1979 - val_loss: 0.2695\n",
            "Epoch 340/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1976 - val_loss: 0.2695\n",
            "Epoch 341/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1974 - val_loss: 0.2697\n",
            "Epoch 342/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1976 - val_loss: 0.2697\n",
            "Epoch 343/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1973 - val_loss: 0.2695\n",
            "Epoch 344/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1970 - val_loss: 0.2696\n",
            "Epoch 345/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1970 - val_loss: 0.2696\n",
            "Epoch 346/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1968 - val_loss: 0.2697\n",
            "Epoch 347/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1967 - val_loss: 0.2694\n",
            "Epoch 348/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1965 - val_loss: 0.2698\n",
            "Epoch 349/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1964 - val_loss: 0.2697\n",
            "Epoch 350/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1964 - val_loss: 0.2700\n",
            "Epoch 351/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1961 - val_loss: 0.2697\n",
            "Epoch 352/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1959 - val_loss: 0.2695\n",
            "Epoch 353/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1957 - val_loss: 0.2695\n",
            "Epoch 354/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1959 - val_loss: 0.2699\n",
            "Epoch 355/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1957 - val_loss: 0.2695\n",
            "Epoch 356/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1955 - val_loss: 0.2695\n",
            "Epoch 357/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1954 - val_loss: 0.2694\n",
            "Epoch 358/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1952 - val_loss: 0.2697\n",
            "Epoch 359/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1953 - val_loss: 0.2696\n",
            "Epoch 360/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1950 - val_loss: 0.2694\n",
            "Epoch 361/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1949 - val_loss: 0.2696\n",
            "Epoch 362/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1947 - val_loss: 0.2696\n",
            "Epoch 363/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1945 - val_loss: 0.2695\n",
            "Epoch 364/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1946 - val_loss: 0.2698\n",
            "Epoch 365/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1943 - val_loss: 0.2695\n",
            "Epoch 366/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1942 - val_loss: 0.2697\n",
            "Epoch 367/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1941 - val_loss: 0.2695\n",
            "Epoch 368/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1940 - val_loss: 0.2698\n",
            "Epoch 369/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1937 - val_loss: 0.2695\n",
            "Epoch 370/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1936 - val_loss: 0.2696\n",
            "Epoch 371/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1937 - val_loss: 0.2696\n",
            "Epoch 372/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1933 - val_loss: 0.2695\n",
            "Epoch 373/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1934 - val_loss: 0.2696\n",
            "Epoch 374/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1932 - val_loss: 0.2695\n",
            "Epoch 375/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1930 - val_loss: 0.2695\n",
            "Epoch 376/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1930 - val_loss: 0.2696\n",
            "Epoch 377/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1928 - val_loss: 0.2695\n",
            "Epoch 378/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1927 - val_loss: 0.2695\n",
            "Epoch 379/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1925 - val_loss: 0.2695\n",
            "Epoch 380/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1924 - val_loss: 0.2699\n",
            "Epoch 381/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1925 - val_loss: 0.2696\n",
            "Epoch 382/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1922 - val_loss: 0.2695\n",
            "Epoch 383/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1921 - val_loss: 0.2696\n",
            "Epoch 384/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1919 - val_loss: 0.2696\n",
            "Epoch 385/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1919 - val_loss: 0.2697\n",
            "Epoch 386/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1917 - val_loss: 0.2697\n",
            "Epoch 387/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1916 - val_loss: 0.2695\n",
            "Epoch 388/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1915 - val_loss: 0.2695\n",
            "Epoch 389/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1914 - val_loss: 0.2701\n",
            "Epoch 390/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1914 - val_loss: 0.2696\n",
            "Epoch 391/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1911 - val_loss: 0.2696\n",
            "Epoch 392/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1909 - val_loss: 0.2695\n",
            "Epoch 393/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1909 - val_loss: 0.2699\n",
            "Epoch 394/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1910 - val_loss: 0.2696\n",
            "Epoch 395/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1907 - val_loss: 0.2698\n",
            "Epoch 396/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1906 - val_loss: 0.2696\n",
            "Epoch 397/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1904 - val_loss: 0.2696\n",
            "Epoch 398/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1905 - val_loss: 0.2697\n",
            "Epoch 399/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1903 - val_loss: 0.2697\n",
            "Epoch 400/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1900 - val_loss: 0.2696\n",
            "Epoch 401/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1900 - val_loss: 0.2695\n",
            "Epoch 402/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1898 - val_loss: 0.2698\n",
            "Epoch 403/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1899 - val_loss: 0.2698\n",
            "Epoch 404/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1896 - val_loss: 0.2697\n",
            "Epoch 405/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1896 - val_loss: 0.2697\n",
            "Epoch 406/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1894 - val_loss: 0.2696\n",
            "Epoch 407/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1893 - val_loss: 0.2697\n",
            "Epoch 408/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1892 - val_loss: 0.2697\n",
            "Epoch 409/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1892 - val_loss: 0.2697\n",
            "Epoch 410/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1889 - val_loss: 0.2698\n",
            "Epoch 411/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1888 - val_loss: 0.2698\n",
            "Epoch 412/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1889 - val_loss: 0.2702\n",
            "Epoch 413/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1888 - val_loss: 0.2698\n",
            "Epoch 414/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1885 - val_loss: 0.2698\n",
            "Epoch 415/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1884 - val_loss: 0.2697\n",
            "Epoch 416/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1884 - val_loss: 0.2697\n",
            "Epoch 417/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1882 - val_loss: 0.2698\n",
            "Epoch 418/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1883 - val_loss: 0.2700\n",
            "Epoch 419/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1880 - val_loss: 0.2703\n",
            "Epoch 420/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1878 - val_loss: 0.2697\n",
            "Epoch 421/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1880 - val_loss: 0.2697\n",
            "Epoch 422/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1878 - val_loss: 0.2701\n",
            "Epoch 423/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1878 - val_loss: 0.2699\n",
            "Epoch 424/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1875 - val_loss: 0.2702\n",
            "Epoch 425/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1875 - val_loss: 0.2699\n",
            "Epoch 426/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1873 - val_loss: 0.2697\n",
            "Epoch 427/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1873 - val_loss: 0.2698\n",
            "Epoch 428/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1871 - val_loss: 0.2700\n",
            "Epoch 429/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1870 - val_loss: 0.2701\n",
            "Epoch 430/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1868 - val_loss: 0.2699\n",
            "Epoch 431/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1869 - val_loss: 0.2700\n",
            "Epoch 432/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1867 - val_loss: 0.2699\n",
            "Epoch 433/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1865 - val_loss: 0.2698\n",
            "Epoch 434/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1866 - val_loss: 0.2704\n",
            "Epoch 435/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1866 - val_loss: 0.2699\n",
            "Epoch 436/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1862 - val_loss: 0.2699\n",
            "Epoch 437/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1862 - val_loss: 0.2701\n",
            "Epoch 438/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1862 - val_loss: 0.2701\n",
            "Epoch 439/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1859 - val_loss: 0.2703\n",
            "Epoch 440/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1860 - val_loss: 0.2706\n",
            "Epoch 441/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1858 - val_loss: 0.2700\n",
            "Epoch 442/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1858 - val_loss: 0.2700\n",
            "Epoch 443/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1856 - val_loss: 0.2703\n",
            "Epoch 444/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1854 - val_loss: 0.2699\n",
            "Epoch 445/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1854 - val_loss: 0.2701\n",
            "Epoch 446/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1854 - val_loss: 0.2700\n",
            "Epoch 447/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1851 - val_loss: 0.2700\n",
            "Epoch 448/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1851 - val_loss: 0.2701\n",
            "Epoch 449/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1850 - val_loss: 0.2700\n",
            "Epoch 450/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1848 - val_loss: 0.2702\n",
            "Epoch 451/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1847 - val_loss: 0.2704\n",
            "Epoch 452/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1848 - val_loss: 0.2701\n",
            "Epoch 453/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1846 - val_loss: 0.2702\n",
            "Epoch 454/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1846 - val_loss: 0.2703\n",
            "Epoch 455/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1846 - val_loss: 0.2704\n",
            "Epoch 456/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1843 - val_loss: 0.2701\n",
            "Epoch 457/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1842 - val_loss: 0.2702\n",
            "Epoch 458/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1842 - val_loss: 0.2705\n",
            "Epoch 459/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1842 - val_loss: 0.2702\n",
            "Epoch 460/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1840 - val_loss: 0.2702\n",
            "Epoch 461/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1839 - val_loss: 0.2702\n",
            "Epoch 462/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1840 - val_loss: 0.2703\n",
            "Epoch 463/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1837 - val_loss: 0.2702\n",
            "Epoch 464/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1836 - val_loss: 0.2702\n",
            "Epoch 465/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1835 - val_loss: 0.2702\n",
            "Epoch 466/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1834 - val_loss: 0.2703\n",
            "Epoch 467/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1832 - val_loss: 0.2701\n",
            "Epoch 468/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1832 - val_loss: 0.2702\n",
            "Epoch 469/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1830 - val_loss: 0.2704\n",
            "Epoch 470/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1834 - val_loss: 0.2706\n",
            "Epoch 471/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1830 - val_loss: 0.2702\n",
            "Epoch 472/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1828 - val_loss: 0.2703\n",
            "Epoch 473/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1827 - val_loss: 0.2702\n",
            "Epoch 474/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1826 - val_loss: 0.2704\n",
            "Epoch 475/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1827 - val_loss: 0.2706\n",
            "Epoch 476/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1825 - val_loss: 0.2703\n",
            "Epoch 477/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1825 - val_loss: 0.2705\n",
            "Epoch 478/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1824 - val_loss: 0.2704\n",
            "Epoch 479/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1822 - val_loss: 0.2703\n",
            "Epoch 480/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1822 - val_loss: 0.2704\n",
            "Epoch 481/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1821 - val_loss: 0.2703\n",
            "Epoch 482/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1819 - val_loss: 0.2704\n",
            "Epoch 483/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1818 - val_loss: 0.2705\n",
            "Epoch 484/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1822 - val_loss: 0.2705\n",
            "Epoch 485/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1818 - val_loss: 0.2703\n",
            "Epoch 486/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1818 - val_loss: 0.2707\n",
            "Epoch 487/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1815 - val_loss: 0.2705\n",
            "Epoch 488/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1815 - val_loss: 0.2705\n",
            "Epoch 489/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1814 - val_loss: 0.2707\n",
            "Epoch 490/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1813 - val_loss: 0.2705\n",
            "Epoch 491/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1812 - val_loss: 0.2705\n",
            "Epoch 492/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1812 - val_loss: 0.2707\n",
            "Epoch 493/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1810 - val_loss: 0.2706\n",
            "Epoch 494/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1811 - val_loss: 0.2707\n",
            "Epoch 495/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1809 - val_loss: 0.2708\n",
            "Epoch 496/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1808 - val_loss: 0.2706\n",
            "Epoch 497/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1808 - val_loss: 0.2706\n",
            "Epoch 498/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1808 - val_loss: 0.2709\n",
            "Epoch 499/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1806 - val_loss: 0.2707\n",
            "Epoch 500/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1805 - val_loss: 0.2707\n",
            "Epoch 501/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1803 - val_loss: 0.2707\n",
            "Epoch 502/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1803 - val_loss: 0.2707\n",
            "Epoch 503/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1802 - val_loss: 0.2709\n",
            "Epoch 504/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1801 - val_loss: 0.2708\n",
            "Epoch 505/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1801 - val_loss: 0.2709\n",
            "Epoch 506/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1803 - val_loss: 0.2707\n",
            "Epoch 507/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1799 - val_loss: 0.2707\n",
            "Epoch 508/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1798 - val_loss: 0.2709\n",
            "Epoch 509/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1797 - val_loss: 0.2708\n",
            "Epoch 510/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1796 - val_loss: 0.2711\n",
            "Epoch 511/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1796 - val_loss: 0.2708\n",
            "Epoch 512/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1796 - val_loss: 0.2709\n",
            "Epoch 513/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1796 - val_loss: 0.2712\n",
            "Epoch 514/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1795 - val_loss: 0.2709\n",
            "Epoch 515/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1793 - val_loss: 0.2710\n",
            "Epoch 516/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1792 - val_loss: 0.2708\n",
            "Epoch 517/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1792 - val_loss: 0.2711\n",
            "Epoch 518/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1790 - val_loss: 0.2710\n",
            "Epoch 519/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1793 - val_loss: 0.2711\n",
            "Epoch 520/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1788 - val_loss: 0.2713\n",
            "Epoch 521/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1787 - val_loss: 0.2711\n",
            "Epoch 522/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1788 - val_loss: 0.2711\n",
            "Epoch 523/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1788 - val_loss: 0.2710\n",
            "Epoch 524/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1786 - val_loss: 0.2710\n",
            "Epoch 525/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1785 - val_loss: 0.2710\n",
            "Epoch 526/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1783 - val_loss: 0.2709\n",
            "Epoch 527/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1784 - val_loss: 0.2710\n",
            "Epoch 528/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1785 - val_loss: 0.2714\n",
            "Epoch 529/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1784 - val_loss: 0.2714\n",
            "Epoch 530/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1782 - val_loss: 0.2710\n",
            "Epoch 531/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1781 - val_loss: 0.2712\n",
            "Epoch 532/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1780 - val_loss: 0.2711\n",
            "Epoch 533/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1780 - val_loss: 0.2715\n",
            "Epoch 534/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1779 - val_loss: 0.2711\n",
            "Epoch 535/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1778 - val_loss: 0.2715\n",
            "Epoch 536/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1777 - val_loss: 0.2715\n",
            "Epoch 537/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1777 - val_loss: 0.2711\n",
            "Epoch 538/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1775 - val_loss: 0.2712\n",
            "Epoch 539/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1776 - val_loss: 0.2713\n",
            "Epoch 540/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1775 - val_loss: 0.2715\n",
            "Epoch 541/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1775 - val_loss: 0.2712\n",
            "Epoch 542/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1771 - val_loss: 0.2713\n",
            "Epoch 543/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1774 - val_loss: 0.2716\n",
            "Epoch 544/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1772 - val_loss: 0.2714\n",
            "Epoch 545/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1771 - val_loss: 0.2714\n",
            "Epoch 546/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1769 - val_loss: 0.2713\n",
            "Epoch 547/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1769 - val_loss: 0.2720\n",
            "Epoch 548/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1769 - val_loss: 0.2714\n",
            "Epoch 549/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1769 - val_loss: 0.2715\n",
            "Epoch 550/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1770 - val_loss: 0.2715\n",
            "Epoch 551/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1766 - val_loss: 0.2714\n",
            "Epoch 552/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1766 - val_loss: 0.2714\n",
            "Epoch 553/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1765 - val_loss: 0.2714\n",
            "Epoch 554/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1765 - val_loss: 0.2718\n",
            "Epoch 555/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1765 - val_loss: 0.2719\n",
            "Epoch 556/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1764 - val_loss: 0.2716\n",
            "Epoch 557/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1762 - val_loss: 0.2714\n",
            "Epoch 558/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1762 - val_loss: 0.2714\n",
            "Epoch 559/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1763 - val_loss: 0.2718\n",
            "Epoch 560/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1760 - val_loss: 0.2715\n",
            "Epoch 561/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1760 - val_loss: 0.2719\n",
            "Epoch 562/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1760 - val_loss: 0.2716\n",
            "Epoch 563/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1759 - val_loss: 0.2721\n",
            "Epoch 564/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1761 - val_loss: 0.2716\n",
            "Epoch 565/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1756 - val_loss: 0.2716\n",
            "Epoch 566/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1757 - val_loss: 0.2716\n",
            "Epoch 567/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1755 - val_loss: 0.2718\n",
            "Epoch 568/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1757 - val_loss: 0.2717\n",
            "Epoch 569/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1755 - val_loss: 0.2718\n",
            "Epoch 570/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1753 - val_loss: 0.2719\n",
            "Epoch 571/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1753 - val_loss: 0.2720\n",
            "Epoch 572/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1752 - val_loss: 0.2716\n",
            "Epoch 573/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1752 - val_loss: 0.2722\n",
            "Epoch 574/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1752 - val_loss: 0.2722\n",
            "Epoch 575/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1750 - val_loss: 0.2720\n",
            "Epoch 576/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1751 - val_loss: 0.2722\n",
            "Epoch 577/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1750 - val_loss: 0.2719\n",
            "Epoch 578/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1749 - val_loss: 0.2720\n",
            "Epoch 579/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1747 - val_loss: 0.2718\n",
            "Epoch 580/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1747 - val_loss: 0.2720\n",
            "Epoch 581/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1748 - val_loss: 0.2721\n",
            "Epoch 582/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1747 - val_loss: 0.2720\n",
            "Epoch 583/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1747 - val_loss: 0.2721\n",
            "Epoch 584/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1745 - val_loss: 0.2719\n",
            "Epoch 585/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1745 - val_loss: 0.2719\n",
            "Epoch 586/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1743 - val_loss: 0.2720\n",
            "Epoch 587/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1742 - val_loss: 0.2723\n",
            "Epoch 588/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1743 - val_loss: 0.2724\n",
            "Epoch 589/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1742 - val_loss: 0.2723\n",
            "Epoch 590/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1742 - val_loss: 0.2722\n",
            "Epoch 591/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1740 - val_loss: 0.2721\n",
            "Epoch 592/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1739 - val_loss: 0.2722\n",
            "Epoch 593/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1740 - val_loss: 0.2722\n",
            "Epoch 594/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1739 - val_loss: 0.2724\n",
            "Epoch 595/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1738 - val_loss: 0.2725\n",
            "Epoch 596/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1738 - val_loss: 0.2721\n",
            "Epoch 597/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1737 - val_loss: 0.2723\n",
            "Epoch 598/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1736 - val_loss: 0.2723\n",
            "Epoch 599/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1738 - val_loss: 0.2723\n",
            "Epoch 600/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1735 - val_loss: 0.2721\n",
            "Epoch 601/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1734 - val_loss: 0.2721\n",
            "Epoch 602/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1734 - val_loss: 0.2722\n",
            "Epoch 603/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1733 - val_loss: 0.2722\n",
            "Epoch 604/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1734 - val_loss: 0.2723\n",
            "Epoch 605/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1731 - val_loss: 0.2722\n",
            "Epoch 606/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1733 - val_loss: 0.2724\n",
            "Epoch 607/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1732 - val_loss: 0.2725\n",
            "Epoch 608/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1731 - val_loss: 0.2725\n",
            "Epoch 609/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1730 - val_loss: 0.2726\n",
            "Epoch 610/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1730 - val_loss: 0.2728\n",
            "Epoch 611/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1729 - val_loss: 0.2726\n",
            "Epoch 612/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1728 - val_loss: 0.2725\n",
            "Epoch 613/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1728 - val_loss: 0.2724\n",
            "Epoch 614/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1727 - val_loss: 0.2724\n",
            "Epoch 615/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1726 - val_loss: 0.2723\n",
            "Epoch 616/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1725 - val_loss: 0.2729\n",
            "Epoch 617/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1727 - val_loss: 0.2725\n",
            "Epoch 618/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1726 - val_loss: 0.2726\n",
            "Epoch 619/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1724 - val_loss: 0.2728\n",
            "Epoch 620/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1724 - val_loss: 0.2726\n",
            "Epoch 621/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1723 - val_loss: 0.2726\n",
            "Epoch 622/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1722 - val_loss: 0.2726\n",
            "Epoch 623/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1721 - val_loss: 0.2725\n",
            "Epoch 624/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1721 - val_loss: 0.2725\n",
            "Epoch 625/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1720 - val_loss: 0.2726\n",
            "Epoch 626/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1721 - val_loss: 0.2729\n",
            "Epoch 627/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1720 - val_loss: 0.2726\n",
            "Epoch 628/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1719 - val_loss: 0.2729\n",
            "Epoch 629/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1719 - val_loss: 0.2727\n",
            "Epoch 630/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1718 - val_loss: 0.2729\n",
            "Epoch 631/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1718 - val_loss: 0.2726\n",
            "Epoch 632/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1716 - val_loss: 0.2728\n",
            "Epoch 633/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1716 - val_loss: 0.2726\n",
            "Epoch 634/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1715 - val_loss: 0.2728\n",
            "Epoch 635/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1715 - val_loss: 0.2730\n",
            "Epoch 636/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1716 - val_loss: 0.2728\n",
            "Epoch 637/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1713 - val_loss: 0.2728\n",
            "Epoch 638/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1714 - val_loss: 0.2731\n",
            "Epoch 639/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1713 - val_loss: 0.2729\n",
            "Epoch 640/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1712 - val_loss: 0.2731\n",
            "Epoch 641/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1712 - val_loss: 0.2730\n",
            "Epoch 642/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1712 - val_loss: 0.2731\n",
            "Epoch 643/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1711 - val_loss: 0.2730\n",
            "Epoch 644/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1711 - val_loss: 0.2732\n",
            "Epoch 645/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1710 - val_loss: 0.2731\n",
            "Epoch 646/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1710 - val_loss: 0.2731\n",
            "Epoch 647/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1709 - val_loss: 0.2729\n",
            "Epoch 648/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1708 - val_loss: 0.2729\n",
            "Epoch 649/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1708 - val_loss: 0.2732\n",
            "Epoch 650/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1708 - val_loss: 0.2733\n",
            "Epoch 651/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1707 - val_loss: 0.2730\n",
            "Epoch 652/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1706 - val_loss: 0.2731\n",
            "Epoch 653/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1705 - val_loss: 0.2731\n",
            "Epoch 654/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1705 - val_loss: 0.2731\n",
            "Epoch 655/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1704 - val_loss: 0.2734\n",
            "Epoch 656/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1706 - val_loss: 0.2732\n",
            "Epoch 657/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1703 - val_loss: 0.2732\n",
            "Epoch 658/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1702 - val_loss: 0.2738\n",
            "Epoch 659/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1702 - val_loss: 0.2733\n",
            "Epoch 660/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1703 - val_loss: 0.2732\n",
            "Epoch 661/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1702 - val_loss: 0.2734\n",
            "Epoch 662/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1701 - val_loss: 0.2732\n",
            "Epoch 663/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1700 - val_loss: 0.2732\n",
            "Epoch 664/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1701 - val_loss: 0.2734\n",
            "Epoch 665/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1700 - val_loss: 0.2734\n",
            "Epoch 666/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1699 - val_loss: 0.2734\n",
            "Epoch 667/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1700 - val_loss: 0.2735\n",
            "Epoch 668/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1698 - val_loss: 0.2734\n",
            "Epoch 669/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1697 - val_loss: 0.2735\n",
            "Epoch 670/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1697 - val_loss: 0.2734\n",
            "Epoch 671/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1698 - val_loss: 0.2735\n",
            "Epoch 672/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1696 - val_loss: 0.2735\n",
            "Epoch 673/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1697 - val_loss: 0.2736\n",
            "Epoch 674/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1697 - val_loss: 0.2736\n",
            "Epoch 675/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1694 - val_loss: 0.2736\n",
            "Epoch 676/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1694 - val_loss: 0.2738\n",
            "Epoch 677/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1694 - val_loss: 0.2736\n",
            "Epoch 678/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1693 - val_loss: 0.2736\n",
            "Epoch 679/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1694 - val_loss: 0.2736\n",
            "Epoch 680/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1691 - val_loss: 0.2739\n",
            "Epoch 681/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1692 - val_loss: 0.2739\n",
            "Epoch 682/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1691 - val_loss: 0.2738\n",
            "Epoch 683/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1692 - val_loss: 0.2744\n",
            "Epoch 684/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1691 - val_loss: 0.2737\n",
            "Epoch 685/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1690 - val_loss: 0.2741\n",
            "Epoch 686/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1690 - val_loss: 0.2739\n",
            "Epoch 687/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1690 - val_loss: 0.2739\n",
            "Epoch 688/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1688 - val_loss: 0.2740\n",
            "Epoch 689/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1688 - val_loss: 0.2738\n",
            "Epoch 690/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1688 - val_loss: 0.2738\n",
            "Epoch 691/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1688 - val_loss: 0.2738\n",
            "Epoch 692/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1686 - val_loss: 0.2738\n",
            "Epoch 693/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1686 - val_loss: 0.2741\n",
            "Epoch 694/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1686 - val_loss: 0.2739\n",
            "Epoch 695/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1685 - val_loss: 0.2741\n",
            "Epoch 696/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1686 - val_loss: 0.2741\n",
            "Epoch 697/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1683 - val_loss: 0.2742\n",
            "Epoch 698/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1684 - val_loss: 0.2739\n",
            "Epoch 699/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1682 - val_loss: 0.2740\n",
            "Epoch 700/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1683 - val_loss: 0.2740\n",
            "Epoch 701/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1683 - val_loss: 0.2739\n",
            "Epoch 702/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1682 - val_loss: 0.2742\n",
            "Epoch 703/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1682 - val_loss: 0.2741\n",
            "Epoch 704/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1681 - val_loss: 0.2744\n",
            "Epoch 705/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1681 - val_loss: 0.2740\n",
            "Epoch 706/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1679 - val_loss: 0.2740\n",
            "Epoch 707/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1680 - val_loss: 0.2747\n",
            "Epoch 708/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1682 - val_loss: 0.2743\n",
            "Epoch 709/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1679 - val_loss: 0.2741\n",
            "Epoch 710/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1679 - val_loss: 0.2741\n",
            "Epoch 711/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1678 - val_loss: 0.2742\n",
            "Epoch 712/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1678 - val_loss: 0.2743\n",
            "Epoch 713/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1676 - val_loss: 0.2744\n",
            "Epoch 714/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1676 - val_loss: 0.2744\n",
            "Epoch 715/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1677 - val_loss: 0.2742\n",
            "Epoch 716/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1675 - val_loss: 0.2744\n",
            "Epoch 717/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1675 - val_loss: 0.2743\n",
            "Epoch 718/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1674 - val_loss: 0.2744\n",
            "Epoch 719/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1674 - val_loss: 0.2745\n",
            "Epoch 720/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1674 - val_loss: 0.2745\n",
            "Epoch 721/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1675 - val_loss: 0.2745\n",
            "Epoch 722/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1672 - val_loss: 0.2746\n",
            "Epoch 723/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1673 - val_loss: 0.2745\n",
            "Epoch 724/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1674 - val_loss: 0.2750\n",
            "Epoch 725/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1671 - val_loss: 0.2747\n",
            "Epoch 726/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1672 - val_loss: 0.2747\n",
            "Epoch 727/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1672 - val_loss: 0.2744\n",
            "Epoch 728/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1672 - val_loss: 0.2745\n",
            "Epoch 729/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1670 - val_loss: 0.2744\n",
            "Epoch 730/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1669 - val_loss: 0.2744\n",
            "Epoch 731/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1669 - val_loss: 0.2744\n",
            "Epoch 732/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1669 - val_loss: 0.2746\n",
            "Epoch 733/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1669 - val_loss: 0.2745\n",
            "Epoch 734/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1667 - val_loss: 0.2748\n",
            "Epoch 735/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1668 - val_loss: 0.2749\n",
            "Epoch 736/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1669 - val_loss: 0.2746\n",
            "Epoch 737/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1667 - val_loss: 0.2746\n",
            "Epoch 738/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1666 - val_loss: 0.2746\n",
            "Epoch 739/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1667 - val_loss: 0.2754\n",
            "Epoch 740/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1666 - val_loss: 0.2749\n",
            "Epoch 741/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1665 - val_loss: 0.2746\n",
            "Epoch 742/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1664 - val_loss: 0.2747\n",
            "Epoch 743/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1665 - val_loss: 0.2747\n",
            "Epoch 744/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1664 - val_loss: 0.2754\n",
            "Epoch 745/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1665 - val_loss: 0.2748\n",
            "Epoch 746/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1663 - val_loss: 0.2747\n",
            "Epoch 747/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1663 - val_loss: 0.2750\n",
            "Epoch 748/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1663 - val_loss: 0.2749\n",
            "Epoch 749/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1661 - val_loss: 0.2750\n",
            "Epoch 750/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1662 - val_loss: 0.2750\n",
            "Epoch 751/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1661 - val_loss: 0.2751\n",
            "Epoch 752/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1662 - val_loss: 0.2750\n",
            "Epoch 753/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1661 - val_loss: 0.2749\n",
            "Epoch 754/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1660 - val_loss: 0.2748\n",
            "Epoch 755/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1660 - val_loss: 0.2753\n",
            "Epoch 756/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1660 - val_loss: 0.2751\n",
            "Epoch 757/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1659 - val_loss: 0.2753\n",
            "Epoch 758/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1659 - val_loss: 0.2751\n",
            "Epoch 759/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1659 - val_loss: 0.2751\n",
            "Epoch 760/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1657 - val_loss: 0.2752\n",
            "Epoch 761/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1657 - val_loss: 0.2755\n",
            "Epoch 762/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1656 - val_loss: 0.2751\n",
            "Epoch 763/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1655 - val_loss: 0.2750\n",
            "Epoch 764/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1657 - val_loss: 0.2750\n",
            "Epoch 765/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1655 - val_loss: 0.2753\n",
            "Epoch 766/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1656 - val_loss: 0.2752\n",
            "Epoch 767/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1655 - val_loss: 0.2752\n",
            "Epoch 768/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1654 - val_loss: 0.2751\n",
            "Epoch 769/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1654 - val_loss: 0.2752\n",
            "Epoch 770/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1653 - val_loss: 0.2755\n",
            "Epoch 771/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1656 - val_loss: 0.2753\n",
            "Epoch 772/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1652 - val_loss: 0.2753\n",
            "Epoch 773/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1652 - val_loss: 0.2753\n",
            "Epoch 774/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1651 - val_loss: 0.2753\n",
            "Epoch 775/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1652 - val_loss: 0.2754\n",
            "Epoch 776/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1652 - val_loss: 0.2752\n",
            "Epoch 777/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1650 - val_loss: 0.2753\n",
            "Epoch 778/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1651 - val_loss: 0.2759\n",
            "Epoch 779/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1653 - val_loss: 0.2757\n",
            "Epoch 780/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1651 - val_loss: 0.2755\n",
            "Epoch 781/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1649 - val_loss: 0.2755\n",
            "Epoch 782/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1648 - val_loss: 0.2756\n",
            "Epoch 783/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1649 - val_loss: 0.2755\n",
            "Epoch 784/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1650 - val_loss: 0.2760\n",
            "Epoch 785/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1649 - val_loss: 0.2756\n",
            "Epoch 786/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1648 - val_loss: 0.2755\n",
            "Epoch 787/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1647 - val_loss: 0.2756\n",
            "Epoch 788/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1647 - val_loss: 0.2757\n",
            "Epoch 789/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1647 - val_loss: 0.2755\n",
            "Epoch 790/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1645 - val_loss: 0.2759\n",
            "Epoch 791/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1647 - val_loss: 0.2757\n",
            "Epoch 792/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1647 - val_loss: 0.2761\n",
            "Epoch 793/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1646 - val_loss: 0.2757\n",
            "Epoch 794/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1645 - val_loss: 0.2760\n",
            "Epoch 795/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1644 - val_loss: 0.2758\n",
            "Epoch 796/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1643 - val_loss: 0.2756\n",
            "Epoch 797/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1644 - val_loss: 0.2757\n",
            "Epoch 798/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1643 - val_loss: 0.2756\n",
            "Epoch 799/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1642 - val_loss: 0.2760\n",
            "Epoch 800/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1644 - val_loss: 0.2757\n",
            "Epoch 801/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1643 - val_loss: 0.2760\n",
            "Epoch 802/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1642 - val_loss: 0.2758\n",
            "Epoch 803/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1643 - val_loss: 0.2758\n",
            "Epoch 804/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1640 - val_loss: 0.2759\n",
            "Epoch 805/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1640 - val_loss: 0.2759\n",
            "Epoch 806/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1641 - val_loss: 0.2761\n",
            "Epoch 807/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1640 - val_loss: 0.2759\n",
            "Epoch 808/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1641 - val_loss: 0.2761\n",
            "Epoch 809/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1640 - val_loss: 0.2759\n",
            "Epoch 810/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1640 - val_loss: 0.2760\n",
            "Epoch 811/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1639 - val_loss: 0.2764\n",
            "Epoch 812/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1639 - val_loss: 0.2760\n",
            "Epoch 813/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1638 - val_loss: 0.2760\n",
            "Epoch 814/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1637 - val_loss: 0.2761\n",
            "Epoch 815/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1637 - val_loss: 0.2762\n",
            "Epoch 816/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1637 - val_loss: 0.2760\n",
            "Epoch 817/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1638 - val_loss: 0.2762\n",
            "Epoch 818/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1637 - val_loss: 0.2762\n",
            "Epoch 819/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1636 - val_loss: 0.2760\n",
            "Epoch 820/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1636 - val_loss: 0.2766\n",
            "Epoch 821/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1637 - val_loss: 0.2764\n",
            "Epoch 822/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1635 - val_loss: 0.2762\n",
            "Epoch 823/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1635 - val_loss: 0.2763\n",
            "Epoch 824/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1634 - val_loss: 0.2763\n",
            "Epoch 825/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1635 - val_loss: 0.2767\n",
            "Epoch 826/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1633 - val_loss: 0.2762\n",
            "Epoch 827/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1634 - val_loss: 0.2763\n",
            "Epoch 828/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1635 - val_loss: 0.2763\n",
            "Epoch 829/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1634 - val_loss: 0.2764\n",
            "Epoch 830/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1631 - val_loss: 0.2763\n",
            "Epoch 831/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1632 - val_loss: 0.2766\n",
            "Epoch 832/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1632 - val_loss: 0.2765\n",
            "Epoch 833/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1631 - val_loss: 0.2765\n",
            "Epoch 834/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1632 - val_loss: 0.2768\n",
            "Epoch 835/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1631 - val_loss: 0.2766\n",
            "Epoch 836/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1630 - val_loss: 0.2763\n",
            "Epoch 837/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1629 - val_loss: 0.2764\n",
            "Epoch 838/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1630 - val_loss: 0.2764\n",
            "Epoch 839/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1630 - val_loss: 0.2764\n",
            "Epoch 840/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1630 - val_loss: 0.2766\n",
            "Epoch 841/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1629 - val_loss: 0.2766\n",
            "Epoch 842/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1630 - val_loss: 0.2765\n",
            "Epoch 843/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1628 - val_loss: 0.2766\n",
            "Epoch 844/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1628 - val_loss: 0.2766\n",
            "Epoch 845/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1627 - val_loss: 0.2769\n",
            "Epoch 846/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1628 - val_loss: 0.2767\n",
            "Epoch 847/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1627 - val_loss: 0.2767\n",
            "Epoch 848/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1626 - val_loss: 0.2765\n",
            "Epoch 849/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1625 - val_loss: 0.2766\n",
            "Epoch 850/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1628 - val_loss: 0.2766\n",
            "Epoch 851/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1626 - val_loss: 0.2767\n",
            "Epoch 852/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1625 - val_loss: 0.2770\n",
            "Epoch 853/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1625 - val_loss: 0.2769\n",
            "Epoch 854/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1624 - val_loss: 0.2770\n",
            "Epoch 855/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1625 - val_loss: 0.2769\n",
            "Epoch 856/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1623 - val_loss: 0.2768\n",
            "Epoch 857/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1623 - val_loss: 0.2768\n",
            "Epoch 858/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1623 - val_loss: 0.2769\n",
            "Epoch 859/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1624 - val_loss: 0.2768\n",
            "Epoch 860/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1624 - val_loss: 0.2769\n",
            "Epoch 861/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1623 - val_loss: 0.2770\n",
            "Epoch 862/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1622 - val_loss: 0.2772\n",
            "Epoch 863/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1621 - val_loss: 0.2770\n",
            "Epoch 864/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1623 - val_loss: 0.2768\n",
            "Epoch 865/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1622 - val_loss: 0.2773\n",
            "Epoch 866/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1621 - val_loss: 0.2769\n",
            "Epoch 867/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1622 - val_loss: 0.2773\n",
            "Epoch 868/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1620 - val_loss: 0.2770\n",
            "Epoch 869/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1620 - val_loss: 0.2771\n",
            "Epoch 870/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1619 - val_loss: 0.2774\n",
            "Epoch 871/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1621 - val_loss: 0.2772\n",
            "Epoch 872/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1618 - val_loss: 0.2772\n",
            "Epoch 873/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1619 - val_loss: 0.2774\n",
            "Epoch 874/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1619 - val_loss: 0.2770\n",
            "Epoch 875/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1618 - val_loss: 0.2773\n",
            "Epoch 876/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1618 - val_loss: 0.2771\n",
            "Epoch 877/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1617 - val_loss: 0.2772\n",
            "Epoch 878/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1617 - val_loss: 0.2772\n",
            "Epoch 879/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1617 - val_loss: 0.2774\n",
            "Epoch 880/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1618 - val_loss: 0.2771\n",
            "Epoch 881/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1616 - val_loss: 0.2772\n",
            "Epoch 882/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1616 - val_loss: 0.2779\n",
            "Epoch 883/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1616 - val_loss: 0.2776\n",
            "Epoch 884/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1616 - val_loss: 0.2776\n",
            "Epoch 885/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1615 - val_loss: 0.2776\n",
            "Epoch 886/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1615 - val_loss: 0.2772\n",
            "Epoch 887/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1616 - val_loss: 0.2774\n",
            "Epoch 888/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1616 - val_loss: 0.2775\n",
            "Epoch 889/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1614 - val_loss: 0.2772\n",
            "Epoch 890/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1613 - val_loss: 0.2776\n",
            "Epoch 891/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1614 - val_loss: 0.2776\n",
            "Epoch 892/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1613 - val_loss: 0.2774\n",
            "Epoch 893/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1613 - val_loss: 0.2774\n",
            "Epoch 894/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1613 - val_loss: 0.2775\n",
            "Epoch 895/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1613 - val_loss: 0.2777\n",
            "Epoch 896/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1613 - val_loss: 0.2779\n",
            "Epoch 897/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1611 - val_loss: 0.2775\n",
            "Epoch 898/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1611 - val_loss: 0.2776\n",
            "Epoch 899/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1613 - val_loss: 0.2775\n",
            "Epoch 900/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1611 - val_loss: 0.2776\n",
            "Epoch 901/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1611 - val_loss: 0.2775\n",
            "Epoch 902/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1611 - val_loss: 0.2775\n",
            "Epoch 903/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1609 - val_loss: 0.2782\n",
            "Epoch 904/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1609 - val_loss: 0.2776\n",
            "Epoch 905/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1610 - val_loss: 0.2777\n",
            "Epoch 906/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1610 - val_loss: 0.2777\n",
            "Epoch 907/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1609 - val_loss: 0.2778\n",
            "Epoch 908/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1608 - val_loss: 0.2777\n",
            "Epoch 909/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1610 - val_loss: 0.2777\n",
            "Epoch 910/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1608 - val_loss: 0.2777\n",
            "Epoch 911/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1609 - val_loss: 0.2780\n",
            "Epoch 912/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1608 - val_loss: 0.2777\n",
            "Epoch 913/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1607 - val_loss: 0.2778\n",
            "Epoch 914/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1606 - val_loss: 0.2778\n",
            "Epoch 915/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1606 - val_loss: 0.2778\n",
            "Epoch 916/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1607 - val_loss: 0.2778\n",
            "Epoch 917/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1607 - val_loss: 0.2780\n",
            "Epoch 918/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1608 - val_loss: 0.2779\n",
            "Epoch 919/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1606 - val_loss: 0.2783\n",
            "Epoch 920/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1606 - val_loss: 0.2780\n",
            "Epoch 921/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1605 - val_loss: 0.2778\n",
            "Epoch 922/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1606 - val_loss: 0.2782\n",
            "Epoch 923/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1606 - val_loss: 0.2779\n",
            "Epoch 924/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1603 - val_loss: 0.2781\n",
            "Epoch 925/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1603 - val_loss: 0.2780\n",
            "Epoch 926/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1603 - val_loss: 0.2780\n",
            "Epoch 927/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1604 - val_loss: 0.2781\n",
            "Epoch 928/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1603 - val_loss: 0.2781\n",
            "Epoch 929/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1603 - val_loss: 0.2782\n",
            "Epoch 930/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1603 - val_loss: 0.2781\n",
            "Epoch 931/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1602 - val_loss: 0.2784\n",
            "Epoch 932/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1605 - val_loss: 0.2782\n",
            "Epoch 933/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1602 - val_loss: 0.2781\n",
            "Epoch 934/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1601 - val_loss: 0.2781\n",
            "Epoch 935/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1602 - val_loss: 0.2782\n",
            "Epoch 936/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1601 - val_loss: 0.2785\n",
            "Epoch 937/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1601 - val_loss: 0.2786\n",
            "Epoch 938/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1602 - val_loss: 0.2781\n",
            "Epoch 939/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1600 - val_loss: 0.2783\n",
            "Epoch 940/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1599 - val_loss: 0.2782\n",
            "Epoch 941/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1600 - val_loss: 0.2783\n",
            "Epoch 942/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1601 - val_loss: 0.2783\n",
            "Epoch 943/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1599 - val_loss: 0.2784\n",
            "Epoch 944/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1599 - val_loss: 0.2783\n",
            "Epoch 945/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1599 - val_loss: 0.2789\n",
            "Epoch 946/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1599 - val_loss: 0.2784\n",
            "Epoch 947/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1598 - val_loss: 0.2792\n",
            "Epoch 948/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1598 - val_loss: 0.2783\n",
            "Epoch 949/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1597 - val_loss: 0.2783\n",
            "Epoch 950/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1598 - val_loss: 0.2785\n",
            "Epoch 951/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1598 - val_loss: 0.2784\n",
            "Epoch 952/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1597 - val_loss: 0.2785\n",
            "Epoch 953/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1596 - val_loss: 0.2787\n",
            "Epoch 954/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1597 - val_loss: 0.2784\n",
            "Epoch 955/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1596 - val_loss: 0.2788\n",
            "Epoch 956/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1597 - val_loss: 0.2785\n",
            "Epoch 957/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1596 - val_loss: 0.2792\n",
            "Epoch 958/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1597 - val_loss: 0.2786\n",
            "Epoch 959/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1595 - val_loss: 0.2788\n",
            "Epoch 960/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1595 - val_loss: 0.2787\n",
            "Epoch 961/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1595 - val_loss: 0.2787\n",
            "Epoch 962/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1594 - val_loss: 0.2787\n",
            "Epoch 963/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1594 - val_loss: 0.2787\n",
            "Epoch 964/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1595 - val_loss: 0.2789\n",
            "Epoch 965/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1596 - val_loss: 0.2788\n",
            "Epoch 966/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1594 - val_loss: 0.2787\n",
            "Epoch 967/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1594 - val_loss: 0.2787\n",
            "Epoch 968/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1593 - val_loss: 0.2787\n",
            "Epoch 969/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1592 - val_loss: 0.2787\n",
            "Epoch 970/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1592 - val_loss: 0.2792\n",
            "Epoch 971/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1594 - val_loss: 0.2789\n",
            "Epoch 972/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1593 - val_loss: 0.2788\n",
            "Epoch 973/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1593 - val_loss: 0.2789\n",
            "Epoch 974/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1592 - val_loss: 0.2789\n",
            "Epoch 975/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1592 - val_loss: 0.2788\n",
            "Epoch 976/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1591 - val_loss: 0.2793\n",
            "Epoch 977/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1591 - val_loss: 0.2791\n",
            "Epoch 978/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1589 - val_loss: 0.2791\n",
            "Epoch 979/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1590 - val_loss: 0.2790\n",
            "Epoch 980/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1592 - val_loss: 0.2790\n",
            "Epoch 981/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1591 - val_loss: 0.2789\n",
            "Epoch 982/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1589 - val_loss: 0.2790\n",
            "Epoch 983/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1589 - val_loss: 0.2790\n",
            "Epoch 984/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1588 - val_loss: 0.2790\n",
            "Epoch 985/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1588 - val_loss: 0.2792\n",
            "Epoch 986/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1589 - val_loss: 0.2791\n",
            "Epoch 987/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1589 - val_loss: 0.2791\n",
            "Epoch 988/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1588 - val_loss: 0.2791\n",
            "Epoch 989/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1588 - val_loss: 0.2796\n",
            "Epoch 990/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1588 - val_loss: 0.2795\n",
            "Epoch 991/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1588 - val_loss: 0.2793\n",
            "Epoch 992/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1587 - val_loss: 0.2791\n",
            "Epoch 993/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1587 - val_loss: 0.2794\n",
            "Epoch 994/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1587 - val_loss: 0.2791\n",
            "Epoch 995/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1587 - val_loss: 0.2792\n",
            "Epoch 996/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1586 - val_loss: 0.2793\n",
            "Epoch 997/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1586 - val_loss: 0.2793\n",
            "Epoch 998/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1586 - val_loss: 0.2792\n",
            "Epoch 999/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1586 - val_loss: 0.2792\n",
            "Epoch 1000/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1585 - val_loss: 0.2795\n",
            "3/3 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0.034874942, 0.2212694204456747, 3.6797975968037435), (0.032329243, 0.23580026189427816, 3.7484075950663445)]\n",
            "Epoch 1/1000\n",
            "34/34 [==============================] - 2s 31ms/step - loss: 0.6074 - val_loss: 0.5525\n",
            "Epoch 2/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.4431 - val_loss: 0.5173\n",
            "Epoch 3/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.4016 - val_loss: 0.4846\n",
            "Epoch 4/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3812 - val_loss: 0.4568\n",
            "Epoch 5/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3663 - val_loss: 0.4320\n",
            "Epoch 6/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3543 - val_loss: 0.4108\n",
            "Epoch 7/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3445 - val_loss: 0.3934\n",
            "Epoch 8/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3361 - val_loss: 0.3798\n",
            "Epoch 9/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3286 - val_loss: 0.3688\n",
            "Epoch 10/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.3220 - val_loss: 0.3594\n",
            "Epoch 11/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3162 - val_loss: 0.3516\n",
            "Epoch 12/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3108 - val_loss: 0.3453\n",
            "Epoch 13/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.3062 - val_loss: 0.3394\n",
            "Epoch 14/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3019 - val_loss: 0.3344\n",
            "Epoch 15/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2981 - val_loss: 0.3301\n",
            "Epoch 16/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2946 - val_loss: 0.3260\n",
            "Epoch 17/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2914 - val_loss: 0.3226\n",
            "Epoch 18/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2886 - val_loss: 0.3195\n",
            "Epoch 19/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2861 - val_loss: 0.3168\n",
            "Epoch 20/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2837 - val_loss: 0.3139\n",
            "Epoch 21/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2815 - val_loss: 0.3116\n",
            "Epoch 22/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2795 - val_loss: 0.3093\n",
            "Epoch 23/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2776 - val_loss: 0.3074\n",
            "Epoch 24/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2758 - val_loss: 0.3054\n",
            "Epoch 25/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2741 - val_loss: 0.3038\n",
            "Epoch 26/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2726 - val_loss: 0.3022\n",
            "Epoch 27/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2712 - val_loss: 0.3006\n",
            "Epoch 28/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2699 - val_loss: 0.2992\n",
            "Epoch 29/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2687 - val_loss: 0.2980\n",
            "Epoch 30/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2676 - val_loss: 0.2967\n",
            "Epoch 31/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2666 - val_loss: 0.2956\n",
            "Epoch 32/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2656 - val_loss: 0.2946\n",
            "Epoch 33/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2647 - val_loss: 0.2935\n",
            "Epoch 34/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2639 - val_loss: 0.2927\n",
            "Epoch 35/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2631 - val_loss: 0.2919\n",
            "Epoch 36/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2624 - val_loss: 0.2912\n",
            "Epoch 37/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2618 - val_loss: 0.2904\n",
            "Epoch 38/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2612 - val_loss: 0.2899\n",
            "Epoch 39/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2606 - val_loss: 0.2893\n",
            "Epoch 40/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2601 - val_loss: 0.2887\n",
            "Epoch 41/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2596 - val_loss: 0.2882\n",
            "Epoch 42/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2593 - val_loss: 0.2877\n",
            "Epoch 43/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2589 - val_loss: 0.2873\n",
            "Epoch 44/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2585 - val_loss: 0.2869\n",
            "Epoch 45/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2582 - val_loss: 0.2866\n",
            "Epoch 46/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2578 - val_loss: 0.2862\n",
            "Epoch 47/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2576 - val_loss: 0.2860\n",
            "Epoch 48/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2574 - val_loss: 0.2857\n",
            "Epoch 49/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2571 - val_loss: 0.2854\n",
            "Epoch 50/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2569 - val_loss: 0.2852\n",
            "Epoch 51/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2565 - val_loss: 0.2850\n",
            "Epoch 52/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2563 - val_loss: 0.2847\n",
            "Epoch 53/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2562 - val_loss: 0.2845\n",
            "Epoch 54/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2561 - val_loss: 0.2844\n",
            "Epoch 55/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2558 - val_loss: 0.2842\n",
            "Epoch 56/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2557 - val_loss: 0.2840\n",
            "Epoch 57/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2555 - val_loss: 0.2838\n",
            "Epoch 58/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2553 - val_loss: 0.2837\n",
            "Epoch 59/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2552 - val_loss: 0.2836\n",
            "Epoch 60/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2551 - val_loss: 0.2834\n",
            "Epoch 61/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2549 - val_loss: 0.2833\n",
            "Epoch 62/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2548 - val_loss: 0.2832\n",
            "Epoch 63/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2546 - val_loss: 0.2831\n",
            "Epoch 64/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2545 - val_loss: 0.2829\n",
            "Epoch 65/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2544 - val_loss: 0.2828\n",
            "Epoch 66/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2543 - val_loss: 0.2827\n",
            "Epoch 67/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2543 - val_loss: 0.2826\n",
            "Epoch 68/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2541 - val_loss: 0.2825\n",
            "Epoch 69/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2540 - val_loss: 0.2825\n",
            "Epoch 70/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2539 - val_loss: 0.2823\n",
            "Epoch 71/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2538 - val_loss: 0.2823\n",
            "Epoch 72/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2536 - val_loss: 0.2822\n",
            "Epoch 73/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2535 - val_loss: 0.2821\n",
            "Epoch 74/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2534 - val_loss: 0.2821\n",
            "Epoch 75/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2534 - val_loss: 0.2820\n",
            "Epoch 76/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2532 - val_loss: 0.2819\n",
            "Epoch 77/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2531 - val_loss: 0.2818\n",
            "Epoch 78/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2530 - val_loss: 0.2817\n",
            "Epoch 79/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2530 - val_loss: 0.2817\n",
            "Epoch 80/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2528 - val_loss: 0.2816\n",
            "Epoch 81/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2527 - val_loss: 0.2816\n",
            "Epoch 82/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2526 - val_loss: 0.2815\n",
            "Epoch 83/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2525 - val_loss: 0.2814\n",
            "Epoch 84/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2524 - val_loss: 0.2813\n",
            "Epoch 85/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2524 - val_loss: 0.2813\n",
            "Epoch 86/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2522 - val_loss: 0.2812\n",
            "Epoch 87/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2522 - val_loss: 0.2811\n",
            "Epoch 88/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2521 - val_loss: 0.2811\n",
            "Epoch 89/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2519 - val_loss: 0.2811\n",
            "Epoch 90/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2518 - val_loss: 0.2811\n",
            "Epoch 91/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2518 - val_loss: 0.2810\n",
            "Epoch 92/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2516 - val_loss: 0.2809\n",
            "Epoch 93/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2515 - val_loss: 0.2808\n",
            "Epoch 94/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2514 - val_loss: 0.2808\n",
            "Epoch 95/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2513 - val_loss: 0.2807\n",
            "Epoch 96/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2512 - val_loss: 0.2806\n",
            "Epoch 97/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2511 - val_loss: 0.2806\n",
            "Epoch 98/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2510 - val_loss: 0.2805\n",
            "Epoch 99/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2509 - val_loss: 0.2805\n",
            "Epoch 100/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2507 - val_loss: 0.2804\n",
            "Epoch 101/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2505 - val_loss: 0.2803\n",
            "Epoch 102/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2505 - val_loss: 0.2803\n",
            "Epoch 103/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2503 - val_loss: 0.2803\n",
            "Epoch 104/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2503 - val_loss: 0.2802\n",
            "Epoch 105/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2502 - val_loss: 0.2802\n",
            "Epoch 106/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2500 - val_loss: 0.2801\n",
            "Epoch 107/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2498 - val_loss: 0.2800\n",
            "Epoch 108/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2497 - val_loss: 0.2800\n",
            "Epoch 109/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2496 - val_loss: 0.2799\n",
            "Epoch 110/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2494 - val_loss: 0.2799\n",
            "Epoch 111/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2493 - val_loss: 0.2798\n",
            "Epoch 112/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2492 - val_loss: 0.2798\n",
            "Epoch 113/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2492 - val_loss: 0.2797\n",
            "Epoch 114/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2489 - val_loss: 0.2797\n",
            "Epoch 115/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2488 - val_loss: 0.2797\n",
            "Epoch 116/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2487 - val_loss: 0.2796\n",
            "Epoch 117/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2485 - val_loss: 0.2795\n",
            "Epoch 118/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2484 - val_loss: 0.2794\n",
            "Epoch 119/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2482 - val_loss: 0.2794\n",
            "Epoch 120/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2480 - val_loss: 0.2793\n",
            "Epoch 121/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2478 - val_loss: 0.2793\n",
            "Epoch 122/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2477 - val_loss: 0.2792\n",
            "Epoch 123/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2476 - val_loss: 0.2791\n",
            "Epoch 124/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2475 - val_loss: 0.2791\n",
            "Epoch 125/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2473 - val_loss: 0.2790\n",
            "Epoch 126/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2472 - val_loss: 0.2790\n",
            "Epoch 127/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2469 - val_loss: 0.2789\n",
            "Epoch 128/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2468 - val_loss: 0.2789\n",
            "Epoch 129/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2466 - val_loss: 0.2789\n",
            "Epoch 130/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2465 - val_loss: 0.2788\n",
            "Epoch 131/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2463 - val_loss: 0.2787\n",
            "Epoch 132/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2462 - val_loss: 0.2786\n",
            "Epoch 133/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2459 - val_loss: 0.2786\n",
            "Epoch 134/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2458 - val_loss: 0.2785\n",
            "Epoch 135/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2456 - val_loss: 0.2785\n",
            "Epoch 136/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2454 - val_loss: 0.2784\n",
            "Epoch 137/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2452 - val_loss: 0.2784\n",
            "Epoch 138/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2451 - val_loss: 0.2783\n",
            "Epoch 139/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2449 - val_loss: 0.2782\n",
            "Epoch 140/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2447 - val_loss: 0.2781\n",
            "Epoch 141/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2445 - val_loss: 0.2781\n",
            "Epoch 142/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2444 - val_loss: 0.2781\n",
            "Epoch 143/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2442 - val_loss: 0.2780\n",
            "Epoch 144/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2440 - val_loss: 0.2780\n",
            "Epoch 145/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2439 - val_loss: 0.2779\n",
            "Epoch 146/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2437 - val_loss: 0.2778\n",
            "Epoch 147/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2434 - val_loss: 0.2778\n",
            "Epoch 148/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2433 - val_loss: 0.2777\n",
            "Epoch 149/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2431 - val_loss: 0.2776\n",
            "Epoch 150/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2429 - val_loss: 0.2776\n",
            "Epoch 151/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2427 - val_loss: 0.2775\n",
            "Epoch 152/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2426 - val_loss: 0.2774\n",
            "Epoch 153/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2423 - val_loss: 0.2774\n",
            "Epoch 154/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2422 - val_loss: 0.2774\n",
            "Epoch 155/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2420 - val_loss: 0.2773\n",
            "Epoch 156/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2418 - val_loss: 0.2772\n",
            "Epoch 157/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2416 - val_loss: 0.2771\n",
            "Epoch 158/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2415 - val_loss: 0.2771\n",
            "Epoch 159/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2412 - val_loss: 0.2770\n",
            "Epoch 160/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2410 - val_loss: 0.2770\n",
            "Epoch 161/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2408 - val_loss: 0.2770\n",
            "Epoch 162/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2405 - val_loss: 0.2769\n",
            "Epoch 163/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2404 - val_loss: 0.2768\n",
            "Epoch 164/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2403 - val_loss: 0.2768\n",
            "Epoch 165/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2401 - val_loss: 0.2767\n",
            "Epoch 166/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2399 - val_loss: 0.2766\n",
            "Epoch 167/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2397 - val_loss: 0.2766\n",
            "Epoch 168/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2394 - val_loss: 0.2765\n",
            "Epoch 169/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2393 - val_loss: 0.2764\n",
            "Epoch 170/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2391 - val_loss: 0.2764\n",
            "Epoch 171/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2388 - val_loss: 0.2763\n",
            "Epoch 172/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2387 - val_loss: 0.2763\n",
            "Epoch 173/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2385 - val_loss: 0.2762\n",
            "Epoch 174/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2382 - val_loss: 0.2762\n",
            "Epoch 175/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2380 - val_loss: 0.2761\n",
            "Epoch 176/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2378 - val_loss: 0.2761\n",
            "Epoch 177/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2377 - val_loss: 0.2760\n",
            "Epoch 178/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2376 - val_loss: 0.2760\n",
            "Epoch 179/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2372 - val_loss: 0.2759\n",
            "Epoch 180/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2370 - val_loss: 0.2759\n",
            "Epoch 181/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2369 - val_loss: 0.2758\n",
            "Epoch 182/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2367 - val_loss: 0.2757\n",
            "Epoch 183/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2364 - val_loss: 0.2757\n",
            "Epoch 184/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2363 - val_loss: 0.2757\n",
            "Epoch 185/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2361 - val_loss: 0.2756\n",
            "Epoch 186/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2359 - val_loss: 0.2755\n",
            "Epoch 187/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2356 - val_loss: 0.2754\n",
            "Epoch 188/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2355 - val_loss: 0.2757\n",
            "Epoch 189/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2352 - val_loss: 0.2753\n",
            "Epoch 190/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2351 - val_loss: 0.2753\n",
            "Epoch 191/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2349 - val_loss: 0.2752\n",
            "Epoch 192/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2347 - val_loss: 0.2753\n",
            "Epoch 193/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2345 - val_loss: 0.2753\n",
            "Epoch 194/1000\n",
            "34/34 [==============================] - 1s 21ms/step - loss: 0.2343 - val_loss: 0.2751\n",
            "Epoch 195/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2340 - val_loss: 0.2750\n",
            "Epoch 196/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2338 - val_loss: 0.2749\n",
            "Epoch 197/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2336 - val_loss: 0.2749\n",
            "Epoch 198/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2335 - val_loss: 0.2750\n",
            "Epoch 199/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2332 - val_loss: 0.2748\n",
            "Epoch 200/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2331 - val_loss: 0.2748\n",
            "Epoch 201/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2329 - val_loss: 0.2747\n",
            "Epoch 202/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2327 - val_loss: 0.2748\n",
            "Epoch 203/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2325 - val_loss: 0.2746\n",
            "Epoch 204/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2322 - val_loss: 0.2747\n",
            "Epoch 205/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2321 - val_loss: 0.2745\n",
            "Epoch 206/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2319 - val_loss: 0.2745\n",
            "Epoch 207/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2317 - val_loss: 0.2743\n",
            "Epoch 208/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2314 - val_loss: 0.2744\n",
            "Epoch 209/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2313 - val_loss: 0.2743\n",
            "Epoch 210/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2310 - val_loss: 0.2743\n",
            "Epoch 211/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2308 - val_loss: 0.2743\n",
            "Epoch 212/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2307 - val_loss: 0.2742\n",
            "Epoch 213/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2304 - val_loss: 0.2741\n",
            "Epoch 214/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2303 - val_loss: 0.2740\n",
            "Epoch 215/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2301 - val_loss: 0.2741\n",
            "Epoch 216/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2298 - val_loss: 0.2739\n",
            "Epoch 217/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2297 - val_loss: 0.2739\n",
            "Epoch 218/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2295 - val_loss: 0.2739\n",
            "Epoch 219/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2293 - val_loss: 0.2738\n",
            "Epoch 220/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2291 - val_loss: 0.2737\n",
            "Epoch 221/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2288 - val_loss: 0.2737\n",
            "Epoch 222/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2287 - val_loss: 0.2737\n",
            "Epoch 223/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2284 - val_loss: 0.2736\n",
            "Epoch 224/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2284 - val_loss: 0.2735\n",
            "Epoch 225/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2281 - val_loss: 0.2737\n",
            "Epoch 226/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2280 - val_loss: 0.2734\n",
            "Epoch 227/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2276 - val_loss: 0.2735\n",
            "Epoch 228/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2275 - val_loss: 0.2735\n",
            "Epoch 229/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2274 - val_loss: 0.2734\n",
            "Epoch 230/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2271 - val_loss: 0.2733\n",
            "Epoch 231/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2269 - val_loss: 0.2732\n",
            "Epoch 232/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2268 - val_loss: 0.2732\n",
            "Epoch 233/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2265 - val_loss: 0.2732\n",
            "Epoch 234/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2263 - val_loss: 0.2732\n",
            "Epoch 235/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2261 - val_loss: 0.2731\n",
            "Epoch 236/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2260 - val_loss: 0.2730\n",
            "Epoch 237/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2258 - val_loss: 0.2731\n",
            "Epoch 238/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2256 - val_loss: 0.2730\n",
            "Epoch 239/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2253 - val_loss: 0.2729\n",
            "Epoch 240/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2252 - val_loss: 0.2730\n",
            "Epoch 241/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2250 - val_loss: 0.2729\n",
            "Epoch 242/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2248 - val_loss: 0.2728\n",
            "Epoch 243/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2247 - val_loss: 0.2728\n",
            "Epoch 244/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2245 - val_loss: 0.2727\n",
            "Epoch 245/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2243 - val_loss: 0.2727\n",
            "Epoch 246/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2241 - val_loss: 0.2726\n",
            "Epoch 247/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2239 - val_loss: 0.2726\n",
            "Epoch 248/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2237 - val_loss: 0.2725\n",
            "Epoch 249/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2235 - val_loss: 0.2726\n",
            "Epoch 250/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2234 - val_loss: 0.2725\n",
            "Epoch 251/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2231 - val_loss: 0.2725\n",
            "Epoch 252/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2230 - val_loss: 0.2727\n",
            "Epoch 253/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2228 - val_loss: 0.2724\n",
            "Epoch 254/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2226 - val_loss: 0.2724\n",
            "Epoch 255/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2225 - val_loss: 0.2722\n",
            "Epoch 256/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2222 - val_loss: 0.2722\n",
            "Epoch 257/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2221 - val_loss: 0.2725\n",
            "Epoch 258/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2219 - val_loss: 0.2722\n",
            "Epoch 259/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2218 - val_loss: 0.2721\n",
            "Epoch 260/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2216 - val_loss: 0.2722\n",
            "Epoch 261/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2214 - val_loss: 0.2722\n",
            "Epoch 262/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2212 - val_loss: 0.2721\n",
            "Epoch 263/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2210 - val_loss: 0.2720\n",
            "Epoch 264/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2208 - val_loss: 0.2720\n",
            "Epoch 265/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2206 - val_loss: 0.2719\n",
            "Epoch 266/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2205 - val_loss: 0.2720\n",
            "Epoch 267/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2202 - val_loss: 0.2719\n",
            "Epoch 268/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2202 - val_loss: 0.2718\n",
            "Epoch 269/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2199 - val_loss: 0.2718\n",
            "Epoch 270/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2198 - val_loss: 0.2719\n",
            "Epoch 271/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2196 - val_loss: 0.2718\n",
            "Epoch 272/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2194 - val_loss: 0.2717\n",
            "Epoch 273/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2193 - val_loss: 0.2717\n",
            "Epoch 274/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2190 - val_loss: 0.2720\n",
            "Epoch 275/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2188 - val_loss: 0.2716\n",
            "Epoch 276/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2187 - val_loss: 0.2717\n",
            "Epoch 277/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2185 - val_loss: 0.2716\n",
            "Epoch 278/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2185 - val_loss: 0.2715\n",
            "Epoch 279/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2182 - val_loss: 0.2716\n",
            "Epoch 280/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2182 - val_loss: 0.2716\n",
            "Epoch 281/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2179 - val_loss: 0.2717\n",
            "Epoch 282/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2177 - val_loss: 0.2714\n",
            "Epoch 283/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2175 - val_loss: 0.2715\n",
            "Epoch 284/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2174 - val_loss: 0.2715\n",
            "Epoch 285/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2172 - val_loss: 0.2713\n",
            "Epoch 286/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2170 - val_loss: 0.2715\n",
            "Epoch 287/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2169 - val_loss: 0.2713\n",
            "Epoch 288/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2167 - val_loss: 0.2714\n",
            "Epoch 289/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2165 - val_loss: 0.2713\n",
            "Epoch 290/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2163 - val_loss: 0.2713\n",
            "Epoch 291/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2162 - val_loss: 0.2713\n",
            "Epoch 292/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2160 - val_loss: 0.2711\n",
            "Epoch 293/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2158 - val_loss: 0.2712\n",
            "Epoch 294/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2157 - val_loss: 0.2711\n",
            "Epoch 295/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2155 - val_loss: 0.2713\n",
            "Epoch 296/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2154 - val_loss: 0.2713\n",
            "Epoch 297/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2152 - val_loss: 0.2709\n",
            "Epoch 298/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2150 - val_loss: 0.2710\n",
            "Epoch 299/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2148 - val_loss: 0.2710\n",
            "Epoch 300/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2147 - val_loss: 0.2710\n",
            "Epoch 301/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2145 - val_loss: 0.2710\n",
            "Epoch 302/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2143 - val_loss: 0.2709\n",
            "Epoch 303/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2143 - val_loss: 0.2711\n",
            "Epoch 304/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2141 - val_loss: 0.2711\n",
            "Epoch 305/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2139 - val_loss: 0.2714\n",
            "Epoch 306/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2138 - val_loss: 0.2708\n",
            "Epoch 307/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2136 - val_loss: 0.2707\n",
            "Epoch 308/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2135 - val_loss: 0.2707\n",
            "Epoch 309/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2133 - val_loss: 0.2708\n",
            "Epoch 310/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2132 - val_loss: 0.2707\n",
            "Epoch 311/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2129 - val_loss: 0.2709\n",
            "Epoch 312/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2129 - val_loss: 0.2707\n",
            "Epoch 313/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2128 - val_loss: 0.2709\n",
            "Epoch 314/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2125 - val_loss: 0.2710\n",
            "Epoch 315/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2124 - val_loss: 0.2707\n",
            "Epoch 316/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2122 - val_loss: 0.2710\n",
            "Epoch 317/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2120 - val_loss: 0.2706\n",
            "Epoch 318/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2119 - val_loss: 0.2714\n",
            "Epoch 319/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2118 - val_loss: 0.2707\n",
            "Epoch 320/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2115 - val_loss: 0.2705\n",
            "Epoch 321/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2115 - val_loss: 0.2704\n",
            "Epoch 322/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2113 - val_loss: 0.2704\n",
            "Epoch 323/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2112 - val_loss: 0.2706\n",
            "Epoch 324/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2111 - val_loss: 0.2705\n",
            "Epoch 325/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2109 - val_loss: 0.2704\n",
            "Epoch 326/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2107 - val_loss: 0.2703\n",
            "Epoch 327/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2105 - val_loss: 0.2704\n",
            "Epoch 328/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2105 - val_loss: 0.2704\n",
            "Epoch 329/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2103 - val_loss: 0.2703\n",
            "Epoch 330/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2101 - val_loss: 0.2703\n",
            "Epoch 331/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2100 - val_loss: 0.2703\n",
            "Epoch 332/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2098 - val_loss: 0.2707\n",
            "Epoch 333/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2097 - val_loss: 0.2702\n",
            "Epoch 334/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2095 - val_loss: 0.2703\n",
            "Epoch 335/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2095 - val_loss: 0.2702\n",
            "Epoch 336/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2091 - val_loss: 0.2702\n",
            "Epoch 337/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2092 - val_loss: 0.2704\n",
            "Epoch 338/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2090 - val_loss: 0.2706\n",
            "Epoch 339/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2088 - val_loss: 0.2702\n",
            "Epoch 340/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2087 - val_loss: 0.2703\n",
            "Epoch 341/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2086 - val_loss: 0.2701\n",
            "Epoch 342/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2084 - val_loss: 0.2702\n",
            "Epoch 343/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2081 - val_loss: 0.2700\n",
            "Epoch 344/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2082 - val_loss: 0.2700\n",
            "Epoch 345/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2079 - val_loss: 0.2700\n",
            "Epoch 346/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2078 - val_loss: 0.2700\n",
            "Epoch 347/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2076 - val_loss: 0.2702\n",
            "Epoch 348/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2075 - val_loss: 0.2700\n",
            "Epoch 349/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2074 - val_loss: 0.2702\n",
            "Epoch 350/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2073 - val_loss: 0.2701\n",
            "Epoch 351/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2071 - val_loss: 0.2700\n",
            "Epoch 352/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2070 - val_loss: 0.2702\n",
            "Epoch 353/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2068 - val_loss: 0.2700\n",
            "Epoch 354/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2067 - val_loss: 0.2699\n",
            "Epoch 355/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2066 - val_loss: 0.2699\n",
            "Epoch 356/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2064 - val_loss: 0.2702\n",
            "Epoch 357/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2064 - val_loss: 0.2700\n",
            "Epoch 358/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2061 - val_loss: 0.2700\n",
            "Epoch 359/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2061 - val_loss: 0.2701\n",
            "Epoch 360/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2059 - val_loss: 0.2699\n",
            "Epoch 361/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2058 - val_loss: 0.2698\n",
            "Epoch 362/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2056 - val_loss: 0.2699\n",
            "Epoch 363/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2055 - val_loss: 0.2701\n",
            "Epoch 364/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2054 - val_loss: 0.2698\n",
            "Epoch 365/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2052 - val_loss: 0.2697\n",
            "Epoch 366/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2052 - val_loss: 0.2698\n",
            "Epoch 367/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2049 - val_loss: 0.2697\n",
            "Epoch 368/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2048 - val_loss: 0.2700\n",
            "Epoch 369/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2046 - val_loss: 0.2698\n",
            "Epoch 370/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2048 - val_loss: 0.2698\n",
            "Epoch 371/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2044 - val_loss: 0.2698\n",
            "Epoch 372/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2042 - val_loss: 0.2700\n",
            "Epoch 373/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2042 - val_loss: 0.2697\n",
            "Epoch 374/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2040 - val_loss: 0.2699\n",
            "Epoch 375/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2040 - val_loss: 0.2700\n",
            "Epoch 376/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2037 - val_loss: 0.2698\n",
            "Epoch 377/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2037 - val_loss: 0.2698\n",
            "Epoch 378/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2036 - val_loss: 0.2699\n",
            "Epoch 379/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2034 - val_loss: 0.2703\n",
            "Epoch 380/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2033 - val_loss: 0.2698\n",
            "Epoch 381/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2031 - val_loss: 0.2696\n",
            "Epoch 382/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2029 - val_loss: 0.2697\n",
            "Epoch 383/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2029 - val_loss: 0.2700\n",
            "Epoch 384/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2027 - val_loss: 0.2697\n",
            "Epoch 385/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2027 - val_loss: 0.2697\n",
            "Epoch 386/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2027 - val_loss: 0.2697\n",
            "Epoch 387/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2024 - val_loss: 0.2695\n",
            "Epoch 388/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2022 - val_loss: 0.2700\n",
            "Epoch 389/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2021 - val_loss: 0.2698\n",
            "Epoch 390/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2020 - val_loss: 0.2697\n",
            "Epoch 391/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2019 - val_loss: 0.2696\n",
            "Epoch 392/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2017 - val_loss: 0.2694\n",
            "Epoch 393/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2015 - val_loss: 0.2695\n",
            "Epoch 394/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2016 - val_loss: 0.2695\n",
            "Epoch 395/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2014 - val_loss: 0.2698\n",
            "Epoch 396/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2012 - val_loss: 0.2700\n",
            "Epoch 397/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2012 - val_loss: 0.2697\n",
            "Epoch 398/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2011 - val_loss: 0.2695\n",
            "Epoch 399/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2009 - val_loss: 0.2696\n",
            "Epoch 400/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2008 - val_loss: 0.2694\n",
            "Epoch 401/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2007 - val_loss: 0.2700\n",
            "Epoch 402/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2006 - val_loss: 0.2699\n",
            "Epoch 403/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2005 - val_loss: 0.2694\n",
            "Epoch 404/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2003 - val_loss: 0.2694\n",
            "Epoch 405/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2002 - val_loss: 0.2695\n",
            "Epoch 406/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2002 - val_loss: 0.2695\n",
            "Epoch 407/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1999 - val_loss: 0.2695\n",
            "Epoch 408/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2000 - val_loss: 0.2695\n",
            "Epoch 409/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1997 - val_loss: 0.2694\n",
            "Epoch 410/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1996 - val_loss: 0.2695\n",
            "Epoch 411/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1995 - val_loss: 0.2694\n",
            "Epoch 412/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1993 - val_loss: 0.2696\n",
            "Epoch 413/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1993 - val_loss: 0.2695\n",
            "Epoch 414/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1991 - val_loss: 0.2695\n",
            "Epoch 415/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1990 - val_loss: 0.2695\n",
            "Epoch 416/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1988 - val_loss: 0.2694\n",
            "Epoch 417/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1988 - val_loss: 0.2698\n",
            "Epoch 418/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1987 - val_loss: 0.2694\n",
            "Epoch 419/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1985 - val_loss: 0.2695\n",
            "Epoch 420/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1984 - val_loss: 0.2699\n",
            "Epoch 421/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1983 - val_loss: 0.2696\n",
            "Epoch 422/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1981 - val_loss: 0.2701\n",
            "Epoch 423/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1981 - val_loss: 0.2695\n",
            "Epoch 424/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1981 - val_loss: 0.2699\n",
            "Epoch 425/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1980 - val_loss: 0.2694\n",
            "Epoch 426/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1978 - val_loss: 0.2696\n",
            "Epoch 427/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1977 - val_loss: 0.2694\n",
            "Epoch 428/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1975 - val_loss: 0.2695\n",
            "Epoch 429/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1975 - val_loss: 0.2694\n",
            "Epoch 430/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1973 - val_loss: 0.2693\n",
            "Epoch 431/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1972 - val_loss: 0.2695\n",
            "Epoch 432/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1971 - val_loss: 0.2693\n",
            "Epoch 433/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1970 - val_loss: 0.2700\n",
            "Epoch 434/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1971 - val_loss: 0.2693\n",
            "Epoch 435/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1967 - val_loss: 0.2696\n",
            "Epoch 436/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1967 - val_loss: 0.2694\n",
            "Epoch 437/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1965 - val_loss: 0.2693\n",
            "Epoch 438/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1964 - val_loss: 0.2694\n",
            "Epoch 439/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1964 - val_loss: 0.2693\n",
            "Epoch 440/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1963 - val_loss: 0.2694\n",
            "Epoch 441/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1962 - val_loss: 0.2693\n",
            "Epoch 442/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1961 - val_loss: 0.2695\n",
            "Epoch 443/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1960 - val_loss: 0.2694\n",
            "Epoch 444/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1958 - val_loss: 0.2693\n",
            "Epoch 445/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1957 - val_loss: 0.2694\n",
            "Epoch 446/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1956 - val_loss: 0.2694\n",
            "Epoch 447/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1956 - val_loss: 0.2694\n",
            "Epoch 448/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1954 - val_loss: 0.2693\n",
            "Epoch 449/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1952 - val_loss: 0.2692\n",
            "Epoch 450/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1953 - val_loss: 0.2695\n",
            "Epoch 451/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1951 - val_loss: 0.2694\n",
            "Epoch 452/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1951 - val_loss: 0.2694\n",
            "Epoch 453/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1949 - val_loss: 0.2694\n",
            "Epoch 454/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1948 - val_loss: 0.2694\n",
            "Epoch 455/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1948 - val_loss: 0.2697\n",
            "Epoch 456/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1946 - val_loss: 0.2695\n",
            "Epoch 457/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1945 - val_loss: 0.2694\n",
            "Epoch 458/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1943 - val_loss: 0.2693\n",
            "Epoch 459/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1943 - val_loss: 0.2697\n",
            "Epoch 460/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1942 - val_loss: 0.2693\n",
            "Epoch 461/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1940 - val_loss: 0.2694\n",
            "Epoch 462/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1938 - val_loss: 0.2694\n",
            "Epoch 463/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1938 - val_loss: 0.2694\n",
            "Epoch 464/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1938 - val_loss: 0.2693\n",
            "Epoch 465/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1936 - val_loss: 0.2694\n",
            "Epoch 466/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1936 - val_loss: 0.2692\n",
            "Epoch 467/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1935 - val_loss: 0.2699\n",
            "Epoch 468/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1934 - val_loss: 0.2695\n",
            "Epoch 469/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1933 - val_loss: 0.2696\n",
            "Epoch 470/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1931 - val_loss: 0.2695\n",
            "Epoch 471/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1932 - val_loss: 0.2693\n",
            "Epoch 472/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1929 - val_loss: 0.2696\n",
            "Epoch 473/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1929 - val_loss: 0.2693\n",
            "Epoch 474/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1928 - val_loss: 0.2696\n",
            "Epoch 475/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1927 - val_loss: 0.2706\n",
            "Epoch 476/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1927 - val_loss: 0.2693\n",
            "Epoch 477/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1925 - val_loss: 0.2696\n",
            "Epoch 478/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1923 - val_loss: 0.2695\n",
            "Epoch 479/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1922 - val_loss: 0.2715\n",
            "Epoch 480/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1924 - val_loss: 0.2694\n",
            "Epoch 481/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1921 - val_loss: 0.2693\n",
            "Epoch 482/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1919 - val_loss: 0.2693\n",
            "Epoch 483/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1919 - val_loss: 0.2695\n",
            "Epoch 484/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1918 - val_loss: 0.2694\n",
            "Epoch 485/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1917 - val_loss: 0.2694\n",
            "Epoch 486/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1916 - val_loss: 0.2693\n",
            "Epoch 487/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1916 - val_loss: 0.2693\n",
            "Epoch 488/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1914 - val_loss: 0.2694\n",
            "Epoch 489/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1915 - val_loss: 0.2694\n",
            "Epoch 490/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1913 - val_loss: 0.2698\n",
            "Epoch 491/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1913 - val_loss: 0.2694\n",
            "Epoch 492/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1910 - val_loss: 0.2693\n",
            "Epoch 493/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1909 - val_loss: 0.2697\n",
            "Epoch 494/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1908 - val_loss: 0.2698\n",
            "Epoch 495/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1908 - val_loss: 0.2694\n",
            "Epoch 496/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1907 - val_loss: 0.2694\n",
            "Epoch 497/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1906 - val_loss: 0.2696\n",
            "Epoch 498/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1905 - val_loss: 0.2695\n",
            "Epoch 499/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1904 - val_loss: 0.2693\n",
            "Epoch 500/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1903 - val_loss: 0.2694\n",
            "Epoch 501/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1901 - val_loss: 0.2696\n",
            "Epoch 502/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1903 - val_loss: 0.2694\n",
            "Epoch 503/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1900 - val_loss: 0.2697\n",
            "Epoch 504/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1899 - val_loss: 0.2696\n",
            "Epoch 505/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1899 - val_loss: 0.2694\n",
            "Epoch 506/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1897 - val_loss: 0.2694\n",
            "Epoch 507/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1896 - val_loss: 0.2696\n",
            "Epoch 508/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1896 - val_loss: 0.2699\n",
            "Epoch 509/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1896 - val_loss: 0.2695\n",
            "Epoch 510/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1894 - val_loss: 0.2694\n",
            "Epoch 511/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1893 - val_loss: 0.2695\n",
            "Epoch 512/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1892 - val_loss: 0.2694\n",
            "Epoch 513/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1892 - val_loss: 0.2697\n",
            "Epoch 514/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1890 - val_loss: 0.2695\n",
            "Epoch 515/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1891 - val_loss: 0.2698\n",
            "Epoch 516/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1891 - val_loss: 0.2698\n",
            "Epoch 517/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1888 - val_loss: 0.2697\n",
            "Epoch 518/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1888 - val_loss: 0.2695\n",
            "Epoch 519/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1886 - val_loss: 0.2696\n",
            "Epoch 520/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1887 - val_loss: 0.2698\n",
            "Epoch 521/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1885 - val_loss: 0.2694\n",
            "Epoch 522/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1884 - val_loss: 0.2695\n",
            "Epoch 523/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1883 - val_loss: 0.2695\n",
            "Epoch 524/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1883 - val_loss: 0.2697\n",
            "Epoch 525/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1881 - val_loss: 0.2696\n",
            "Epoch 526/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1881 - val_loss: 0.2696\n",
            "Epoch 527/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1880 - val_loss: 0.2696\n",
            "Epoch 528/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1878 - val_loss: 0.2705\n",
            "Epoch 529/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1880 - val_loss: 0.2698\n",
            "Epoch 530/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1876 - val_loss: 0.2696\n",
            "Epoch 531/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1875 - val_loss: 0.2694\n",
            "Epoch 532/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1876 - val_loss: 0.2707\n",
            "Epoch 533/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1876 - val_loss: 0.2697\n",
            "Epoch 534/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1874 - val_loss: 0.2696\n",
            "Epoch 535/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1873 - val_loss: 0.2696\n",
            "Epoch 536/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1871 - val_loss: 0.2698\n",
            "Epoch 537/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1871 - val_loss: 0.2695\n",
            "Epoch 538/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1870 - val_loss: 0.2707\n",
            "Epoch 539/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1869 - val_loss: 0.2702\n",
            "Epoch 540/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1870 - val_loss: 0.2695\n",
            "Epoch 541/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1867 - val_loss: 0.2697\n",
            "Epoch 542/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1867 - val_loss: 0.2700\n",
            "Epoch 543/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1867 - val_loss: 0.2697\n",
            "Epoch 544/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1867 - val_loss: 0.2698\n",
            "Epoch 545/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1867 - val_loss: 0.2700\n",
            "Epoch 546/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1864 - val_loss: 0.2697\n",
            "Epoch 547/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1863 - val_loss: 0.2696\n",
            "Epoch 548/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1861 - val_loss: 0.2707\n",
            "Epoch 549/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1862 - val_loss: 0.2698\n",
            "Epoch 550/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1861 - val_loss: 0.2699\n",
            "Epoch 551/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1860 - val_loss: 0.2697\n",
            "Epoch 552/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1858 - val_loss: 0.2697\n",
            "Epoch 553/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1858 - val_loss: 0.2696\n",
            "Epoch 554/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1858 - val_loss: 0.2699\n",
            "Epoch 555/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1857 - val_loss: 0.2697\n",
            "Epoch 556/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1856 - val_loss: 0.2698\n",
            "Epoch 557/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1854 - val_loss: 0.2698\n",
            "Epoch 558/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1854 - val_loss: 0.2699\n",
            "Epoch 559/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1854 - val_loss: 0.2700\n",
            "Epoch 560/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1853 - val_loss: 0.2699\n",
            "Epoch 561/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1853 - val_loss: 0.2698\n",
            "Epoch 562/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1851 - val_loss: 0.2699\n",
            "Epoch 563/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1851 - val_loss: 0.2698\n",
            "Epoch 564/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1850 - val_loss: 0.2698\n",
            "Epoch 565/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1848 - val_loss: 0.2700\n",
            "Epoch 566/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1849 - val_loss: 0.2698\n",
            "Epoch 567/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1848 - val_loss: 0.2698\n",
            "Epoch 568/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1846 - val_loss: 0.2698\n",
            "Epoch 569/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1845 - val_loss: 0.2700\n",
            "Epoch 570/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1846 - val_loss: 0.2702\n",
            "Epoch 571/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1845 - val_loss: 0.2700\n",
            "Epoch 572/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1843 - val_loss: 0.2706\n",
            "Epoch 573/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1844 - val_loss: 0.2700\n",
            "Epoch 574/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1842 - val_loss: 0.2700\n",
            "Epoch 575/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1842 - val_loss: 0.2703\n",
            "Epoch 576/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1841 - val_loss: 0.2699\n",
            "Epoch 577/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1840 - val_loss: 0.2700\n",
            "Epoch 578/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1839 - val_loss: 0.2703\n",
            "Epoch 579/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1840 - val_loss: 0.2698\n",
            "Epoch 580/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1838 - val_loss: 0.2700\n",
            "Epoch 581/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1837 - val_loss: 0.2699\n",
            "Epoch 582/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1837 - val_loss: 0.2703\n",
            "Epoch 583/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1835 - val_loss: 0.2699\n",
            "Epoch 584/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1834 - val_loss: 0.2700\n",
            "Epoch 585/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1834 - val_loss: 0.2699\n",
            "Epoch 586/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1833 - val_loss: 0.2699\n",
            "Epoch 587/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1832 - val_loss: 0.2707\n",
            "Epoch 588/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1832 - val_loss: 0.2700\n",
            "Epoch 589/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1831 - val_loss: 0.2699\n",
            "Epoch 590/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1832 - val_loss: 0.2703\n",
            "Epoch 591/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1830 - val_loss: 0.2700\n",
            "Epoch 592/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1828 - val_loss: 0.2703\n",
            "Epoch 593/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1828 - val_loss: 0.2700\n",
            "Epoch 594/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1827 - val_loss: 0.2700\n",
            "Epoch 595/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1826 - val_loss: 0.2700\n",
            "Epoch 596/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1828 - val_loss: 0.2701\n",
            "Epoch 597/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1824 - val_loss: 0.2701\n",
            "Epoch 598/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1825 - val_loss: 0.2701\n",
            "Epoch 599/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1824 - val_loss: 0.2702\n",
            "Epoch 600/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1824 - val_loss: 0.2704\n",
            "Epoch 601/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1822 - val_loss: 0.2709\n",
            "Epoch 602/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1822 - val_loss: 0.2702\n",
            "Epoch 603/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1821 - val_loss: 0.2702\n",
            "Epoch 604/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1823 - val_loss: 0.2703\n",
            "Epoch 605/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1820 - val_loss: 0.2701\n",
            "Epoch 606/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1819 - val_loss: 0.2703\n",
            "Epoch 607/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1818 - val_loss: 0.2704\n",
            "Epoch 608/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1817 - val_loss: 0.2704\n",
            "Epoch 609/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1818 - val_loss: 0.2706\n",
            "Epoch 610/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1816 - val_loss: 0.2703\n",
            "Epoch 611/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1815 - val_loss: 0.2706\n",
            "Epoch 612/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1815 - val_loss: 0.2702\n",
            "Epoch 613/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1815 - val_loss: 0.2703\n",
            "Epoch 614/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1814 - val_loss: 0.2704\n",
            "Epoch 615/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1812 - val_loss: 0.2703\n",
            "Epoch 616/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1813 - val_loss: 0.2704\n",
            "Epoch 617/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1812 - val_loss: 0.2704\n",
            "Epoch 618/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1811 - val_loss: 0.2702\n",
            "Epoch 619/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1811 - val_loss: 0.2703\n",
            "Epoch 620/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1810 - val_loss: 0.2703\n",
            "Epoch 621/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1810 - val_loss: 0.2706\n",
            "Epoch 622/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1810 - val_loss: 0.2707\n",
            "Epoch 623/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1807 - val_loss: 0.2704\n",
            "Epoch 624/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1807 - val_loss: 0.2705\n",
            "Epoch 625/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1807 - val_loss: 0.2705\n",
            "Epoch 626/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1805 - val_loss: 0.2705\n",
            "Epoch 627/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1806 - val_loss: 0.2707\n",
            "Epoch 628/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1805 - val_loss: 0.2706\n",
            "Epoch 629/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1804 - val_loss: 0.2704\n",
            "Epoch 630/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1804 - val_loss: 0.2705\n",
            "Epoch 631/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1802 - val_loss: 0.2703\n",
            "Epoch 632/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1802 - val_loss: 0.2710\n",
            "Epoch 633/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1803 - val_loss: 0.2706\n",
            "Epoch 634/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1801 - val_loss: 0.2705\n",
            "Epoch 635/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1800 - val_loss: 0.2707\n",
            "Epoch 636/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1799 - val_loss: 0.2705\n",
            "Epoch 637/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1799 - val_loss: 0.2707\n",
            "Epoch 638/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1798 - val_loss: 0.2706\n",
            "Epoch 639/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1798 - val_loss: 0.2707\n",
            "Epoch 640/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1797 - val_loss: 0.2711\n",
            "Epoch 641/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1796 - val_loss: 0.2705\n",
            "Epoch 642/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1797 - val_loss: 0.2706\n",
            "Epoch 643/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1794 - val_loss: 0.2706\n",
            "Epoch 644/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1794 - val_loss: 0.2706\n",
            "Epoch 645/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1793 - val_loss: 0.2706\n",
            "Epoch 646/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1792 - val_loss: 0.2710\n",
            "Epoch 647/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1793 - val_loss: 0.2706\n",
            "Epoch 648/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1791 - val_loss: 0.2707\n",
            "Epoch 649/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1791 - val_loss: 0.2707\n",
            "Epoch 650/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1792 - val_loss: 0.2710\n",
            "Epoch 651/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1790 - val_loss: 0.2709\n",
            "Epoch 652/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1789 - val_loss: 0.2715\n",
            "Epoch 653/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1789 - val_loss: 0.2706\n",
            "Epoch 654/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1788 - val_loss: 0.2715\n",
            "Epoch 655/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1787 - val_loss: 0.2707\n",
            "Epoch 656/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1786 - val_loss: 0.2709\n",
            "Epoch 657/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1786 - val_loss: 0.2706\n",
            "Epoch 658/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1787 - val_loss: 0.2710\n",
            "Epoch 659/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1786 - val_loss: 0.2713\n",
            "Epoch 660/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1784 - val_loss: 0.2708\n",
            "Epoch 661/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1783 - val_loss: 0.2707\n",
            "Epoch 662/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1783 - val_loss: 0.2710\n",
            "Epoch 663/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1783 - val_loss: 0.2709\n",
            "Epoch 664/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1783 - val_loss: 0.2709\n",
            "Epoch 665/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1783 - val_loss: 0.2712\n",
            "Epoch 666/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1782 - val_loss: 0.2714\n",
            "Epoch 667/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1781 - val_loss: 0.2710\n",
            "Epoch 668/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1780 - val_loss: 0.2710\n",
            "Epoch 669/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1780 - val_loss: 0.2709\n",
            "Epoch 670/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1778 - val_loss: 0.2714\n",
            "Epoch 671/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1778 - val_loss: 0.2708\n",
            "Epoch 672/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1777 - val_loss: 0.2709\n",
            "Epoch 673/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1777 - val_loss: 0.2709\n",
            "Epoch 674/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1776 - val_loss: 0.2711\n",
            "Epoch 675/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1775 - val_loss: 0.2709\n",
            "Epoch 676/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1775 - val_loss: 0.2709\n",
            "Epoch 677/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1776 - val_loss: 0.2715\n",
            "Epoch 678/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1775 - val_loss: 0.2712\n",
            "Epoch 679/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1773 - val_loss: 0.2709\n",
            "Epoch 680/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1773 - val_loss: 0.2714\n",
            "Epoch 681/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1772 - val_loss: 0.2712\n",
            "Epoch 682/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1773 - val_loss: 0.2716\n",
            "Epoch 683/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1772 - val_loss: 0.2711\n",
            "Epoch 684/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1772 - val_loss: 0.2716\n",
            "Epoch 685/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1769 - val_loss: 0.2725\n",
            "Epoch 686/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1771 - val_loss: 0.2710\n",
            "Epoch 687/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1768 - val_loss: 0.2711\n",
            "Epoch 688/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1769 - val_loss: 0.2722\n",
            "Epoch 689/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1768 - val_loss: 0.2710\n",
            "Epoch 690/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1768 - val_loss: 0.2711\n",
            "Epoch 691/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1767 - val_loss: 0.2711\n",
            "Epoch 692/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1765 - val_loss: 0.2715\n",
            "Epoch 693/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1766 - val_loss: 0.2711\n",
            "Epoch 694/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1765 - val_loss: 0.2713\n",
            "Epoch 695/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1765 - val_loss: 0.2714\n",
            "Epoch 696/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1765 - val_loss: 0.2714\n",
            "Epoch 697/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1764 - val_loss: 0.2712\n",
            "Epoch 698/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1762 - val_loss: 0.2713\n",
            "Epoch 699/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1763 - val_loss: 0.2715\n",
            "Epoch 700/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1761 - val_loss: 0.2715\n",
            "Epoch 701/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1761 - val_loss: 0.2714\n",
            "Epoch 702/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1761 - val_loss: 0.2713\n",
            "Epoch 703/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1761 - val_loss: 0.2714\n",
            "Epoch 704/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1759 - val_loss: 0.2716\n",
            "Epoch 705/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1760 - val_loss: 0.2715\n",
            "Epoch 706/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1758 - val_loss: 0.2714\n",
            "Epoch 707/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1758 - val_loss: 0.2715\n",
            "Epoch 708/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1756 - val_loss: 0.2713\n",
            "Epoch 709/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1757 - val_loss: 0.2715\n",
            "Epoch 710/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1756 - val_loss: 0.2715\n",
            "Epoch 711/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1755 - val_loss: 0.2715\n",
            "Epoch 712/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1755 - val_loss: 0.2714\n",
            "Epoch 713/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1755 - val_loss: 0.2715\n",
            "Epoch 714/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1754 - val_loss: 0.2717\n",
            "Epoch 715/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1753 - val_loss: 0.2716\n",
            "Epoch 716/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1754 - val_loss: 0.2716\n",
            "Epoch 717/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1752 - val_loss: 0.2722\n",
            "Epoch 718/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1754 - val_loss: 0.2718\n",
            "Epoch 719/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1751 - val_loss: 0.2716\n",
            "Epoch 720/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1750 - val_loss: 0.2716\n",
            "Epoch 721/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1750 - val_loss: 0.2719\n",
            "Epoch 722/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1749 - val_loss: 0.2716\n",
            "Epoch 723/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1750 - val_loss: 0.2721\n",
            "Epoch 724/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1750 - val_loss: 0.2716\n",
            "Epoch 725/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1748 - val_loss: 0.2718\n",
            "Epoch 726/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1748 - val_loss: 0.2715\n",
            "Epoch 727/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1746 - val_loss: 0.2715\n",
            "Epoch 728/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1747 - val_loss: 0.2715\n",
            "Epoch 729/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1746 - val_loss: 0.2716\n",
            "Epoch 730/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1745 - val_loss: 0.2717\n",
            "Epoch 731/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1745 - val_loss: 0.2717\n",
            "Epoch 732/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1745 - val_loss: 0.2718\n",
            "Epoch 733/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1744 - val_loss: 0.2717\n",
            "Epoch 734/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1743 - val_loss: 0.2727\n",
            "Epoch 735/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1744 - val_loss: 0.2720\n",
            "Epoch 736/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1744 - val_loss: 0.2718\n",
            "Epoch 737/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1740 - val_loss: 0.2717\n",
            "Epoch 738/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1742 - val_loss: 0.2718\n",
            "Epoch 739/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1742 - val_loss: 0.2718\n",
            "Epoch 740/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1741 - val_loss: 0.2719\n",
            "Epoch 741/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1740 - val_loss: 0.2716\n",
            "Epoch 742/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1740 - val_loss: 0.2719\n",
            "Epoch 743/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1739 - val_loss: 0.2718\n",
            "Epoch 744/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1739 - val_loss: 0.2719\n",
            "Epoch 745/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1740 - val_loss: 0.2718\n",
            "Epoch 746/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1738 - val_loss: 0.2721\n",
            "Epoch 747/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1738 - val_loss: 0.2719\n",
            "Epoch 748/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1736 - val_loss: 0.2719\n",
            "Epoch 749/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1736 - val_loss: 0.2721\n",
            "Epoch 750/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1736 - val_loss: 0.2719\n",
            "Epoch 751/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1736 - val_loss: 0.2723\n",
            "Epoch 752/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1736 - val_loss: 0.2731\n",
            "Epoch 753/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1735 - val_loss: 0.2719\n",
            "Epoch 754/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1734 - val_loss: 0.2727\n",
            "Epoch 755/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1735 - val_loss: 0.2719\n",
            "Epoch 756/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1735 - val_loss: 0.2724\n",
            "Epoch 757/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1732 - val_loss: 0.2725\n",
            "Epoch 758/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1733 - val_loss: 0.2725\n",
            "Epoch 759/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1732 - val_loss: 0.2721\n",
            "Epoch 760/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1731 - val_loss: 0.2723\n",
            "Epoch 761/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1732 - val_loss: 0.2722\n",
            "Epoch 762/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1731 - val_loss: 0.2723\n",
            "Epoch 763/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1730 - val_loss: 0.2722\n",
            "Epoch 764/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1730 - val_loss: 0.2730\n",
            "Epoch 765/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1729 - val_loss: 0.2721\n",
            "Epoch 766/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1727 - val_loss: 0.2721\n",
            "Epoch 767/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1727 - val_loss: 0.2721\n",
            "Epoch 768/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1727 - val_loss: 0.2720\n",
            "Epoch 769/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1727 - val_loss: 0.2723\n",
            "Epoch 770/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1727 - val_loss: 0.2724\n",
            "Epoch 771/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1726 - val_loss: 0.2724\n",
            "Epoch 772/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1726 - val_loss: 0.2727\n",
            "Epoch 773/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1725 - val_loss: 0.2721\n",
            "Epoch 774/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1724 - val_loss: 0.2727\n",
            "Epoch 775/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1724 - val_loss: 0.2722\n",
            "Epoch 776/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1724 - val_loss: 0.2722\n",
            "Epoch 777/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1723 - val_loss: 0.2722\n",
            "Epoch 778/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1722 - val_loss: 0.2725\n",
            "Epoch 779/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1721 - val_loss: 0.2730\n",
            "Epoch 780/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1724 - val_loss: 0.2725\n",
            "Epoch 781/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1722 - val_loss: 0.2724\n",
            "Epoch 782/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1722 - val_loss: 0.2723\n",
            "Epoch 783/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1720 - val_loss: 0.2727\n",
            "Epoch 784/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1720 - val_loss: 0.2723\n",
            "Epoch 785/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1720 - val_loss: 0.2725\n",
            "Epoch 786/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1720 - val_loss: 0.2728\n",
            "Epoch 787/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1720 - val_loss: 0.2725\n",
            "Epoch 788/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1718 - val_loss: 0.2731\n",
            "Epoch 789/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1718 - val_loss: 0.2725\n",
            "Epoch 790/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1717 - val_loss: 0.2726\n",
            "Epoch 791/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1717 - val_loss: 0.2727\n",
            "Epoch 792/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1716 - val_loss: 0.2724\n",
            "Epoch 793/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1715 - val_loss: 0.2727\n",
            "Epoch 794/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1717 - val_loss: 0.2728\n",
            "Epoch 795/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1716 - val_loss: 0.2726\n",
            "Epoch 796/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1715 - val_loss: 0.2726\n",
            "Epoch 797/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1715 - val_loss: 0.2725\n",
            "Epoch 798/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1714 - val_loss: 0.2725\n",
            "Epoch 799/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1714 - val_loss: 0.2727\n",
            "Epoch 800/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1713 - val_loss: 0.2739\n",
            "Epoch 801/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1713 - val_loss: 0.2726\n",
            "Epoch 802/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1711 - val_loss: 0.2727\n",
            "Epoch 803/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1712 - val_loss: 0.2728\n",
            "Epoch 804/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1711 - val_loss: 0.2729\n",
            "Epoch 805/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1711 - val_loss: 0.2729\n",
            "Epoch 806/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1710 - val_loss: 0.2728\n",
            "Epoch 807/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1711 - val_loss: 0.2729\n",
            "Epoch 808/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1711 - val_loss: 0.2727\n",
            "Epoch 809/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1710 - val_loss: 0.2726\n",
            "Epoch 810/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1710 - val_loss: 0.2727\n",
            "Epoch 811/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1707 - val_loss: 0.2727\n",
            "Epoch 812/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1707 - val_loss: 0.2730\n",
            "Epoch 813/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1708 - val_loss: 0.2727\n",
            "Epoch 814/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1708 - val_loss: 0.2727\n",
            "Epoch 815/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1706 - val_loss: 0.2727\n",
            "Epoch 816/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1706 - val_loss: 0.2728\n",
            "Epoch 817/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1705 - val_loss: 0.2728\n",
            "Epoch 818/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1705 - val_loss: 0.2729\n",
            "Epoch 819/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1705 - val_loss: 0.2730\n",
            "Epoch 820/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1704 - val_loss: 0.2729\n",
            "Epoch 821/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1703 - val_loss: 0.2737\n",
            "Epoch 822/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1704 - val_loss: 0.2730\n",
            "Epoch 823/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1703 - val_loss: 0.2729\n",
            "Epoch 824/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1704 - val_loss: 0.2732\n",
            "Epoch 825/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1703 - val_loss: 0.2730\n",
            "Epoch 826/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1702 - val_loss: 0.2731\n",
            "Epoch 827/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1701 - val_loss: 0.2731\n",
            "Epoch 828/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1702 - val_loss: 0.2730\n",
            "Epoch 829/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1701 - val_loss: 0.2730\n",
            "Epoch 830/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1701 - val_loss: 0.2730\n",
            "Epoch 831/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1700 - val_loss: 0.2730\n",
            "Epoch 832/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1700 - val_loss: 0.2732\n",
            "Epoch 833/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1700 - val_loss: 0.2730\n",
            "Epoch 834/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1699 - val_loss: 0.2734\n",
            "Epoch 835/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1699 - val_loss: 0.2732\n",
            "Epoch 836/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1699 - val_loss: 0.2733\n",
            "Epoch 837/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1699 - val_loss: 0.2730\n",
            "Epoch 838/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1698 - val_loss: 0.2734\n",
            "Epoch 839/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1696 - val_loss: 0.2734\n",
            "Epoch 840/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1696 - val_loss: 0.2730\n",
            "Epoch 841/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1695 - val_loss: 0.2734\n",
            "Epoch 842/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1695 - val_loss: 0.2734\n",
            "Epoch 843/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1696 - val_loss: 0.2731\n",
            "Epoch 844/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1695 - val_loss: 0.2731\n",
            "Epoch 845/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1694 - val_loss: 0.2735\n",
            "Epoch 846/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1694 - val_loss: 0.2734\n",
            "Epoch 847/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1694 - val_loss: 0.2733\n",
            "Epoch 848/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1693 - val_loss: 0.2733\n",
            "Epoch 849/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1692 - val_loss: 0.2733\n",
            "Epoch 850/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1693 - val_loss: 0.2734\n",
            "Epoch 851/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1692 - val_loss: 0.2733\n",
            "Epoch 852/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1692 - val_loss: 0.2732\n",
            "Epoch 853/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1690 - val_loss: 0.2734\n",
            "Epoch 854/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1691 - val_loss: 0.2735\n",
            "Epoch 855/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1691 - val_loss: 0.2736\n",
            "Epoch 856/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1690 - val_loss: 0.2737\n",
            "Epoch 857/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1691 - val_loss: 0.2735\n",
            "Epoch 858/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1689 - val_loss: 0.2737\n",
            "Epoch 859/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1688 - val_loss: 0.2733\n",
            "Epoch 860/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1688 - val_loss: 0.2735\n",
            "Epoch 861/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1688 - val_loss: 0.2737\n",
            "Epoch 862/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1689 - val_loss: 0.2735\n",
            "Epoch 863/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1688 - val_loss: 0.2735\n",
            "Epoch 864/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1687 - val_loss: 0.2736\n",
            "Epoch 865/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1687 - val_loss: 0.2735\n",
            "Epoch 866/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1686 - val_loss: 0.2740\n",
            "Epoch 867/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1686 - val_loss: 0.2738\n",
            "Epoch 868/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1686 - val_loss: 0.2740\n",
            "Epoch 869/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1685 - val_loss: 0.2735\n",
            "Epoch 870/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1684 - val_loss: 0.2738\n",
            "Epoch 871/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1684 - val_loss: 0.2736\n",
            "Epoch 872/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1685 - val_loss: 0.2737\n",
            "Epoch 873/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1684 - val_loss: 0.2736\n",
            "Epoch 874/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1683 - val_loss: 0.2737\n",
            "Epoch 875/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1683 - val_loss: 0.2738\n",
            "Epoch 876/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1684 - val_loss: 0.2743\n",
            "Epoch 877/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1683 - val_loss: 0.2736\n",
            "Epoch 878/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1682 - val_loss: 0.2737\n",
            "Epoch 879/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1681 - val_loss: 0.2738\n",
            "Epoch 880/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1682 - val_loss: 0.2739\n",
            "Epoch 881/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1681 - val_loss: 0.2737\n",
            "Epoch 882/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1680 - val_loss: 0.2743\n",
            "Epoch 883/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1680 - val_loss: 0.2745\n",
            "Epoch 884/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1680 - val_loss: 0.2739\n",
            "Epoch 885/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1679 - val_loss: 0.2738\n",
            "Epoch 886/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1680 - val_loss: 0.2741\n",
            "Epoch 887/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1678 - val_loss: 0.2739\n",
            "Epoch 888/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1680 - val_loss: 0.2741\n",
            "Epoch 889/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1677 - val_loss: 0.2738\n",
            "Epoch 890/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1677 - val_loss: 0.2741\n",
            "Epoch 891/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1678 - val_loss: 0.2743\n",
            "Epoch 892/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1678 - val_loss: 0.2740\n",
            "Epoch 893/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1676 - val_loss: 0.2739\n",
            "Epoch 894/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1676 - val_loss: 0.2741\n",
            "Epoch 895/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1676 - val_loss: 0.2743\n",
            "Epoch 896/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1676 - val_loss: 0.2738\n",
            "Epoch 897/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1675 - val_loss: 0.2739\n",
            "Epoch 898/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1675 - val_loss: 0.2739\n",
            "Epoch 899/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1675 - val_loss: 0.2748\n",
            "Epoch 900/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1674 - val_loss: 0.2739\n",
            "Epoch 901/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1674 - val_loss: 0.2743\n",
            "Epoch 902/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1674 - val_loss: 0.2747\n",
            "Epoch 903/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1673 - val_loss: 0.2742\n",
            "Epoch 904/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1675 - val_loss: 0.2744\n",
            "Epoch 905/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1674 - val_loss: 0.2743\n",
            "Epoch 906/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1671 - val_loss: 0.2741\n",
            "Epoch 907/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1672 - val_loss: 0.2743\n",
            "Epoch 908/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1671 - val_loss: 0.2743\n",
            "Epoch 909/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1671 - val_loss: 0.2742\n",
            "Epoch 910/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1670 - val_loss: 0.2741\n",
            "Epoch 911/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1670 - val_loss: 0.2748\n",
            "Epoch 912/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1671 - val_loss: 0.2741\n",
            "Epoch 913/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1669 - val_loss: 0.2744\n",
            "Epoch 914/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1670 - val_loss: 0.2742\n",
            "Epoch 915/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1668 - val_loss: 0.2743\n",
            "Epoch 916/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1668 - val_loss: 0.2745\n",
            "Epoch 917/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1668 - val_loss: 0.2743\n",
            "Epoch 918/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1669 - val_loss: 0.2746\n",
            "Epoch 919/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1668 - val_loss: 0.2742\n",
            "Epoch 920/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1667 - val_loss: 0.2743\n",
            "Epoch 921/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1666 - val_loss: 0.2743\n",
            "Epoch 922/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1666 - val_loss: 0.2744\n",
            "Epoch 923/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1668 - val_loss: 0.2742\n",
            "Epoch 924/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1665 - val_loss: 0.2742\n",
            "Epoch 925/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1666 - val_loss: 0.2743\n",
            "Epoch 926/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1665 - val_loss: 0.2747\n",
            "Epoch 927/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1665 - val_loss: 0.2744\n",
            "Epoch 928/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1665 - val_loss: 0.2744\n",
            "Epoch 929/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1663 - val_loss: 0.2743\n",
            "Epoch 930/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1664 - val_loss: 0.2744\n",
            "Epoch 931/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1663 - val_loss: 0.2746\n",
            "Epoch 932/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1663 - val_loss: 0.2748\n",
            "Epoch 933/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1663 - val_loss: 0.2747\n",
            "Epoch 934/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1664 - val_loss: 0.2747\n",
            "Epoch 935/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1663 - val_loss: 0.2749\n",
            "Epoch 936/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1662 - val_loss: 0.2747\n",
            "Epoch 937/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1662 - val_loss: 0.2745\n",
            "Epoch 938/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1661 - val_loss: 0.2746\n",
            "Epoch 939/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1661 - val_loss: 0.2749\n",
            "Epoch 940/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1661 - val_loss: 0.2746\n",
            "Epoch 941/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1659 - val_loss: 0.2748\n",
            "Epoch 942/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1660 - val_loss: 0.2748\n",
            "Epoch 943/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1660 - val_loss: 0.2751\n",
            "Epoch 944/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1659 - val_loss: 0.2747\n",
            "Epoch 945/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1659 - val_loss: 0.2748\n",
            "Epoch 946/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1658 - val_loss: 0.2749\n",
            "Epoch 947/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1659 - val_loss: 0.2747\n",
            "Epoch 948/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1658 - val_loss: 0.2747\n",
            "Epoch 949/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1657 - val_loss: 0.2747\n",
            "Epoch 950/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1658 - val_loss: 0.2753\n",
            "Epoch 951/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1657 - val_loss: 0.2748\n",
            "Epoch 952/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1658 - val_loss: 0.2747\n",
            "Epoch 953/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1656 - val_loss: 0.2751\n",
            "Epoch 954/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1656 - val_loss: 0.2747\n",
            "Epoch 955/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1655 - val_loss: 0.2748\n",
            "Epoch 956/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1656 - val_loss: 0.2748\n",
            "Epoch 957/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1656 - val_loss: 0.2747\n",
            "Epoch 958/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1654 - val_loss: 0.2750\n",
            "Epoch 959/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1654 - val_loss: 0.2750\n",
            "Epoch 960/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1655 - val_loss: 0.2750\n",
            "Epoch 961/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1653 - val_loss: 0.2750\n",
            "Epoch 962/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1654 - val_loss: 0.2749\n",
            "Epoch 963/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1653 - val_loss: 0.2752\n",
            "Epoch 964/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1654 - val_loss: 0.2751\n",
            "Epoch 965/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1653 - val_loss: 0.2751\n",
            "Epoch 966/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1651 - val_loss: 0.2749\n",
            "Epoch 967/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1653 - val_loss: 0.2754\n",
            "Epoch 968/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1652 - val_loss: 0.2751\n",
            "Epoch 969/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1652 - val_loss: 0.2750\n",
            "Epoch 970/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1650 - val_loss: 0.2750\n",
            "Epoch 971/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1650 - val_loss: 0.2751\n",
            "Epoch 972/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1650 - val_loss: 0.2750\n",
            "Epoch 973/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1651 - val_loss: 0.2751\n",
            "Epoch 974/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1650 - val_loss: 0.2756\n",
            "Epoch 975/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1650 - val_loss: 0.2752\n",
            "Epoch 976/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1649 - val_loss: 0.2752\n",
            "Epoch 977/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1650 - val_loss: 0.2750\n",
            "Epoch 978/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1648 - val_loss: 0.2753\n",
            "Epoch 979/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1649 - val_loss: 0.2756\n",
            "Epoch 980/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1649 - val_loss: 0.2752\n",
            "Epoch 981/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1649 - val_loss: 0.2751\n",
            "Epoch 982/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1647 - val_loss: 0.2756\n",
            "Epoch 983/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1648 - val_loss: 0.2756\n",
            "Epoch 984/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1647 - val_loss: 0.2760\n",
            "Epoch 985/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1647 - val_loss: 0.2751\n",
            "Epoch 986/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1647 - val_loss: 0.2757\n",
            "Epoch 987/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1646 - val_loss: 0.2753\n",
            "Epoch 988/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1647 - val_loss: 0.2752\n",
            "Epoch 989/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1645 - val_loss: 0.2759\n",
            "Epoch 990/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1645 - val_loss: 0.2752\n",
            "Epoch 991/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1644 - val_loss: 0.2752\n",
            "Epoch 992/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1645 - val_loss: 0.2752\n",
            "Epoch 993/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1645 - val_loss: 0.2755\n",
            "Epoch 994/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1644 - val_loss: 0.2754\n",
            "Epoch 995/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1644 - val_loss: 0.2752\n",
            "Epoch 996/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1643 - val_loss: 0.2753\n",
            "Epoch 997/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1644 - val_loss: 0.2755\n",
            "Epoch 998/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1642 - val_loss: 0.2756\n",
            "Epoch 999/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1642 - val_loss: 0.2757\n",
            "Epoch 1000/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1642 - val_loss: 0.2757\n",
            "3/3 [==============================] - 0s 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0.034874942, 0.2212694204456747, 3.6797975968037435), (0.032329243, 0.23580026189427816, 3.7484075950663445), (0.030779244, 0.2276339840277261, 3.6787310024161917)]\n",
            "Epoch 1/1000\n",
            "34/34 [==============================] - 2s 33ms/step - loss: 0.6208 - val_loss: 0.5600\n",
            "Epoch 2/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.4519 - val_loss: 0.5276\n",
            "Epoch 3/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.4097 - val_loss: 0.4929\n",
            "Epoch 4/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3876 - val_loss: 0.4643\n",
            "Epoch 5/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3731 - val_loss: 0.4409\n",
            "Epoch 6/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3618 - val_loss: 0.4212\n",
            "Epoch 7/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3523 - val_loss: 0.4049\n",
            "Epoch 8/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3439 - val_loss: 0.3917\n",
            "Epoch 9/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3366 - val_loss: 0.3808\n",
            "Epoch 10/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3298 - val_loss: 0.3718\n",
            "Epoch 11/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3237 - val_loss: 0.3641\n",
            "Epoch 12/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3184 - val_loss: 0.3576\n",
            "Epoch 13/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3135 - val_loss: 0.3520\n",
            "Epoch 14/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3092 - val_loss: 0.3471\n",
            "Epoch 15/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3052 - val_loss: 0.3428\n",
            "Epoch 16/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3017 - val_loss: 0.3389\n",
            "Epoch 17/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2984 - val_loss: 0.3354\n",
            "Epoch 18/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2955 - val_loss: 0.3321\n",
            "Epoch 19/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2927 - val_loss: 0.3290\n",
            "Epoch 20/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2901 - val_loss: 0.3261\n",
            "Epoch 21/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2877 - val_loss: 0.3235\n",
            "Epoch 22/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2855 - val_loss: 0.3210\n",
            "Epoch 23/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2834 - val_loss: 0.3187\n",
            "Epoch 24/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2815 - val_loss: 0.3165\n",
            "Epoch 25/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2797 - val_loss: 0.3146\n",
            "Epoch 26/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2779 - val_loss: 0.3127\n",
            "Epoch 27/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2763 - val_loss: 0.3109\n",
            "Epoch 28/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2748 - val_loss: 0.3092\n",
            "Epoch 29/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2733 - val_loss: 0.3077\n",
            "Epoch 30/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2721 - val_loss: 0.3062\n",
            "Epoch 31/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2709 - val_loss: 0.3049\n",
            "Epoch 32/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2697 - val_loss: 0.3036\n",
            "Epoch 33/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2686 - val_loss: 0.3024\n",
            "Epoch 34/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2676 - val_loss: 0.3012\n",
            "Epoch 35/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2666 - val_loss: 0.3002\n",
            "Epoch 36/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2657 - val_loss: 0.2992\n",
            "Epoch 37/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2649 - val_loss: 0.2982\n",
            "Epoch 38/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2640 - val_loss: 0.2974\n",
            "Epoch 39/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2634 - val_loss: 0.2965\n",
            "Epoch 40/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2627 - val_loss: 0.2958\n",
            "Epoch 41/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2619 - val_loss: 0.2950\n",
            "Epoch 42/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2613 - val_loss: 0.2943\n",
            "Epoch 43/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2607 - val_loss: 0.2937\n",
            "Epoch 44/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2603 - val_loss: 0.2931\n",
            "Epoch 45/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2597 - val_loss: 0.2926\n",
            "Epoch 46/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2592 - val_loss: 0.2921\n",
            "Epoch 47/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2588 - val_loss: 0.2916\n",
            "Epoch 48/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2584 - val_loss: 0.2911\n",
            "Epoch 49/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2581 - val_loss: 0.2907\n",
            "Epoch 50/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2576 - val_loss: 0.2903\n",
            "Epoch 51/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2573 - val_loss: 0.2899\n",
            "Epoch 52/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2570 - val_loss: 0.2896\n",
            "Epoch 53/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2568 - val_loss: 0.2893\n",
            "Epoch 54/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2565 - val_loss: 0.2890\n",
            "Epoch 55/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2562 - val_loss: 0.2887\n",
            "Epoch 56/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2560 - val_loss: 0.2884\n",
            "Epoch 57/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2558 - val_loss: 0.2882\n",
            "Epoch 58/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2555 - val_loss: 0.2880\n",
            "Epoch 59/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2554 - val_loss: 0.2878\n",
            "Epoch 60/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2551 - val_loss: 0.2875\n",
            "Epoch 61/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2549 - val_loss: 0.2874\n",
            "Epoch 62/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2547 - val_loss: 0.2872\n",
            "Epoch 63/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2546 - val_loss: 0.2870\n",
            "Epoch 64/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2545 - val_loss: 0.2868\n",
            "Epoch 65/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2543 - val_loss: 0.2867\n",
            "Epoch 66/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2543 - val_loss: 0.2865\n",
            "Epoch 67/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2540 - val_loss: 0.2864\n",
            "Epoch 68/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2539 - val_loss: 0.2862\n",
            "Epoch 69/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2538 - val_loss: 0.2861\n",
            "Epoch 70/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2537 - val_loss: 0.2860\n",
            "Epoch 71/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2535 - val_loss: 0.2859\n",
            "Epoch 72/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2534 - val_loss: 0.2858\n",
            "Epoch 73/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2533 - val_loss: 0.2857\n",
            "Epoch 74/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2533 - val_loss: 0.2855\n",
            "Epoch 75/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2532 - val_loss: 0.2854\n",
            "Epoch 76/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2531 - val_loss: 0.2854\n",
            "Epoch 77/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2529 - val_loss: 0.2853\n",
            "Epoch 78/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2528 - val_loss: 0.2852\n",
            "Epoch 79/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2527 - val_loss: 0.2851\n",
            "Epoch 80/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2527 - val_loss: 0.2850\n",
            "Epoch 81/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2525 - val_loss: 0.2849\n",
            "Epoch 82/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2524 - val_loss: 0.2848\n",
            "Epoch 83/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2525 - val_loss: 0.2848\n",
            "Epoch 84/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2523 - val_loss: 0.2847\n",
            "Epoch 85/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2522 - val_loss: 0.2846\n",
            "Epoch 86/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2521 - val_loss: 0.2845\n",
            "Epoch 87/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2521 - val_loss: 0.2845\n",
            "Epoch 88/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2520 - val_loss: 0.2844\n",
            "Epoch 89/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2519 - val_loss: 0.2843\n",
            "Epoch 90/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2518 - val_loss: 0.2842\n",
            "Epoch 91/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2518 - val_loss: 0.2842\n",
            "Epoch 92/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2516 - val_loss: 0.2841\n",
            "Epoch 93/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2516 - val_loss: 0.2841\n",
            "Epoch 94/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2515 - val_loss: 0.2840\n",
            "Epoch 95/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2514 - val_loss: 0.2840\n",
            "Epoch 96/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2513 - val_loss: 0.2839\n",
            "Epoch 97/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2512 - val_loss: 0.2838\n",
            "Epoch 98/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2511 - val_loss: 0.2838\n",
            "Epoch 99/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2510 - val_loss: 0.2837\n",
            "Epoch 100/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2510 - val_loss: 0.2837\n",
            "Epoch 101/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2509 - val_loss: 0.2836\n",
            "Epoch 102/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2508 - val_loss: 0.2836\n",
            "Epoch 103/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2507 - val_loss: 0.2835\n",
            "Epoch 104/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2507 - val_loss: 0.2835\n",
            "Epoch 105/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2506 - val_loss: 0.2834\n",
            "Epoch 106/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2505 - val_loss: 0.2833\n",
            "Epoch 107/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2504 - val_loss: 0.2833\n",
            "Epoch 108/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2504 - val_loss: 0.2832\n",
            "Epoch 109/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2502 - val_loss: 0.2832\n",
            "Epoch 110/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2501 - val_loss: 0.2831\n",
            "Epoch 111/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2501 - val_loss: 0.2831\n",
            "Epoch 112/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2500 - val_loss: 0.2830\n",
            "Epoch 113/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2499 - val_loss: 0.2831\n",
            "Epoch 114/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2497 - val_loss: 0.2829\n",
            "Epoch 115/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2496 - val_loss: 0.2829\n",
            "Epoch 116/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2496 - val_loss: 0.2828\n",
            "Epoch 117/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2496 - val_loss: 0.2828\n",
            "Epoch 118/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2495 - val_loss: 0.2828\n",
            "Epoch 119/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2494 - val_loss: 0.2827\n",
            "Epoch 120/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2492 - val_loss: 0.2826\n",
            "Epoch 121/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2491 - val_loss: 0.2826\n",
            "Epoch 122/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2490 - val_loss: 0.2825\n",
            "Epoch 123/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2490 - val_loss: 0.2825\n",
            "Epoch 124/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2488 - val_loss: 0.2824\n",
            "Epoch 125/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2488 - val_loss: 0.2824\n",
            "Epoch 126/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2487 - val_loss: 0.2823\n",
            "Epoch 127/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2486 - val_loss: 0.2823\n",
            "Epoch 128/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2484 - val_loss: 0.2823\n",
            "Epoch 129/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2483 - val_loss: 0.2822\n",
            "Epoch 130/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2482 - val_loss: 0.2821\n",
            "Epoch 131/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2481 - val_loss: 0.2821\n",
            "Epoch 132/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2480 - val_loss: 0.2820\n",
            "Epoch 133/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2479 - val_loss: 0.2820\n",
            "Epoch 134/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2477 - val_loss: 0.2819\n",
            "Epoch 135/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2476 - val_loss: 0.2819\n",
            "Epoch 136/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2476 - val_loss: 0.2818\n",
            "Epoch 137/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2474 - val_loss: 0.2818\n",
            "Epoch 138/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2473 - val_loss: 0.2818\n",
            "Epoch 139/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2472 - val_loss: 0.2817\n",
            "Epoch 140/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2472 - val_loss: 0.2816\n",
            "Epoch 141/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2470 - val_loss: 0.2816\n",
            "Epoch 142/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2469 - val_loss: 0.2816\n",
            "Epoch 143/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2467 - val_loss: 0.2815\n",
            "Epoch 144/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2467 - val_loss: 0.2815\n",
            "Epoch 145/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2466 - val_loss: 0.2814\n",
            "Epoch 146/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2464 - val_loss: 0.2813\n",
            "Epoch 147/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2464 - val_loss: 0.2813\n",
            "Epoch 148/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2461 - val_loss: 0.2813\n",
            "Epoch 149/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2461 - val_loss: 0.2812\n",
            "Epoch 150/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2459 - val_loss: 0.2811\n",
            "Epoch 151/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2457 - val_loss: 0.2811\n",
            "Epoch 152/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2457 - val_loss: 0.2811\n",
            "Epoch 153/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2455 - val_loss: 0.2810\n",
            "Epoch 154/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2454 - val_loss: 0.2809\n",
            "Epoch 155/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2452 - val_loss: 0.2809\n",
            "Epoch 156/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2452 - val_loss: 0.2808\n",
            "Epoch 157/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2450 - val_loss: 0.2808\n",
            "Epoch 158/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2448 - val_loss: 0.2808\n",
            "Epoch 159/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2447 - val_loss: 0.2807\n",
            "Epoch 160/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2446 - val_loss: 0.2807\n",
            "Epoch 161/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2445 - val_loss: 0.2806\n",
            "Epoch 162/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2444 - val_loss: 0.2806\n",
            "Epoch 163/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2442 - val_loss: 0.2805\n",
            "Epoch 164/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2441 - val_loss: 0.2804\n",
            "Epoch 165/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2440 - val_loss: 0.2804\n",
            "Epoch 166/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2438 - val_loss: 0.2804\n",
            "Epoch 167/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2436 - val_loss: 0.2803\n",
            "Epoch 168/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2434 - val_loss: 0.2803\n",
            "Epoch 169/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2434 - val_loss: 0.2802\n",
            "Epoch 170/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2432 - val_loss: 0.2801\n",
            "Epoch 171/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2431 - val_loss: 0.2801\n",
            "Epoch 172/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2429 - val_loss: 0.2801\n",
            "Epoch 173/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2427 - val_loss: 0.2800\n",
            "Epoch 174/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2427 - val_loss: 0.2799\n",
            "Epoch 175/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2425 - val_loss: 0.2799\n",
            "Epoch 176/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2423 - val_loss: 0.2798\n",
            "Epoch 177/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2422 - val_loss: 0.2798\n",
            "Epoch 178/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2420 - val_loss: 0.2797\n",
            "Epoch 179/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2419 - val_loss: 0.2797\n",
            "Epoch 180/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2418 - val_loss: 0.2797\n",
            "Epoch 181/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2415 - val_loss: 0.2796\n",
            "Epoch 182/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2415 - val_loss: 0.2795\n",
            "Epoch 183/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2413 - val_loss: 0.2795\n",
            "Epoch 184/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2411 - val_loss: 0.2794\n",
            "Epoch 185/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2410 - val_loss: 0.2794\n",
            "Epoch 186/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2408 - val_loss: 0.2793\n",
            "Epoch 187/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2407 - val_loss: 0.2793\n",
            "Epoch 188/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2406 - val_loss: 0.2792\n",
            "Epoch 189/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2403 - val_loss: 0.2792\n",
            "Epoch 190/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2402 - val_loss: 0.2791\n",
            "Epoch 191/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2402 - val_loss: 0.2791\n",
            "Epoch 192/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2400 - val_loss: 0.2791\n",
            "Epoch 193/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2398 - val_loss: 0.2790\n",
            "Epoch 194/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2396 - val_loss: 0.2790\n",
            "Epoch 195/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2394 - val_loss: 0.2789\n",
            "Epoch 196/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2393 - val_loss: 0.2788\n",
            "Epoch 197/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2391 - val_loss: 0.2788\n",
            "Epoch 198/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2389 - val_loss: 0.2787\n",
            "Epoch 199/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2388 - val_loss: 0.2787\n",
            "Epoch 200/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2387 - val_loss: 0.2786\n",
            "Epoch 201/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2386 - val_loss: 0.2786\n",
            "Epoch 202/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2384 - val_loss: 0.2785\n",
            "Epoch 203/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2383 - val_loss: 0.2785\n",
            "Epoch 204/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2381 - val_loss: 0.2784\n",
            "Epoch 205/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2380 - val_loss: 0.2784\n",
            "Epoch 206/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2378 - val_loss: 0.2783\n",
            "Epoch 207/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2376 - val_loss: 0.2783\n",
            "Epoch 208/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2374 - val_loss: 0.2782\n",
            "Epoch 209/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2373 - val_loss: 0.2782\n",
            "Epoch 210/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2371 - val_loss: 0.2782\n",
            "Epoch 211/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2370 - val_loss: 0.2781\n",
            "Epoch 212/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2368 - val_loss: 0.2780\n",
            "Epoch 213/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2366 - val_loss: 0.2780\n",
            "Epoch 214/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2365 - val_loss: 0.2779\n",
            "Epoch 215/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2364 - val_loss: 0.2779\n",
            "Epoch 216/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2361 - val_loss: 0.2778\n",
            "Epoch 217/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2360 - val_loss: 0.2778\n",
            "Epoch 218/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2359 - val_loss: 0.2777\n",
            "Epoch 219/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2356 - val_loss: 0.2777\n",
            "Epoch 220/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2354 - val_loss: 0.2776\n",
            "Epoch 221/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2354 - val_loss: 0.2776\n",
            "Epoch 222/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2354 - val_loss: 0.2775\n",
            "Epoch 223/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2350 - val_loss: 0.2775\n",
            "Epoch 224/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2350 - val_loss: 0.2774\n",
            "Epoch 225/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2347 - val_loss: 0.2774\n",
            "Epoch 226/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2346 - val_loss: 0.2773\n",
            "Epoch 227/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2344 - val_loss: 0.2773\n",
            "Epoch 228/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2343 - val_loss: 0.2773\n",
            "Epoch 229/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2341 - val_loss: 0.2772\n",
            "Epoch 230/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2339 - val_loss: 0.2771\n",
            "Epoch 231/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2338 - val_loss: 0.2771\n",
            "Epoch 232/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2336 - val_loss: 0.2771\n",
            "Epoch 233/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2335 - val_loss: 0.2770\n",
            "Epoch 234/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2333 - val_loss: 0.2769\n",
            "Epoch 235/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2331 - val_loss: 0.2769\n",
            "Epoch 236/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2330 - val_loss: 0.2769\n",
            "Epoch 237/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2329 - val_loss: 0.2768\n",
            "Epoch 238/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2327 - val_loss: 0.2768\n",
            "Epoch 239/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2325 - val_loss: 0.2767\n",
            "Epoch 240/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2324 - val_loss: 0.2767\n",
            "Epoch 241/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2322 - val_loss: 0.2766\n",
            "Epoch 242/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2320 - val_loss: 0.2766\n",
            "Epoch 243/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2319 - val_loss: 0.2765\n",
            "Epoch 244/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2317 - val_loss: 0.2765\n",
            "Epoch 245/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2315 - val_loss: 0.2764\n",
            "Epoch 246/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2314 - val_loss: 0.2764\n",
            "Epoch 247/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2313 - val_loss: 0.2763\n",
            "Epoch 248/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2310 - val_loss: 0.2763\n",
            "Epoch 249/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2309 - val_loss: 0.2763\n",
            "Epoch 250/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2307 - val_loss: 0.2763\n",
            "Epoch 251/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2306 - val_loss: 0.2762\n",
            "Epoch 252/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2304 - val_loss: 0.2761\n",
            "Epoch 253/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2303 - val_loss: 0.2761\n",
            "Epoch 254/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2302 - val_loss: 0.2761\n",
            "Epoch 255/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2300 - val_loss: 0.2760\n",
            "Epoch 256/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2298 - val_loss: 0.2760\n",
            "Epoch 257/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2296 - val_loss: 0.2759\n",
            "Epoch 258/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2295 - val_loss: 0.2759\n",
            "Epoch 259/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2293 - val_loss: 0.2759\n",
            "Epoch 260/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2292 - val_loss: 0.2758\n",
            "Epoch 261/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2290 - val_loss: 0.2757\n",
            "Epoch 262/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2288 - val_loss: 0.2758\n",
            "Epoch 263/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2286 - val_loss: 0.2757\n",
            "Epoch 264/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2286 - val_loss: 0.2756\n",
            "Epoch 265/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2285 - val_loss: 0.2756\n",
            "Epoch 266/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2281 - val_loss: 0.2756\n",
            "Epoch 267/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2281 - val_loss: 0.2755\n",
            "Epoch 268/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2279 - val_loss: 0.2755\n",
            "Epoch 269/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2278 - val_loss: 0.2754\n",
            "Epoch 270/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2276 - val_loss: 0.2754\n",
            "Epoch 271/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2274 - val_loss: 0.2754\n",
            "Epoch 272/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2272 - val_loss: 0.2753\n",
            "Epoch 273/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2271 - val_loss: 0.2753\n",
            "Epoch 274/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2270 - val_loss: 0.2752\n",
            "Epoch 275/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2268 - val_loss: 0.2752\n",
            "Epoch 276/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2266 - val_loss: 0.2751\n",
            "Epoch 277/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2266 - val_loss: 0.2751\n",
            "Epoch 278/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2263 - val_loss: 0.2751\n",
            "Epoch 279/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2262 - val_loss: 0.2751\n",
            "Epoch 280/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2261 - val_loss: 0.2751\n",
            "Epoch 281/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2259 - val_loss: 0.2750\n",
            "Epoch 282/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2257 - val_loss: 0.2749\n",
            "Epoch 283/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2257 - val_loss: 0.2749\n",
            "Epoch 284/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2254 - val_loss: 0.2749\n",
            "Epoch 285/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2253 - val_loss: 0.2748\n",
            "Epoch 286/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2251 - val_loss: 0.2748\n",
            "Epoch 287/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2250 - val_loss: 0.2747\n",
            "Epoch 288/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2248 - val_loss: 0.2747\n",
            "Epoch 289/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2246 - val_loss: 0.2747\n",
            "Epoch 290/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2245 - val_loss: 0.2747\n",
            "Epoch 291/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2244 - val_loss: 0.2746\n",
            "Epoch 292/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2243 - val_loss: 0.2746\n",
            "Epoch 293/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2241 - val_loss: 0.2745\n",
            "Epoch 294/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2239 - val_loss: 0.2745\n",
            "Epoch 295/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2238 - val_loss: 0.2745\n",
            "Epoch 296/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2235 - val_loss: 0.2744\n",
            "Epoch 297/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2235 - val_loss: 0.2744\n",
            "Epoch 298/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2234 - val_loss: 0.2744\n",
            "Epoch 299/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2231 - val_loss: 0.2744\n",
            "Epoch 300/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2230 - val_loss: 0.2743\n",
            "Epoch 301/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2228 - val_loss: 0.2744\n",
            "Epoch 302/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2228 - val_loss: 0.2743\n",
            "Epoch 303/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2226 - val_loss: 0.2742\n",
            "Epoch 304/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2225 - val_loss: 0.2741\n",
            "Epoch 305/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2223 - val_loss: 0.2741\n",
            "Epoch 306/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2221 - val_loss: 0.2741\n",
            "Epoch 307/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2219 - val_loss: 0.2740\n",
            "Epoch 308/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2218 - val_loss: 0.2742\n",
            "Epoch 309/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2218 - val_loss: 0.2741\n",
            "Epoch 310/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2216 - val_loss: 0.2740\n",
            "Epoch 311/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2214 - val_loss: 0.2739\n",
            "Epoch 312/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2213 - val_loss: 0.2739\n",
            "Epoch 313/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2211 - val_loss: 0.2739\n",
            "Epoch 314/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2211 - val_loss: 0.2740\n",
            "Epoch 315/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2209 - val_loss: 0.2738\n",
            "Epoch 316/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2207 - val_loss: 0.2738\n",
            "Epoch 317/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2206 - val_loss: 0.2738\n",
            "Epoch 318/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2204 - val_loss: 0.2737\n",
            "Epoch 319/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2203 - val_loss: 0.2737\n",
            "Epoch 320/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2201 - val_loss: 0.2736\n",
            "Epoch 321/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2201 - val_loss: 0.2736\n",
            "Epoch 322/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2199 - val_loss: 0.2736\n",
            "Epoch 323/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2197 - val_loss: 0.2735\n",
            "Epoch 324/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2195 - val_loss: 0.2736\n",
            "Epoch 325/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2194 - val_loss: 0.2735\n",
            "Epoch 326/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2192 - val_loss: 0.2735\n",
            "Epoch 327/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2191 - val_loss: 0.2735\n",
            "Epoch 328/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2190 - val_loss: 0.2734\n",
            "Epoch 329/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2189 - val_loss: 0.2734\n",
            "Epoch 330/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2187 - val_loss: 0.2734\n",
            "Epoch 331/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2186 - val_loss: 0.2734\n",
            "Epoch 332/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2185 - val_loss: 0.2734\n",
            "Epoch 333/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2183 - val_loss: 0.2733\n",
            "Epoch 334/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2182 - val_loss: 0.2733\n",
            "Epoch 335/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2181 - val_loss: 0.2733\n",
            "Epoch 336/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2180 - val_loss: 0.2733\n",
            "Epoch 337/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2178 - val_loss: 0.2732\n",
            "Epoch 338/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2176 - val_loss: 0.2732\n",
            "Epoch 339/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2175 - val_loss: 0.2731\n",
            "Epoch 340/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2173 - val_loss: 0.2731\n",
            "Epoch 341/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2172 - val_loss: 0.2731\n",
            "Epoch 342/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2170 - val_loss: 0.2730\n",
            "Epoch 343/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2168 - val_loss: 0.2730\n",
            "Epoch 344/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2168 - val_loss: 0.2730\n",
            "Epoch 345/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2166 - val_loss: 0.2730\n",
            "Epoch 346/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2165 - val_loss: 0.2730\n",
            "Epoch 347/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2164 - val_loss: 0.2729\n",
            "Epoch 348/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2162 - val_loss: 0.2729\n",
            "Epoch 349/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2161 - val_loss: 0.2729\n",
            "Epoch 350/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2160 - val_loss: 0.2728\n",
            "Epoch 351/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2159 - val_loss: 0.2728\n",
            "Epoch 352/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2156 - val_loss: 0.2729\n",
            "Epoch 353/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2156 - val_loss: 0.2728\n",
            "Epoch 354/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2155 - val_loss: 0.2728\n",
            "Epoch 355/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2153 - val_loss: 0.2728\n",
            "Epoch 356/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2152 - val_loss: 0.2727\n",
            "Epoch 357/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2149 - val_loss: 0.2727\n",
            "Epoch 358/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2150 - val_loss: 0.2727\n",
            "Epoch 359/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2147 - val_loss: 0.2727\n",
            "Epoch 360/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2148 - val_loss: 0.2726\n",
            "Epoch 361/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2145 - val_loss: 0.2726\n",
            "Epoch 362/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2144 - val_loss: 0.2727\n",
            "Epoch 363/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2142 - val_loss: 0.2726\n",
            "Epoch 364/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2141 - val_loss: 0.2726\n",
            "Epoch 365/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2140 - val_loss: 0.2726\n",
            "Epoch 366/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2139 - val_loss: 0.2725\n",
            "Epoch 367/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2136 - val_loss: 0.2725\n",
            "Epoch 368/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2136 - val_loss: 0.2726\n",
            "Epoch 369/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2135 - val_loss: 0.2725\n",
            "Epoch 370/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2134 - val_loss: 0.2724\n",
            "Epoch 371/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2133 - val_loss: 0.2726\n",
            "Epoch 372/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2131 - val_loss: 0.2724\n",
            "Epoch 373/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2129 - val_loss: 0.2724\n",
            "Epoch 374/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2128 - val_loss: 0.2723\n",
            "Epoch 375/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2126 - val_loss: 0.2723\n",
            "Epoch 376/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2126 - val_loss: 0.2723\n",
            "Epoch 377/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2124 - val_loss: 0.2723\n",
            "Epoch 378/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2123 - val_loss: 0.2723\n",
            "Epoch 379/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2122 - val_loss: 0.2723\n",
            "Epoch 380/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2121 - val_loss: 0.2723\n",
            "Epoch 381/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2120 - val_loss: 0.2722\n",
            "Epoch 382/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2119 - val_loss: 0.2722\n",
            "Epoch 383/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2117 - val_loss: 0.2722\n",
            "Epoch 384/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2116 - val_loss: 0.2722\n",
            "Epoch 385/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2115 - val_loss: 0.2721\n",
            "Epoch 386/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2114 - val_loss: 0.2722\n",
            "Epoch 387/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2113 - val_loss: 0.2722\n",
            "Epoch 388/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2111 - val_loss: 0.2721\n",
            "Epoch 389/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2110 - val_loss: 0.2721\n",
            "Epoch 390/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2109 - val_loss: 0.2721\n",
            "Epoch 391/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2107 - val_loss: 0.2721\n",
            "Epoch 392/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2105 - val_loss: 0.2720\n",
            "Epoch 393/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2105 - val_loss: 0.2721\n",
            "Epoch 394/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2104 - val_loss: 0.2720\n",
            "Epoch 395/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2103 - val_loss: 0.2720\n",
            "Epoch 396/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2101 - val_loss: 0.2720\n",
            "Epoch 397/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2100 - val_loss: 0.2720\n",
            "Epoch 398/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2100 - val_loss: 0.2720\n",
            "Epoch 399/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2098 - val_loss: 0.2720\n",
            "Epoch 400/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2097 - val_loss: 0.2719\n",
            "Epoch 401/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2095 - val_loss: 0.2719\n",
            "Epoch 402/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2094 - val_loss: 0.2719\n",
            "Epoch 403/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2092 - val_loss: 0.2719\n",
            "Epoch 404/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2092 - val_loss: 0.2719\n",
            "Epoch 405/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2090 - val_loss: 0.2719\n",
            "Epoch 406/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2089 - val_loss: 0.2718\n",
            "Epoch 407/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2088 - val_loss: 0.2718\n",
            "Epoch 408/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2088 - val_loss: 0.2719\n",
            "Epoch 409/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2087 - val_loss: 0.2718\n",
            "Epoch 410/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2085 - val_loss: 0.2718\n",
            "Epoch 411/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2084 - val_loss: 0.2717\n",
            "Epoch 412/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2083 - val_loss: 0.2719\n",
            "Epoch 413/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2082 - val_loss: 0.2717\n",
            "Epoch 414/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2080 - val_loss: 0.2717\n",
            "Epoch 415/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2079 - val_loss: 0.2717\n",
            "Epoch 416/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2078 - val_loss: 0.2717\n",
            "Epoch 417/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2076 - val_loss: 0.2718\n",
            "Epoch 418/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2075 - val_loss: 0.2717\n",
            "Epoch 419/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2075 - val_loss: 0.2717\n",
            "Epoch 420/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2073 - val_loss: 0.2717\n",
            "Epoch 421/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2072 - val_loss: 0.2716\n",
            "Epoch 422/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2072 - val_loss: 0.2718\n",
            "Epoch 423/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2070 - val_loss: 0.2716\n",
            "Epoch 424/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2069 - val_loss: 0.2716\n",
            "Epoch 425/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2068 - val_loss: 0.2717\n",
            "Epoch 426/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2066 - val_loss: 0.2716\n",
            "Epoch 427/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2065 - val_loss: 0.2716\n",
            "Epoch 428/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2064 - val_loss: 0.2716\n",
            "Epoch 429/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2063 - val_loss: 0.2716\n",
            "Epoch 430/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2062 - val_loss: 0.2716\n",
            "Epoch 431/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2061 - val_loss: 0.2715\n",
            "Epoch 432/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2060 - val_loss: 0.2717\n",
            "Epoch 433/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2059 - val_loss: 0.2715\n",
            "Epoch 434/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2058 - val_loss: 0.2715\n",
            "Epoch 435/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2057 - val_loss: 0.2715\n",
            "Epoch 436/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2056 - val_loss: 0.2715\n",
            "Epoch 437/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2055 - val_loss: 0.2715\n",
            "Epoch 438/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2052 - val_loss: 0.2714\n",
            "Epoch 439/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2052 - val_loss: 0.2715\n",
            "Epoch 440/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2051 - val_loss: 0.2714\n",
            "Epoch 441/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2050 - val_loss: 0.2714\n",
            "Epoch 442/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2049 - val_loss: 0.2714\n",
            "Epoch 443/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2047 - val_loss: 0.2715\n",
            "Epoch 444/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2046 - val_loss: 0.2714\n",
            "Epoch 445/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2047 - val_loss: 0.2714\n",
            "Epoch 446/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2045 - val_loss: 0.2716\n",
            "Epoch 447/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2044 - val_loss: 0.2714\n",
            "Epoch 448/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2042 - val_loss: 0.2714\n",
            "Epoch 449/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2042 - val_loss: 0.2713\n",
            "Epoch 450/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2040 - val_loss: 0.2713\n",
            "Epoch 451/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2040 - val_loss: 0.2714\n",
            "Epoch 452/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2039 - val_loss: 0.2716\n",
            "Epoch 453/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2038 - val_loss: 0.2714\n",
            "Epoch 454/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2036 - val_loss: 0.2713\n",
            "Epoch 455/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2035 - val_loss: 0.2714\n",
            "Epoch 456/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2034 - val_loss: 0.2713\n",
            "Epoch 457/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2033 - val_loss: 0.2713\n",
            "Epoch 458/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2033 - val_loss: 0.2714\n",
            "Epoch 459/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2031 - val_loss: 0.2713\n",
            "Epoch 460/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2031 - val_loss: 0.2713\n",
            "Epoch 461/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2028 - val_loss: 0.2713\n",
            "Epoch 462/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2028 - val_loss: 0.2712\n",
            "Epoch 463/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2026 - val_loss: 0.2713\n",
            "Epoch 464/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2026 - val_loss: 0.2713\n",
            "Epoch 465/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2025 - val_loss: 0.2713\n",
            "Epoch 466/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2023 - val_loss: 0.2712\n",
            "Epoch 467/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2023 - val_loss: 0.2713\n",
            "Epoch 468/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2021 - val_loss: 0.2714\n",
            "Epoch 469/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2020 - val_loss: 0.2713\n",
            "Epoch 470/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2019 - val_loss: 0.2712\n",
            "Epoch 471/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2018 - val_loss: 0.2712\n",
            "Epoch 472/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2018 - val_loss: 0.2712\n",
            "Epoch 473/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2017 - val_loss: 0.2713\n",
            "Epoch 474/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2016 - val_loss: 0.2712\n",
            "Epoch 475/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2015 - val_loss: 0.2712\n",
            "Epoch 476/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2013 - val_loss: 0.2713\n",
            "Epoch 477/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2012 - val_loss: 0.2712\n",
            "Epoch 478/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2011 - val_loss: 0.2712\n",
            "Epoch 479/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2011 - val_loss: 0.2715\n",
            "Epoch 480/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2009 - val_loss: 0.2712\n",
            "Epoch 481/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2008 - val_loss: 0.2711\n",
            "Epoch 482/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2007 - val_loss: 0.2713\n",
            "Epoch 483/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2007 - val_loss: 0.2713\n",
            "Epoch 484/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2006 - val_loss: 0.2711\n",
            "Epoch 485/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2007 - val_loss: 0.2712\n",
            "Epoch 486/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2004 - val_loss: 0.2714\n",
            "Epoch 487/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2004 - val_loss: 0.2711\n",
            "Epoch 488/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2001 - val_loss: 0.2712\n",
            "Epoch 489/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2001 - val_loss: 0.2715\n",
            "Epoch 490/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2000 - val_loss: 0.2711\n",
            "Epoch 491/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1999 - val_loss: 0.2711\n",
            "Epoch 492/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1998 - val_loss: 0.2711\n",
            "Epoch 493/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1997 - val_loss: 0.2712\n",
            "Epoch 494/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1997 - val_loss: 0.2711\n",
            "Epoch 495/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1995 - val_loss: 0.2711\n",
            "Epoch 496/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1994 - val_loss: 0.2711\n",
            "Epoch 497/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1993 - val_loss: 0.2713\n",
            "Epoch 498/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1993 - val_loss: 0.2711\n",
            "Epoch 499/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1992 - val_loss: 0.2712\n",
            "Epoch 500/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1990 - val_loss: 0.2711\n",
            "Epoch 501/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1990 - val_loss: 0.2711\n",
            "Epoch 502/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1988 - val_loss: 0.2712\n",
            "Epoch 503/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1987 - val_loss: 0.2715\n",
            "Epoch 504/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1986 - val_loss: 0.2711\n",
            "Epoch 505/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1986 - val_loss: 0.2711\n",
            "Epoch 506/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1985 - val_loss: 0.2711\n",
            "Epoch 507/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1984 - val_loss: 0.2711\n",
            "Epoch 508/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1983 - val_loss: 0.2713\n",
            "Epoch 509/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1982 - val_loss: 0.2712\n",
            "Epoch 510/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1981 - val_loss: 0.2712\n",
            "Epoch 511/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1981 - val_loss: 0.2712\n",
            "Epoch 512/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1979 - val_loss: 0.2711\n",
            "Epoch 513/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1979 - val_loss: 0.2712\n",
            "Epoch 514/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1977 - val_loss: 0.2710\n",
            "Epoch 515/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1977 - val_loss: 0.2711\n",
            "Epoch 516/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1976 - val_loss: 0.2712\n",
            "Epoch 517/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1976 - val_loss: 0.2711\n",
            "Epoch 518/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1974 - val_loss: 0.2711\n",
            "Epoch 519/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1974 - val_loss: 0.2711\n",
            "Epoch 520/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1973 - val_loss: 0.2711\n",
            "Epoch 521/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1972 - val_loss: 0.2713\n",
            "Epoch 522/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1970 - val_loss: 0.2710\n",
            "Epoch 523/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1969 - val_loss: 0.2710\n",
            "Epoch 524/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1969 - val_loss: 0.2716\n",
            "Epoch 525/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1969 - val_loss: 0.2712\n",
            "Epoch 526/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1967 - val_loss: 0.2713\n",
            "Epoch 527/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1966 - val_loss: 0.2712\n",
            "Epoch 528/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1966 - val_loss: 0.2710\n",
            "Epoch 529/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1963 - val_loss: 0.2712\n",
            "Epoch 530/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1963 - val_loss: 0.2710\n",
            "Epoch 531/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1963 - val_loss: 0.2711\n",
            "Epoch 532/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1962 - val_loss: 0.2713\n",
            "Epoch 533/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1962 - val_loss: 0.2710\n",
            "Epoch 534/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1960 - val_loss: 0.2710\n",
            "Epoch 535/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1958 - val_loss: 0.2712\n",
            "Epoch 536/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1958 - val_loss: 0.2711\n",
            "Epoch 537/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1958 - val_loss: 0.2711\n",
            "Epoch 538/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1957 - val_loss: 0.2711\n",
            "Epoch 539/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1955 - val_loss: 0.2710\n",
            "Epoch 540/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1954 - val_loss: 0.2710\n",
            "Epoch 541/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1954 - val_loss: 0.2711\n",
            "Epoch 542/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1953 - val_loss: 0.2711\n",
            "Epoch 543/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1952 - val_loss: 0.2710\n",
            "Epoch 544/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1951 - val_loss: 0.2711\n",
            "Epoch 545/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1950 - val_loss: 0.2712\n",
            "Epoch 546/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1949 - val_loss: 0.2711\n",
            "Epoch 547/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1949 - val_loss: 0.2714\n",
            "Epoch 548/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1948 - val_loss: 0.2715\n",
            "Epoch 549/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1948 - val_loss: 0.2710\n",
            "Epoch 550/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1947 - val_loss: 0.2710\n",
            "Epoch 551/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1945 - val_loss: 0.2711\n",
            "Epoch 552/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1946 - val_loss: 0.2710\n",
            "Epoch 553/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1944 - val_loss: 0.2711\n",
            "Epoch 554/1000\n",
            "34/34 [==============================] - 1s 26ms/step - loss: 0.1944 - val_loss: 0.2711\n",
            "Epoch 555/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1942 - val_loss: 0.2712\n",
            "Epoch 556/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1941 - val_loss: 0.2711\n",
            "Epoch 557/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1941 - val_loss: 0.2711\n",
            "Epoch 558/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1940 - val_loss: 0.2711\n",
            "Epoch 559/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1940 - val_loss: 0.2711\n",
            "Epoch 560/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1938 - val_loss: 0.2711\n",
            "Epoch 561/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1937 - val_loss: 0.2711\n",
            "Epoch 562/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1936 - val_loss: 0.2710\n",
            "Epoch 563/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1935 - val_loss: 0.2711\n",
            "Epoch 564/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1935 - val_loss: 0.2713\n",
            "Epoch 565/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1935 - val_loss: 0.2710\n",
            "Epoch 566/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1933 - val_loss: 0.2711\n",
            "Epoch 567/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1934 - val_loss: 0.2711\n",
            "Epoch 568/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1933 - val_loss: 0.2711\n",
            "Epoch 569/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1931 - val_loss: 0.2713\n",
            "Epoch 570/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1930 - val_loss: 0.2714\n",
            "Epoch 571/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1930 - val_loss: 0.2711\n",
            "Epoch 572/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1928 - val_loss: 0.2714\n",
            "Epoch 573/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1928 - val_loss: 0.2711\n",
            "Epoch 574/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1926 - val_loss: 0.2713\n",
            "Epoch 575/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1925 - val_loss: 0.2710\n",
            "Epoch 576/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1926 - val_loss: 0.2711\n",
            "Epoch 577/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1925 - val_loss: 0.2712\n",
            "Epoch 578/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1924 - val_loss: 0.2712\n",
            "Epoch 579/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1922 - val_loss: 0.2712\n",
            "Epoch 580/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1922 - val_loss: 0.2711\n",
            "Epoch 581/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1921 - val_loss: 0.2711\n",
            "Epoch 582/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1920 - val_loss: 0.2716\n",
            "Epoch 583/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1921 - val_loss: 0.2711\n",
            "Epoch 584/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1919 - val_loss: 0.2711\n",
            "Epoch 585/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1918 - val_loss: 0.2714\n",
            "Epoch 586/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1919 - val_loss: 0.2711\n",
            "Epoch 587/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1917 - val_loss: 0.2712\n",
            "Epoch 588/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1917 - val_loss: 0.2711\n",
            "Epoch 589/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1915 - val_loss: 0.2711\n",
            "Epoch 590/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1914 - val_loss: 0.2711\n",
            "Epoch 591/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1914 - val_loss: 0.2716\n",
            "Epoch 592/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1913 - val_loss: 0.2711\n",
            "Epoch 593/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1912 - val_loss: 0.2712\n",
            "Epoch 594/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1912 - val_loss: 0.2711\n",
            "Epoch 595/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1909 - val_loss: 0.2711\n",
            "Epoch 596/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1909 - val_loss: 0.2712\n",
            "Epoch 597/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1909 - val_loss: 0.2711\n",
            "Epoch 598/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1908 - val_loss: 0.2711\n",
            "Epoch 599/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1908 - val_loss: 0.2715\n",
            "Epoch 600/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1907 - val_loss: 0.2713\n",
            "Epoch 601/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1906 - val_loss: 0.2711\n",
            "Epoch 602/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1906 - val_loss: 0.2712\n",
            "Epoch 603/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1904 - val_loss: 0.2711\n",
            "Epoch 604/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1904 - val_loss: 0.2712\n",
            "Epoch 605/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1902 - val_loss: 0.2712\n",
            "Epoch 606/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1902 - val_loss: 0.2712\n",
            "Epoch 607/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1902 - val_loss: 0.2712\n",
            "Epoch 608/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1901 - val_loss: 0.2712\n",
            "Epoch 609/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1900 - val_loss: 0.2713\n",
            "Epoch 610/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1900 - val_loss: 0.2713\n",
            "Epoch 611/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1899 - val_loss: 0.2713\n",
            "Epoch 612/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1898 - val_loss: 0.2713\n",
            "Epoch 613/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1898 - val_loss: 0.2712\n",
            "Epoch 614/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1896 - val_loss: 0.2713\n",
            "Epoch 615/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1895 - val_loss: 0.2713\n",
            "Epoch 616/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1895 - val_loss: 0.2713\n",
            "Epoch 617/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1894 - val_loss: 0.2714\n",
            "Epoch 618/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1895 - val_loss: 0.2712\n",
            "Epoch 619/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1893 - val_loss: 0.2713\n",
            "Epoch 620/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1892 - val_loss: 0.2713\n",
            "Epoch 621/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1890 - val_loss: 0.2713\n",
            "Epoch 622/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1891 - val_loss: 0.2717\n",
            "Epoch 623/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1891 - val_loss: 0.2712\n",
            "Epoch 624/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1889 - val_loss: 0.2713\n",
            "Epoch 625/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1889 - val_loss: 0.2715\n",
            "Epoch 626/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1888 - val_loss: 0.2713\n",
            "Epoch 627/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1887 - val_loss: 0.2713\n",
            "Epoch 628/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1886 - val_loss: 0.2713\n",
            "Epoch 629/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1886 - val_loss: 0.2715\n",
            "Epoch 630/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1885 - val_loss: 0.2713\n",
            "Epoch 631/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1884 - val_loss: 0.2714\n",
            "Epoch 632/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1884 - val_loss: 0.2713\n",
            "Epoch 633/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1884 - val_loss: 0.2716\n",
            "Epoch 634/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1884 - val_loss: 0.2713\n",
            "Epoch 635/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1881 - val_loss: 0.2713\n",
            "Epoch 636/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1880 - val_loss: 0.2715\n",
            "Epoch 637/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1879 - val_loss: 0.2713\n",
            "Epoch 638/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1879 - val_loss: 0.2714\n",
            "Epoch 639/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1881 - val_loss: 0.2714\n",
            "Epoch 640/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1879 - val_loss: 0.2714\n",
            "Epoch 641/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1879 - val_loss: 0.2714\n",
            "Epoch 642/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1877 - val_loss: 0.2714\n",
            "Epoch 643/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1876 - val_loss: 0.2713\n",
            "Epoch 644/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1876 - val_loss: 0.2714\n",
            "Epoch 645/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1875 - val_loss: 0.2713\n",
            "Epoch 646/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1874 - val_loss: 0.2716\n",
            "Epoch 647/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1873 - val_loss: 0.2718\n",
            "Epoch 648/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1872 - val_loss: 0.2714\n",
            "Epoch 649/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1872 - val_loss: 0.2715\n",
            "Epoch 650/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1873 - val_loss: 0.2714\n",
            "Epoch 651/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1871 - val_loss: 0.2715\n",
            "Epoch 652/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1870 - val_loss: 0.2715\n",
            "Epoch 653/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1869 - val_loss: 0.2715\n",
            "Epoch 654/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1869 - val_loss: 0.2719\n",
            "Epoch 655/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1870 - val_loss: 0.2719\n",
            "Epoch 656/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1868 - val_loss: 0.2716\n",
            "Epoch 657/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1867 - val_loss: 0.2715\n",
            "Epoch 658/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1867 - val_loss: 0.2716\n",
            "Epoch 659/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1865 - val_loss: 0.2715\n",
            "Epoch 660/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1864 - val_loss: 0.2716\n",
            "Epoch 661/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1864 - val_loss: 0.2716\n",
            "Epoch 662/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1863 - val_loss: 0.2715\n",
            "Epoch 663/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1863 - val_loss: 0.2717\n",
            "Epoch 664/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1863 - val_loss: 0.2715\n",
            "Epoch 665/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1861 - val_loss: 0.2715\n",
            "Epoch 666/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1861 - val_loss: 0.2715\n",
            "Epoch 667/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1860 - val_loss: 0.2716\n",
            "Epoch 668/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1860 - val_loss: 0.2716\n",
            "Epoch 669/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1859 - val_loss: 0.2716\n",
            "Epoch 670/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1858 - val_loss: 0.2715\n",
            "Epoch 671/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1858 - val_loss: 0.2718\n",
            "Epoch 672/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1858 - val_loss: 0.2715\n",
            "Epoch 673/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1856 - val_loss: 0.2715\n",
            "Epoch 674/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1857 - val_loss: 0.2716\n",
            "Epoch 675/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1855 - val_loss: 0.2717\n",
            "Epoch 676/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1854 - val_loss: 0.2715\n",
            "Epoch 677/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1853 - val_loss: 0.2718\n",
            "Epoch 678/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1854 - val_loss: 0.2717\n",
            "Epoch 679/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1854 - val_loss: 0.2716\n",
            "Epoch 680/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1851 - val_loss: 0.2716\n",
            "Epoch 681/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1853 - val_loss: 0.2716\n",
            "Epoch 682/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1851 - val_loss: 0.2716\n",
            "Epoch 683/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1851 - val_loss: 0.2719\n",
            "Epoch 684/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1850 - val_loss: 0.2717\n",
            "Epoch 685/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1849 - val_loss: 0.2717\n",
            "Epoch 686/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1848 - val_loss: 0.2719\n",
            "Epoch 687/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1847 - val_loss: 0.2717\n",
            "Epoch 688/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1848 - val_loss: 0.2720\n",
            "Epoch 689/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1849 - val_loss: 0.2716\n",
            "Epoch 690/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1845 - val_loss: 0.2717\n",
            "Epoch 691/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1846 - val_loss: 0.2717\n",
            "Epoch 692/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1845 - val_loss: 0.2717\n",
            "Epoch 693/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1843 - val_loss: 0.2716\n",
            "Epoch 694/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1844 - val_loss: 0.2718\n",
            "Epoch 695/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1843 - val_loss: 0.2717\n",
            "Epoch 696/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1841 - val_loss: 0.2718\n",
            "Epoch 697/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1842 - val_loss: 0.2717\n",
            "Epoch 698/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1841 - val_loss: 0.2718\n",
            "Epoch 699/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1841 - val_loss: 0.2719\n",
            "Epoch 700/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1840 - val_loss: 0.2718\n",
            "Epoch 701/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1839 - val_loss: 0.2719\n",
            "Epoch 702/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1841 - val_loss: 0.2717\n",
            "Epoch 703/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1838 - val_loss: 0.2717\n",
            "Epoch 704/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1837 - val_loss: 0.2719\n",
            "Epoch 705/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1837 - val_loss: 0.2718\n",
            "Epoch 706/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1836 - val_loss: 0.2718\n",
            "Epoch 707/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1835 - val_loss: 0.2717\n",
            "Epoch 708/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1835 - val_loss: 0.2720\n",
            "Epoch 709/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1836 - val_loss: 0.2722\n",
            "Epoch 710/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1834 - val_loss: 0.2720\n",
            "Epoch 711/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1834 - val_loss: 0.2718\n",
            "Epoch 712/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1833 - val_loss: 0.2720\n",
            "Epoch 713/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1832 - val_loss: 0.2718\n",
            "Epoch 714/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1831 - val_loss: 0.2722\n",
            "Epoch 715/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1833 - val_loss: 0.2719\n",
            "Epoch 716/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1831 - val_loss: 0.2718\n",
            "Epoch 717/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1829 - val_loss: 0.2719\n",
            "Epoch 718/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1829 - val_loss: 0.2720\n",
            "Epoch 719/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1829 - val_loss: 0.2719\n",
            "Epoch 720/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1828 - val_loss: 0.2719\n",
            "Epoch 721/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1828 - val_loss: 0.2720\n",
            "Epoch 722/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1827 - val_loss: 0.2718\n",
            "Epoch 723/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1826 - val_loss: 0.2719\n",
            "Epoch 724/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1826 - val_loss: 0.2719\n",
            "Epoch 725/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1825 - val_loss: 0.2720\n",
            "Epoch 726/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1824 - val_loss: 0.2720\n",
            "Epoch 727/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1825 - val_loss: 0.2721\n",
            "Epoch 728/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1824 - val_loss: 0.2721\n",
            "Epoch 729/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1823 - val_loss: 0.2719\n",
            "Epoch 730/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1823 - val_loss: 0.2720\n",
            "Epoch 731/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1822 - val_loss: 0.2721\n",
            "Epoch 732/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1821 - val_loss: 0.2722\n",
            "Epoch 733/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1821 - val_loss: 0.2720\n",
            "Epoch 734/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1819 - val_loss: 0.2721\n",
            "Epoch 735/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1819 - val_loss: 0.2721\n",
            "Epoch 736/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1819 - val_loss: 0.2720\n",
            "Epoch 737/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1818 - val_loss: 0.2722\n",
            "Epoch 738/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1818 - val_loss: 0.2721\n",
            "Epoch 739/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1816 - val_loss: 0.2721\n",
            "Epoch 740/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1817 - val_loss: 0.2723\n",
            "Epoch 741/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1817 - val_loss: 0.2722\n",
            "Epoch 742/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1817 - val_loss: 0.2720\n",
            "Epoch 743/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1815 - val_loss: 0.2722\n",
            "Epoch 744/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1815 - val_loss: 0.2722\n",
            "Epoch 745/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1814 - val_loss: 0.2721\n",
            "Epoch 746/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1814 - val_loss: 0.2721\n",
            "Epoch 747/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1814 - val_loss: 0.2721\n",
            "Epoch 748/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1813 - val_loss: 0.2721\n",
            "Epoch 749/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1812 - val_loss: 0.2722\n",
            "Epoch 750/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1811 - val_loss: 0.2723\n",
            "Epoch 751/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1810 - val_loss: 0.2722\n",
            "Epoch 752/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1810 - val_loss: 0.2721\n",
            "Epoch 753/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1810 - val_loss: 0.2723\n",
            "Epoch 754/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1810 - val_loss: 0.2723\n",
            "Epoch 755/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1809 - val_loss: 0.2722\n",
            "Epoch 756/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1808 - val_loss: 0.2722\n",
            "Epoch 757/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1807 - val_loss: 0.2723\n",
            "Epoch 758/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1807 - val_loss: 0.2723\n",
            "Epoch 759/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1807 - val_loss: 0.2722\n",
            "Epoch 760/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1806 - val_loss: 0.2724\n",
            "Epoch 761/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1806 - val_loss: 0.2724\n",
            "Epoch 762/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1806 - val_loss: 0.2724\n",
            "Epoch 763/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1804 - val_loss: 0.2724\n",
            "Epoch 764/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1803 - val_loss: 0.2724\n",
            "Epoch 765/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1803 - val_loss: 0.2724\n",
            "Epoch 766/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1803 - val_loss: 0.2727\n",
            "Epoch 767/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1803 - val_loss: 0.2724\n",
            "Epoch 768/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1802 - val_loss: 0.2723\n",
            "Epoch 769/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1801 - val_loss: 0.2725\n",
            "Epoch 770/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1799 - val_loss: 0.2728\n",
            "Epoch 771/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1801 - val_loss: 0.2724\n",
            "Epoch 772/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1800 - val_loss: 0.2726\n",
            "Epoch 773/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1799 - val_loss: 0.2725\n",
            "Epoch 774/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1798 - val_loss: 0.2724\n",
            "Epoch 775/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1798 - val_loss: 0.2724\n",
            "Epoch 776/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1797 - val_loss: 0.2726\n",
            "Epoch 777/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1798 - val_loss: 0.2724\n",
            "Epoch 778/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1797 - val_loss: 0.2724\n",
            "Epoch 779/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1795 - val_loss: 0.2724\n",
            "Epoch 780/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1795 - val_loss: 0.2729\n",
            "Epoch 781/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1795 - val_loss: 0.2726\n",
            "Epoch 782/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1795 - val_loss: 0.2727\n",
            "Epoch 783/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1794 - val_loss: 0.2725\n",
            "Epoch 784/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1794 - val_loss: 0.2727\n",
            "Epoch 785/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1793 - val_loss: 0.2727\n",
            "Epoch 786/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1793 - val_loss: 0.2725\n",
            "Epoch 787/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1791 - val_loss: 0.2726\n",
            "Epoch 788/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1791 - val_loss: 0.2726\n",
            "Epoch 789/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1791 - val_loss: 0.2726\n",
            "Epoch 790/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1791 - val_loss: 0.2726\n",
            "Epoch 791/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1790 - val_loss: 0.2725\n",
            "Epoch 792/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1791 - val_loss: 0.2727\n",
            "Epoch 793/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1790 - val_loss: 0.2726\n",
            "Epoch 794/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1790 - val_loss: 0.2726\n",
            "Epoch 795/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1788 - val_loss: 0.2728\n",
            "Epoch 796/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1789 - val_loss: 0.2726\n",
            "Epoch 797/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1787 - val_loss: 0.2726\n",
            "Epoch 798/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1786 - val_loss: 0.2725\n",
            "Epoch 799/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1786 - val_loss: 0.2727\n",
            "Epoch 800/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1785 - val_loss: 0.2726\n",
            "Epoch 801/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1785 - val_loss: 0.2727\n",
            "Epoch 802/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1785 - val_loss: 0.2727\n",
            "Epoch 803/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1785 - val_loss: 0.2726\n",
            "Epoch 804/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1784 - val_loss: 0.2726\n",
            "Epoch 805/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1783 - val_loss: 0.2728\n",
            "Epoch 806/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1783 - val_loss: 0.2728\n",
            "Epoch 807/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1782 - val_loss: 0.2727\n",
            "Epoch 808/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1782 - val_loss: 0.2727\n",
            "Epoch 809/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1781 - val_loss: 0.2727\n",
            "Epoch 810/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1781 - val_loss: 0.2727\n",
            "Epoch 811/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1779 - val_loss: 0.2728\n",
            "Epoch 812/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1780 - val_loss: 0.2729\n",
            "Epoch 813/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1779 - val_loss: 0.2727\n",
            "Epoch 814/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1779 - val_loss: 0.2732\n",
            "Epoch 815/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1779 - val_loss: 0.2730\n",
            "Epoch 816/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1779 - val_loss: 0.2730\n",
            "Epoch 817/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1777 - val_loss: 0.2727\n",
            "Epoch 818/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1776 - val_loss: 0.2730\n",
            "Epoch 819/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1778 - val_loss: 0.2728\n",
            "Epoch 820/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1776 - val_loss: 0.2728\n",
            "Epoch 821/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1776 - val_loss: 0.2729\n",
            "Epoch 822/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1775 - val_loss: 0.2729\n",
            "Epoch 823/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1774 - val_loss: 0.2730\n",
            "Epoch 824/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1774 - val_loss: 0.2729\n",
            "Epoch 825/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1774 - val_loss: 0.2729\n",
            "Epoch 826/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1773 - val_loss: 0.2732\n",
            "Epoch 827/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1774 - val_loss: 0.2729\n",
            "Epoch 828/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1772 - val_loss: 0.2733\n",
            "Epoch 829/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1772 - val_loss: 0.2730\n",
            "Epoch 830/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1771 - val_loss: 0.2729\n",
            "Epoch 831/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1771 - val_loss: 0.2729\n",
            "Epoch 832/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1770 - val_loss: 0.2730\n",
            "Epoch 833/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1771 - val_loss: 0.2734\n",
            "Epoch 834/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1770 - val_loss: 0.2730\n",
            "Epoch 835/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1769 - val_loss: 0.2730\n",
            "Epoch 836/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1769 - val_loss: 0.2731\n",
            "Epoch 837/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1767 - val_loss: 0.2731\n",
            "Epoch 838/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1768 - val_loss: 0.2731\n",
            "Epoch 839/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1769 - val_loss: 0.2730\n",
            "Epoch 840/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1767 - val_loss: 0.2731\n",
            "Epoch 841/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1767 - val_loss: 0.2731\n",
            "Epoch 842/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1766 - val_loss: 0.2732\n",
            "Epoch 843/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1766 - val_loss: 0.2733\n",
            "Epoch 844/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1766 - val_loss: 0.2731\n",
            "Epoch 845/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1765 - val_loss: 0.2731\n",
            "Epoch 846/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1765 - val_loss: 0.2732\n",
            "Epoch 847/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1763 - val_loss: 0.2732\n",
            "Epoch 848/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1764 - val_loss: 0.2731\n",
            "Epoch 849/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1763 - val_loss: 0.2731\n",
            "Epoch 850/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1762 - val_loss: 0.2731\n",
            "Epoch 851/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1763 - val_loss: 0.2735\n",
            "Epoch 852/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1763 - val_loss: 0.2731\n",
            "Epoch 853/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1761 - val_loss: 0.2731\n",
            "Epoch 854/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1760 - val_loss: 0.2732\n",
            "Epoch 855/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1760 - val_loss: 0.2731\n",
            "Epoch 856/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1760 - val_loss: 0.2733\n",
            "Epoch 857/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1759 - val_loss: 0.2732\n",
            "Epoch 858/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1760 - val_loss: 0.2733\n",
            "Epoch 859/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1759 - val_loss: 0.2733\n",
            "Epoch 860/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1757 - val_loss: 0.2733\n",
            "Epoch 861/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1757 - val_loss: 0.2734\n",
            "Epoch 862/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1758 - val_loss: 0.2733\n",
            "Epoch 863/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1757 - val_loss: 0.2733\n",
            "Epoch 864/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1756 - val_loss: 0.2737\n",
            "Epoch 865/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1756 - val_loss: 0.2733\n",
            "Epoch 866/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1754 - val_loss: 0.2733\n",
            "Epoch 867/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1755 - val_loss: 0.2733\n",
            "Epoch 868/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1754 - val_loss: 0.2734\n",
            "Epoch 869/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1755 - val_loss: 0.2737\n",
            "Epoch 870/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1755 - val_loss: 0.2733\n",
            "Epoch 871/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1754 - val_loss: 0.2735\n",
            "Epoch 872/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1754 - val_loss: 0.2740\n",
            "Epoch 873/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1753 - val_loss: 0.2736\n",
            "Epoch 874/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1751 - val_loss: 0.2733\n",
            "Epoch 875/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1751 - val_loss: 0.2734\n",
            "Epoch 876/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1752 - val_loss: 0.2734\n",
            "Epoch 877/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1751 - val_loss: 0.2735\n",
            "Epoch 878/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1751 - val_loss: 0.2736\n",
            "Epoch 879/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1751 - val_loss: 0.2734\n",
            "Epoch 880/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1750 - val_loss: 0.2735\n",
            "Epoch 881/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1749 - val_loss: 0.2735\n",
            "Epoch 882/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1748 - val_loss: 0.2734\n",
            "Epoch 883/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1749 - val_loss: 0.2736\n",
            "Epoch 884/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1748 - val_loss: 0.2735\n",
            "Epoch 885/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1748 - val_loss: 0.2739\n",
            "Epoch 886/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1749 - val_loss: 0.2737\n",
            "Epoch 887/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1748 - val_loss: 0.2736\n",
            "Epoch 888/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1746 - val_loss: 0.2735\n",
            "Epoch 889/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1746 - val_loss: 0.2736\n",
            "Epoch 890/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1747 - val_loss: 0.2737\n",
            "Epoch 891/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1745 - val_loss: 0.2737\n",
            "Epoch 892/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1744 - val_loss: 0.2738\n",
            "Epoch 893/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1745 - val_loss: 0.2739\n",
            "Epoch 894/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1743 - val_loss: 0.2737\n",
            "Epoch 895/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1743 - val_loss: 0.2736\n",
            "Epoch 896/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1743 - val_loss: 0.2736\n",
            "Epoch 897/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1743 - val_loss: 0.2736\n",
            "Epoch 898/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1742 - val_loss: 0.2736\n",
            "Epoch 899/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1742 - val_loss: 0.2740\n",
            "Epoch 900/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1742 - val_loss: 0.2739\n",
            "Epoch 901/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1742 - val_loss: 0.2737\n",
            "Epoch 902/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1740 - val_loss: 0.2737\n",
            "Epoch 903/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1740 - val_loss: 0.2743\n",
            "Epoch 904/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1740 - val_loss: 0.2737\n",
            "Epoch 905/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1739 - val_loss: 0.2737\n",
            "Epoch 906/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1738 - val_loss: 0.2737\n",
            "Epoch 907/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1738 - val_loss: 0.2739\n",
            "Epoch 908/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1739 - val_loss: 0.2738\n",
            "Epoch 909/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1737 - val_loss: 0.2739\n",
            "Epoch 910/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1739 - val_loss: 0.2741\n",
            "Epoch 911/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1737 - val_loss: 0.2739\n",
            "Epoch 912/1000\n",
            "34/34 [==============================] - 1s 26ms/step - loss: 0.1736 - val_loss: 0.2738\n",
            "Epoch 913/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1736 - val_loss: 0.2738\n",
            "Epoch 914/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1736 - val_loss: 0.2746\n",
            "Epoch 915/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1735 - val_loss: 0.2738\n",
            "Epoch 916/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1735 - val_loss: 0.2741\n",
            "Epoch 917/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1735 - val_loss: 0.2740\n",
            "Epoch 918/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1735 - val_loss: 0.2740\n",
            "Epoch 919/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1735 - val_loss: 0.2739\n",
            "Epoch 920/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1733 - val_loss: 0.2739\n",
            "Epoch 921/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1735 - val_loss: 0.2739\n",
            "Epoch 922/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1733 - val_loss: 0.2740\n",
            "Epoch 923/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1732 - val_loss: 0.2740\n",
            "Epoch 924/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1732 - val_loss: 0.2739\n",
            "Epoch 925/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1731 - val_loss: 0.2743\n",
            "Epoch 926/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1732 - val_loss: 0.2740\n",
            "Epoch 927/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1730 - val_loss: 0.2741\n",
            "Epoch 928/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1730 - val_loss: 0.2739\n",
            "Epoch 929/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1731 - val_loss: 0.2740\n",
            "Epoch 930/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1730 - val_loss: 0.2740\n",
            "Epoch 931/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1729 - val_loss: 0.2741\n",
            "Epoch 932/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1729 - val_loss: 0.2740\n",
            "Epoch 933/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1728 - val_loss: 0.2744\n",
            "Epoch 934/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1728 - val_loss: 0.2742\n",
            "Epoch 935/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1727 - val_loss: 0.2741\n",
            "Epoch 936/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1728 - val_loss: 0.2741\n",
            "Epoch 937/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1727 - val_loss: 0.2742\n",
            "Epoch 938/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1727 - val_loss: 0.2741\n",
            "Epoch 939/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1726 - val_loss: 0.2744\n",
            "Epoch 940/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1726 - val_loss: 0.2742\n",
            "Epoch 941/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1725 - val_loss: 0.2741\n",
            "Epoch 942/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1724 - val_loss: 0.2741\n",
            "Epoch 943/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1724 - val_loss: 0.2741\n",
            "Epoch 944/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1724 - val_loss: 0.2742\n",
            "Epoch 945/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1725 - val_loss: 0.2743\n",
            "Epoch 946/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1724 - val_loss: 0.2742\n",
            "Epoch 947/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1724 - val_loss: 0.2743\n",
            "Epoch 948/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1723 - val_loss: 0.2743\n",
            "Epoch 949/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1723 - val_loss: 0.2744\n",
            "Epoch 950/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1723 - val_loss: 0.2743\n",
            "Epoch 951/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1721 - val_loss: 0.2743\n",
            "Epoch 952/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1721 - val_loss: 0.2747\n",
            "Epoch 953/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1721 - val_loss: 0.2743\n",
            "Epoch 954/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1720 - val_loss: 0.2743\n",
            "Epoch 955/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1720 - val_loss: 0.2743\n",
            "Epoch 956/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1721 - val_loss: 0.2743\n",
            "Epoch 957/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1719 - val_loss: 0.2743\n",
            "Epoch 958/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1718 - val_loss: 0.2744\n",
            "Epoch 959/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1719 - val_loss: 0.2743\n",
            "Epoch 960/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1720 - val_loss: 0.2743\n",
            "Epoch 961/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1718 - val_loss: 0.2745\n",
            "Epoch 962/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1717 - val_loss: 0.2745\n",
            "Epoch 963/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1719 - val_loss: 0.2744\n",
            "Epoch 964/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1718 - val_loss: 0.2745\n",
            "Epoch 965/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1718 - val_loss: 0.2745\n",
            "Epoch 966/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1718 - val_loss: 0.2744\n",
            "Epoch 967/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1715 - val_loss: 0.2745\n",
            "Epoch 968/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1716 - val_loss: 0.2745\n",
            "Epoch 969/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1715 - val_loss: 0.2750\n",
            "Epoch 970/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1715 - val_loss: 0.2744\n",
            "Epoch 971/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1715 - val_loss: 0.2745\n",
            "Epoch 972/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1714 - val_loss: 0.2745\n",
            "Epoch 973/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1713 - val_loss: 0.2745\n",
            "Epoch 974/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1713 - val_loss: 0.2746\n",
            "Epoch 975/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1714 - val_loss: 0.2746\n",
            "Epoch 976/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1714 - val_loss: 0.2745\n",
            "Epoch 977/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1712 - val_loss: 0.2747\n",
            "Epoch 978/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1712 - val_loss: 0.2750\n",
            "Epoch 979/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1713 - val_loss: 0.2747\n",
            "Epoch 980/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1711 - val_loss: 0.2746\n",
            "Epoch 981/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1712 - val_loss: 0.2748\n",
            "Epoch 982/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1713 - val_loss: 0.2746\n",
            "Epoch 983/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1710 - val_loss: 0.2745\n",
            "Epoch 984/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1709 - val_loss: 0.2748\n",
            "Epoch 985/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1710 - val_loss: 0.2747\n",
            "Epoch 986/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1709 - val_loss: 0.2746\n",
            "Epoch 987/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1709 - val_loss: 0.2748\n",
            "Epoch 988/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1709 - val_loss: 0.2749\n",
            "Epoch 989/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1708 - val_loss: 0.2748\n",
            "Epoch 990/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1709 - val_loss: 0.2747\n",
            "Epoch 991/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1708 - val_loss: 0.2748\n",
            "Epoch 992/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1708 - val_loss: 0.2748\n",
            "Epoch 993/1000\n",
            "34/34 [==============================] - 1s 26ms/step - loss: 0.1707 - val_loss: 0.2748\n",
            "Epoch 994/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1707 - val_loss: 0.2747\n",
            "Epoch 995/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1705 - val_loss: 0.2748\n",
            "Epoch 996/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1707 - val_loss: 0.2749\n",
            "Epoch 997/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1705 - val_loss: 0.2748\n",
            "Epoch 998/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1705 - val_loss: 0.2748\n",
            "Epoch 999/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1706 - val_loss: 0.2750\n",
            "Epoch 1000/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1705 - val_loss: 0.2749\n",
            "3/3 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0.034874942, 0.2212694204456747, 3.6797975968037435), (0.032329243, 0.23580026189427816, 3.7484075950663445), (0.030779244, 0.2276339840277261, 3.6787310024161917), (0.033040255, 0.23473775004729208, 3.7397849988172176)]\n",
            "Epoch 1/1000\n",
            "34/34 [==============================] - 2s 32ms/step - loss: 0.6265 - val_loss: 0.5658\n",
            "Epoch 2/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.4547 - val_loss: 0.5353\n",
            "Epoch 3/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.4155 - val_loss: 0.4999\n",
            "Epoch 4/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3956 - val_loss: 0.4729\n",
            "Epoch 5/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3825 - val_loss: 0.4509\n",
            "Epoch 6/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3713 - val_loss: 0.4316\n",
            "Epoch 7/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3613 - val_loss: 0.4152\n",
            "Epoch 8/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.3527 - val_loss: 0.4015\n",
            "Epoch 9/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3452 - val_loss: 0.3905\n",
            "Epoch 10/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3386 - val_loss: 0.3816\n",
            "Epoch 11/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.3328 - val_loss: 0.3741\n",
            "Epoch 12/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3275 - val_loss: 0.3678\n",
            "Epoch 13/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3225 - val_loss: 0.3620\n",
            "Epoch 14/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3181 - val_loss: 0.3570\n",
            "Epoch 15/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3139 - val_loss: 0.3523\n",
            "Epoch 16/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3102 - val_loss: 0.3483\n",
            "Epoch 17/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3066 - val_loss: 0.3445\n",
            "Epoch 18/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.3035 - val_loss: 0.3410\n",
            "Epoch 19/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.3006 - val_loss: 0.3379\n",
            "Epoch 20/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2978 - val_loss: 0.3350\n",
            "Epoch 21/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2954 - val_loss: 0.3322\n",
            "Epoch 22/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2930 - val_loss: 0.3297\n",
            "Epoch 23/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2908 - val_loss: 0.3273\n",
            "Epoch 24/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2888 - val_loss: 0.3251\n",
            "Epoch 25/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2868 - val_loss: 0.3230\n",
            "Epoch 26/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2850 - val_loss: 0.3210\n",
            "Epoch 27/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2832 - val_loss: 0.3192\n",
            "Epoch 28/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2816 - val_loss: 0.3173\n",
            "Epoch 29/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2801 - val_loss: 0.3156\n",
            "Epoch 30/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2785 - val_loss: 0.3140\n",
            "Epoch 31/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2772 - val_loss: 0.3124\n",
            "Epoch 32/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2758 - val_loss: 0.3110\n",
            "Epoch 33/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2746 - val_loss: 0.3096\n",
            "Epoch 34/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2733 - val_loss: 0.3082\n",
            "Epoch 35/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2722 - val_loss: 0.3069\n",
            "Epoch 36/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2710 - val_loss: 0.3058\n",
            "Epoch 37/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2701 - val_loss: 0.3046\n",
            "Epoch 38/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2691 - val_loss: 0.3036\n",
            "Epoch 39/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2682 - val_loss: 0.3025\n",
            "Epoch 40/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2673 - val_loss: 0.3016\n",
            "Epoch 41/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2665 - val_loss: 0.3006\n",
            "Epoch 42/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2658 - val_loss: 0.2998\n",
            "Epoch 43/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2650 - val_loss: 0.2990\n",
            "Epoch 44/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2643 - val_loss: 0.2981\n",
            "Epoch 45/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2637 - val_loss: 0.2974\n",
            "Epoch 46/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2630 - val_loss: 0.2967\n",
            "Epoch 47/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2624 - val_loss: 0.2961\n",
            "Epoch 48/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2619 - val_loss: 0.2954\n",
            "Epoch 49/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2614 - val_loss: 0.2948\n",
            "Epoch 50/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2608 - val_loss: 0.2943\n",
            "Epoch 51/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2604 - val_loss: 0.2938\n",
            "Epoch 52/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2599 - val_loss: 0.2932\n",
            "Epoch 53/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2596 - val_loss: 0.2928\n",
            "Epoch 54/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2591 - val_loss: 0.2923\n",
            "Epoch 55/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2588 - val_loss: 0.2919\n",
            "Epoch 56/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2584 - val_loss: 0.2915\n",
            "Epoch 57/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2581 - val_loss: 0.2912\n",
            "Epoch 58/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2578 - val_loss: 0.2908\n",
            "Epoch 59/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2575 - val_loss: 0.2904\n",
            "Epoch 60/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2572 - val_loss: 0.2901\n",
            "Epoch 61/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2569 - val_loss: 0.2898\n",
            "Epoch 62/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2567 - val_loss: 0.2895\n",
            "Epoch 63/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2565 - val_loss: 0.2893\n",
            "Epoch 64/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2563 - val_loss: 0.2890\n",
            "Epoch 65/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2561 - val_loss: 0.2888\n",
            "Epoch 66/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2558 - val_loss: 0.2886\n",
            "Epoch 67/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2556 - val_loss: 0.2883\n",
            "Epoch 68/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2555 - val_loss: 0.2881\n",
            "Epoch 69/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2553 - val_loss: 0.2879\n",
            "Epoch 70/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2551 - val_loss: 0.2878\n",
            "Epoch 71/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2549 - val_loss: 0.2876\n",
            "Epoch 72/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2549 - val_loss: 0.2874\n",
            "Epoch 73/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2547 - val_loss: 0.2872\n",
            "Epoch 74/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2545 - val_loss: 0.2871\n",
            "Epoch 75/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2543 - val_loss: 0.2870\n",
            "Epoch 76/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2543 - val_loss: 0.2868\n",
            "Epoch 77/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2542 - val_loss: 0.2867\n",
            "Epoch 78/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2540 - val_loss: 0.2866\n",
            "Epoch 79/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2539 - val_loss: 0.2864\n",
            "Epoch 80/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2538 - val_loss: 0.2863\n",
            "Epoch 81/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2538 - val_loss: 0.2862\n",
            "Epoch 82/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2536 - val_loss: 0.2861\n",
            "Epoch 83/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2535 - val_loss: 0.2860\n",
            "Epoch 84/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2534 - val_loss: 0.2859\n",
            "Epoch 85/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2533 - val_loss: 0.2858\n",
            "Epoch 86/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2532 - val_loss: 0.2857\n",
            "Epoch 87/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2531 - val_loss: 0.2856\n",
            "Epoch 88/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2531 - val_loss: 0.2855\n",
            "Epoch 89/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2529 - val_loss: 0.2854\n",
            "Epoch 90/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2529 - val_loss: 0.2853\n",
            "Epoch 91/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2528 - val_loss: 0.2852\n",
            "Epoch 92/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2527 - val_loss: 0.2851\n",
            "Epoch 93/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2526 - val_loss: 0.2851\n",
            "Epoch 94/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2525 - val_loss: 0.2850\n",
            "Epoch 95/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2524 - val_loss: 0.2849\n",
            "Epoch 96/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2524 - val_loss: 0.2849\n",
            "Epoch 97/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2523 - val_loss: 0.2848\n",
            "Epoch 98/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2522 - val_loss: 0.2847\n",
            "Epoch 99/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2521 - val_loss: 0.2846\n",
            "Epoch 100/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2520 - val_loss: 0.2846\n",
            "Epoch 101/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2520 - val_loss: 0.2845\n",
            "Epoch 102/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2520 - val_loss: 0.2845\n",
            "Epoch 103/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2518 - val_loss: 0.2844\n",
            "Epoch 104/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2518 - val_loss: 0.2843\n",
            "Epoch 105/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2517 - val_loss: 0.2843\n",
            "Epoch 106/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2516 - val_loss: 0.2842\n",
            "Epoch 107/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2515 - val_loss: 0.2842\n",
            "Epoch 108/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2515 - val_loss: 0.2841\n",
            "Epoch 109/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2515 - val_loss: 0.2840\n",
            "Epoch 110/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2513 - val_loss: 0.2840\n",
            "Epoch 111/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2512 - val_loss: 0.2839\n",
            "Epoch 112/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2511 - val_loss: 0.2839\n",
            "Epoch 113/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2513 - val_loss: 0.2838\n",
            "Epoch 114/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2510 - val_loss: 0.2838\n",
            "Epoch 115/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2511 - val_loss: 0.2837\n",
            "Epoch 116/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2509 - val_loss: 0.2837\n",
            "Epoch 117/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2509 - val_loss: 0.2836\n",
            "Epoch 118/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2507 - val_loss: 0.2836\n",
            "Epoch 119/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2507 - val_loss: 0.2835\n",
            "Epoch 120/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2506 - val_loss: 0.2835\n",
            "Epoch 121/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2506 - val_loss: 0.2834\n",
            "Epoch 122/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2505 - val_loss: 0.2834\n",
            "Epoch 123/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2504 - val_loss: 0.2833\n",
            "Epoch 124/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2503 - val_loss: 0.2833\n",
            "Epoch 125/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2502 - val_loss: 0.2833\n",
            "Epoch 126/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2502 - val_loss: 0.2832\n",
            "Epoch 127/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2502 - val_loss: 0.2831\n",
            "Epoch 128/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2500 - val_loss: 0.2831\n",
            "Epoch 129/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2499 - val_loss: 0.2831\n",
            "Epoch 130/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2499 - val_loss: 0.2831\n",
            "Epoch 131/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2499 - val_loss: 0.2830\n",
            "Epoch 132/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2498 - val_loss: 0.2829\n",
            "Epoch 133/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2496 - val_loss: 0.2829\n",
            "Epoch 134/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2496 - val_loss: 0.2828\n",
            "Epoch 135/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2496 - val_loss: 0.2828\n",
            "Epoch 136/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2494 - val_loss: 0.2827\n",
            "Epoch 137/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2493 - val_loss: 0.2827\n",
            "Epoch 138/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2493 - val_loss: 0.2826\n",
            "Epoch 139/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2491 - val_loss: 0.2826\n",
            "Epoch 140/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2492 - val_loss: 0.2826\n",
            "Epoch 141/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2490 - val_loss: 0.2825\n",
            "Epoch 142/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2490 - val_loss: 0.2825\n",
            "Epoch 143/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2489 - val_loss: 0.2825\n",
            "Epoch 144/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2488 - val_loss: 0.2824\n",
            "Epoch 145/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2487 - val_loss: 0.2824\n",
            "Epoch 146/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2486 - val_loss: 0.2823\n",
            "Epoch 147/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2485 - val_loss: 0.2823\n",
            "Epoch 148/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2485 - val_loss: 0.2822\n",
            "Epoch 149/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2484 - val_loss: 0.2822\n",
            "Epoch 150/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2482 - val_loss: 0.2822\n",
            "Epoch 151/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2481 - val_loss: 0.2821\n",
            "Epoch 152/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2481 - val_loss: 0.2821\n",
            "Epoch 153/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2480 - val_loss: 0.2821\n",
            "Epoch 154/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2479 - val_loss: 0.2820\n",
            "Epoch 155/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2478 - val_loss: 0.2819\n",
            "Epoch 156/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2477 - val_loss: 0.2819\n",
            "Epoch 157/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2476 - val_loss: 0.2819\n",
            "Epoch 158/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2475 - val_loss: 0.2818\n",
            "Epoch 159/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2475 - val_loss: 0.2818\n",
            "Epoch 160/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2474 - val_loss: 0.2817\n",
            "Epoch 161/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2473 - val_loss: 0.2817\n",
            "Epoch 162/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2472 - val_loss: 0.2816\n",
            "Epoch 163/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2471 - val_loss: 0.2816\n",
            "Epoch 164/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2469 - val_loss: 0.2816\n",
            "Epoch 165/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2468 - val_loss: 0.2815\n",
            "Epoch 166/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2467 - val_loss: 0.2815\n",
            "Epoch 167/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2467 - val_loss: 0.2814\n",
            "Epoch 168/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2465 - val_loss: 0.2814\n",
            "Epoch 169/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2465 - val_loss: 0.2814\n",
            "Epoch 170/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2463 - val_loss: 0.2813\n",
            "Epoch 171/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2462 - val_loss: 0.2813\n",
            "Epoch 172/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2461 - val_loss: 0.2812\n",
            "Epoch 173/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2460 - val_loss: 0.2812\n",
            "Epoch 174/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2459 - val_loss: 0.2811\n",
            "Epoch 175/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2458 - val_loss: 0.2811\n",
            "Epoch 176/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2457 - val_loss: 0.2811\n",
            "Epoch 177/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2456 - val_loss: 0.2810\n",
            "Epoch 178/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2455 - val_loss: 0.2810\n",
            "Epoch 179/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2454 - val_loss: 0.2809\n",
            "Epoch 180/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2453 - val_loss: 0.2809\n",
            "Epoch 181/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2452 - val_loss: 0.2808\n",
            "Epoch 182/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2450 - val_loss: 0.2808\n",
            "Epoch 183/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2449 - val_loss: 0.2808\n",
            "Epoch 184/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2448 - val_loss: 0.2807\n",
            "Epoch 185/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2447 - val_loss: 0.2807\n",
            "Epoch 186/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2446 - val_loss: 0.2806\n",
            "Epoch 187/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2445 - val_loss: 0.2806\n",
            "Epoch 188/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2444 - val_loss: 0.2805\n",
            "Epoch 189/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2442 - val_loss: 0.2805\n",
            "Epoch 190/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2441 - val_loss: 0.2805\n",
            "Epoch 191/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2441 - val_loss: 0.2804\n",
            "Epoch 192/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2438 - val_loss: 0.2804\n",
            "Epoch 193/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2438 - val_loss: 0.2803\n",
            "Epoch 194/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2436 - val_loss: 0.2803\n",
            "Epoch 195/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2435 - val_loss: 0.2802\n",
            "Epoch 196/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2434 - val_loss: 0.2802\n",
            "Epoch 197/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2433 - val_loss: 0.2801\n",
            "Epoch 198/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2433 - val_loss: 0.2801\n",
            "Epoch 199/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2430 - val_loss: 0.2801\n",
            "Epoch 200/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2429 - val_loss: 0.2800\n",
            "Epoch 201/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2428 - val_loss: 0.2800\n",
            "Epoch 202/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2427 - val_loss: 0.2799\n",
            "Epoch 203/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2426 - val_loss: 0.2799\n",
            "Epoch 204/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2425 - val_loss: 0.2798\n",
            "Epoch 205/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2424 - val_loss: 0.2798\n",
            "Epoch 206/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2422 - val_loss: 0.2797\n",
            "Epoch 207/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2421 - val_loss: 0.2797\n",
            "Epoch 208/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2421 - val_loss: 0.2797\n",
            "Epoch 209/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2417 - val_loss: 0.2796\n",
            "Epoch 210/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2417 - val_loss: 0.2796\n",
            "Epoch 211/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2416 - val_loss: 0.2795\n",
            "Epoch 212/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2414 - val_loss: 0.2795\n",
            "Epoch 213/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2413 - val_loss: 0.2794\n",
            "Epoch 214/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2412 - val_loss: 0.2794\n",
            "Epoch 215/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2410 - val_loss: 0.2793\n",
            "Epoch 216/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2409 - val_loss: 0.2793\n",
            "Epoch 217/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2408 - val_loss: 0.2793\n",
            "Epoch 218/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2407 - val_loss: 0.2792\n",
            "Epoch 219/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2405 - val_loss: 0.2792\n",
            "Epoch 220/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2404 - val_loss: 0.2791\n",
            "Epoch 221/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2403 - val_loss: 0.2791\n",
            "Epoch 222/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2402 - val_loss: 0.2790\n",
            "Epoch 223/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2401 - val_loss: 0.2790\n",
            "Epoch 224/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2400 - val_loss: 0.2790\n",
            "Epoch 225/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2397 - val_loss: 0.2789\n",
            "Epoch 226/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2397 - val_loss: 0.2789\n",
            "Epoch 227/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2396 - val_loss: 0.2788\n",
            "Epoch 228/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2393 - val_loss: 0.2788\n",
            "Epoch 229/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2393 - val_loss: 0.2787\n",
            "Epoch 230/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2390 - val_loss: 0.2787\n",
            "Epoch 231/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2389 - val_loss: 0.2786\n",
            "Epoch 232/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2388 - val_loss: 0.2786\n",
            "Epoch 233/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2386 - val_loss: 0.2785\n",
            "Epoch 234/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2385 - val_loss: 0.2785\n",
            "Epoch 235/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2384 - val_loss: 0.2784\n",
            "Epoch 236/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2383 - val_loss: 0.2784\n",
            "Epoch 237/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2381 - val_loss: 0.2784\n",
            "Epoch 238/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2380 - val_loss: 0.2783\n",
            "Epoch 239/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2378 - val_loss: 0.2783\n",
            "Epoch 240/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2377 - val_loss: 0.2782\n",
            "Epoch 241/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2375 - val_loss: 0.2782\n",
            "Epoch 242/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2374 - val_loss: 0.2781\n",
            "Epoch 243/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2374 - val_loss: 0.2781\n",
            "Epoch 244/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2372 - val_loss: 0.2780\n",
            "Epoch 245/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2371 - val_loss: 0.2780\n",
            "Epoch 246/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2369 - val_loss: 0.2780\n",
            "Epoch 247/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2367 - val_loss: 0.2779\n",
            "Epoch 248/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2366 - val_loss: 0.2778\n",
            "Epoch 249/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2365 - val_loss: 0.2778\n",
            "Epoch 250/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2364 - val_loss: 0.2778\n",
            "Epoch 251/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2362 - val_loss: 0.2777\n",
            "Epoch 252/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2361 - val_loss: 0.2777\n",
            "Epoch 253/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2359 - val_loss: 0.2777\n",
            "Epoch 254/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2358 - val_loss: 0.2776\n",
            "Epoch 255/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2356 - val_loss: 0.2776\n",
            "Epoch 256/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2355 - val_loss: 0.2775\n",
            "Epoch 257/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2354 - val_loss: 0.2775\n",
            "Epoch 258/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2352 - val_loss: 0.2774\n",
            "Epoch 259/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2352 - val_loss: 0.2774\n",
            "Epoch 260/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2349 - val_loss: 0.2773\n",
            "Epoch 261/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2348 - val_loss: 0.2773\n",
            "Epoch 262/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2347 - val_loss: 0.2773\n",
            "Epoch 263/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2345 - val_loss: 0.2772\n",
            "Epoch 264/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2345 - val_loss: 0.2772\n",
            "Epoch 265/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2343 - val_loss: 0.2771\n",
            "Epoch 266/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2341 - val_loss: 0.2771\n",
            "Epoch 267/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2340 - val_loss: 0.2771\n",
            "Epoch 268/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2339 - val_loss: 0.2770\n",
            "Epoch 269/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2337 - val_loss: 0.2770\n",
            "Epoch 270/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2336 - val_loss: 0.2769\n",
            "Epoch 271/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2335 - val_loss: 0.2769\n",
            "Epoch 272/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2333 - val_loss: 0.2768\n",
            "Epoch 273/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2331 - val_loss: 0.2768\n",
            "Epoch 274/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2331 - val_loss: 0.2767\n",
            "Epoch 275/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2329 - val_loss: 0.2767\n",
            "Epoch 276/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2327 - val_loss: 0.2767\n",
            "Epoch 277/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2327 - val_loss: 0.2766\n",
            "Epoch 278/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2326 - val_loss: 0.2766\n",
            "Epoch 279/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2323 - val_loss: 0.2765\n",
            "Epoch 280/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2322 - val_loss: 0.2765\n",
            "Epoch 281/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2320 - val_loss: 0.2765\n",
            "Epoch 282/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2319 - val_loss: 0.2764\n",
            "Epoch 283/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2318 - val_loss: 0.2764\n",
            "Epoch 284/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2316 - val_loss: 0.2763\n",
            "Epoch 285/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2316 - val_loss: 0.2763\n",
            "Epoch 286/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2314 - val_loss: 0.2762\n",
            "Epoch 287/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2312 - val_loss: 0.2762\n",
            "Epoch 288/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2311 - val_loss: 0.2761\n",
            "Epoch 289/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2309 - val_loss: 0.2761\n",
            "Epoch 290/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2308 - val_loss: 0.2761\n",
            "Epoch 291/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2307 - val_loss: 0.2760\n",
            "Epoch 292/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2306 - val_loss: 0.2760\n",
            "Epoch 293/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2304 - val_loss: 0.2760\n",
            "Epoch 294/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2302 - val_loss: 0.2759\n",
            "Epoch 295/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2302 - val_loss: 0.2759\n",
            "Epoch 296/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2300 - val_loss: 0.2758\n",
            "Epoch 297/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2299 - val_loss: 0.2758\n",
            "Epoch 298/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2297 - val_loss: 0.2758\n",
            "Epoch 299/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2296 - val_loss: 0.2757\n",
            "Epoch 300/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2294 - val_loss: 0.2757\n",
            "Epoch 301/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2292 - val_loss: 0.2756\n",
            "Epoch 302/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2293 - val_loss: 0.2756\n",
            "Epoch 303/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2291 - val_loss: 0.2755\n",
            "Epoch 304/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2289 - val_loss: 0.2755\n",
            "Epoch 305/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2288 - val_loss: 0.2755\n",
            "Epoch 306/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2286 - val_loss: 0.2755\n",
            "Epoch 307/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2285 - val_loss: 0.2754\n",
            "Epoch 308/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2284 - val_loss: 0.2753\n",
            "Epoch 309/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2283 - val_loss: 0.2754\n",
            "Epoch 310/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2281 - val_loss: 0.2753\n",
            "Epoch 311/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2280 - val_loss: 0.2752\n",
            "Epoch 312/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2279 - val_loss: 0.2752\n",
            "Epoch 313/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2276 - val_loss: 0.2752\n",
            "Epoch 314/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2275 - val_loss: 0.2751\n",
            "Epoch 315/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2273 - val_loss: 0.2751\n",
            "Epoch 316/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2273 - val_loss: 0.2750\n",
            "Epoch 317/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2272 - val_loss: 0.2750\n",
            "Epoch 318/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2270 - val_loss: 0.2750\n",
            "Epoch 319/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2270 - val_loss: 0.2750\n",
            "Epoch 320/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2268 - val_loss: 0.2749\n",
            "Epoch 321/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2266 - val_loss: 0.2749\n",
            "Epoch 322/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2265 - val_loss: 0.2748\n",
            "Epoch 323/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2264 - val_loss: 0.2748\n",
            "Epoch 324/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2263 - val_loss: 0.2748\n",
            "Epoch 325/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2261 - val_loss: 0.2748\n",
            "Epoch 326/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2259 - val_loss: 0.2747\n",
            "Epoch 327/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2259 - val_loss: 0.2747\n",
            "Epoch 328/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2257 - val_loss: 0.2747\n",
            "Epoch 329/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2255 - val_loss: 0.2747\n",
            "Epoch 330/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2255 - val_loss: 0.2746\n",
            "Epoch 331/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2253 - val_loss: 0.2746\n",
            "Epoch 332/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2252 - val_loss: 0.2745\n",
            "Epoch 333/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2250 - val_loss: 0.2745\n",
            "Epoch 334/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2249 - val_loss: 0.2744\n",
            "Epoch 335/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2248 - val_loss: 0.2744\n",
            "Epoch 336/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2247 - val_loss: 0.2744\n",
            "Epoch 337/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2245 - val_loss: 0.2743\n",
            "Epoch 338/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2244 - val_loss: 0.2744\n",
            "Epoch 339/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2243 - val_loss: 0.2743\n",
            "Epoch 340/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2241 - val_loss: 0.2742\n",
            "Epoch 341/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2240 - val_loss: 0.2742\n",
            "Epoch 342/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2239 - val_loss: 0.2742\n",
            "Epoch 343/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2238 - val_loss: 0.2741\n",
            "Epoch 344/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2237 - val_loss: 0.2741\n",
            "Epoch 345/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2235 - val_loss: 0.2741\n",
            "Epoch 346/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2234 - val_loss: 0.2742\n",
            "Epoch 347/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2232 - val_loss: 0.2740\n",
            "Epoch 348/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2231 - val_loss: 0.2740\n",
            "Epoch 349/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2230 - val_loss: 0.2739\n",
            "Epoch 350/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2228 - val_loss: 0.2739\n",
            "Epoch 351/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2228 - val_loss: 0.2739\n",
            "Epoch 352/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2226 - val_loss: 0.2738\n",
            "Epoch 353/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2225 - val_loss: 0.2738\n",
            "Epoch 354/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2224 - val_loss: 0.2738\n",
            "Epoch 355/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2223 - val_loss: 0.2738\n",
            "Epoch 356/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2221 - val_loss: 0.2737\n",
            "Epoch 357/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2219 - val_loss: 0.2737\n",
            "Epoch 358/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2219 - val_loss: 0.2737\n",
            "Epoch 359/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2218 - val_loss: 0.2736\n",
            "Epoch 360/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2216 - val_loss: 0.2736\n",
            "Epoch 361/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2215 - val_loss: 0.2736\n",
            "Epoch 362/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2213 - val_loss: 0.2735\n",
            "Epoch 363/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2213 - val_loss: 0.2735\n",
            "Epoch 364/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2212 - val_loss: 0.2735\n",
            "Epoch 365/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2209 - val_loss: 0.2734\n",
            "Epoch 366/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2208 - val_loss: 0.2734\n",
            "Epoch 367/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2208 - val_loss: 0.2734\n",
            "Epoch 368/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2207 - val_loss: 0.2737\n",
            "Epoch 369/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2205 - val_loss: 0.2734\n",
            "Epoch 370/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2204 - val_loss: 0.2733\n",
            "Epoch 371/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2202 - val_loss: 0.2733\n",
            "Epoch 372/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2201 - val_loss: 0.2733\n",
            "Epoch 373/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2201 - val_loss: 0.2733\n",
            "Epoch 374/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2199 - val_loss: 0.2732\n",
            "Epoch 375/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2197 - val_loss: 0.2732\n",
            "Epoch 376/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2197 - val_loss: 0.2732\n",
            "Epoch 377/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2195 - val_loss: 0.2731\n",
            "Epoch 378/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2194 - val_loss: 0.2731\n",
            "Epoch 379/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2192 - val_loss: 0.2731\n",
            "Epoch 380/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2191 - val_loss: 0.2730\n",
            "Epoch 381/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2190 - val_loss: 0.2730\n",
            "Epoch 382/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2189 - val_loss: 0.2730\n",
            "Epoch 383/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2188 - val_loss: 0.2730\n",
            "Epoch 384/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2187 - val_loss: 0.2730\n",
            "Epoch 385/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2186 - val_loss: 0.2729\n",
            "Epoch 386/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2184 - val_loss: 0.2729\n",
            "Epoch 387/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2183 - val_loss: 0.2728\n",
            "Epoch 388/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2182 - val_loss: 0.2729\n",
            "Epoch 389/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2180 - val_loss: 0.2729\n",
            "Epoch 390/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2180 - val_loss: 0.2728\n",
            "Epoch 391/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2178 - val_loss: 0.2728\n",
            "Epoch 392/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2177 - val_loss: 0.2727\n",
            "Epoch 393/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2177 - val_loss: 0.2728\n",
            "Epoch 394/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2174 - val_loss: 0.2727\n",
            "Epoch 395/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2174 - val_loss: 0.2727\n",
            "Epoch 396/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2172 - val_loss: 0.2727\n",
            "Epoch 397/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2171 - val_loss: 0.2726\n",
            "Epoch 398/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2171 - val_loss: 0.2726\n",
            "Epoch 399/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2169 - val_loss: 0.2726\n",
            "Epoch 400/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2168 - val_loss: 0.2725\n",
            "Epoch 401/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2168 - val_loss: 0.2726\n",
            "Epoch 402/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2165 - val_loss: 0.2725\n",
            "Epoch 403/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2165 - val_loss: 0.2725\n",
            "Epoch 404/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2164 - val_loss: 0.2724\n",
            "Epoch 405/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2162 - val_loss: 0.2725\n",
            "Epoch 406/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2161 - val_loss: 0.2724\n",
            "Epoch 407/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2160 - val_loss: 0.2724\n",
            "Epoch 408/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2159 - val_loss: 0.2723\n",
            "Epoch 409/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2158 - val_loss: 0.2723\n",
            "Epoch 410/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2157 - val_loss: 0.2724\n",
            "Epoch 411/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2156 - val_loss: 0.2724\n",
            "Epoch 412/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2155 - val_loss: 0.2723\n",
            "Epoch 413/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2154 - val_loss: 0.2723\n",
            "Epoch 414/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2152 - val_loss: 0.2722\n",
            "Epoch 415/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2151 - val_loss: 0.2722\n",
            "Epoch 416/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2150 - val_loss: 0.2722\n",
            "Epoch 417/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2149 - val_loss: 0.2722\n",
            "Epoch 418/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2148 - val_loss: 0.2721\n",
            "Epoch 419/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2147 - val_loss: 0.2721\n",
            "Epoch 420/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2146 - val_loss: 0.2721\n",
            "Epoch 421/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2145 - val_loss: 0.2721\n",
            "Epoch 422/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2143 - val_loss: 0.2721\n",
            "Epoch 423/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2143 - val_loss: 0.2721\n",
            "Epoch 424/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2142 - val_loss: 0.2722\n",
            "Epoch 425/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2140 - val_loss: 0.2720\n",
            "Epoch 426/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2139 - val_loss: 0.2720\n",
            "Epoch 427/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2138 - val_loss: 0.2720\n",
            "Epoch 428/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2137 - val_loss: 0.2719\n",
            "Epoch 429/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2136 - val_loss: 0.2719\n",
            "Epoch 430/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2135 - val_loss: 0.2719\n",
            "Epoch 431/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2134 - val_loss: 0.2719\n",
            "Epoch 432/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2132 - val_loss: 0.2719\n",
            "Epoch 433/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2131 - val_loss: 0.2719\n",
            "Epoch 434/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2130 - val_loss: 0.2718\n",
            "Epoch 435/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2130 - val_loss: 0.2719\n",
            "Epoch 436/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2128 - val_loss: 0.2718\n",
            "Epoch 437/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2128 - val_loss: 0.2719\n",
            "Epoch 438/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2126 - val_loss: 0.2718\n",
            "Epoch 439/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2124 - val_loss: 0.2718\n",
            "Epoch 440/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2124 - val_loss: 0.2717\n",
            "Epoch 441/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2123 - val_loss: 0.2717\n",
            "Epoch 442/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2122 - val_loss: 0.2717\n",
            "Epoch 443/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2121 - val_loss: 0.2717\n",
            "Epoch 444/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2120 - val_loss: 0.2717\n",
            "Epoch 445/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2119 - val_loss: 0.2717\n",
            "Epoch 446/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2118 - val_loss: 0.2717\n",
            "Epoch 447/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2117 - val_loss: 0.2716\n",
            "Epoch 448/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2116 - val_loss: 0.2716\n",
            "Epoch 449/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2115 - val_loss: 0.2716\n",
            "Epoch 450/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2115 - val_loss: 0.2716\n",
            "Epoch 451/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2113 - val_loss: 0.2715\n",
            "Epoch 452/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2111 - val_loss: 0.2715\n",
            "Epoch 453/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2111 - val_loss: 0.2715\n",
            "Epoch 454/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2110 - val_loss: 0.2715\n",
            "Epoch 455/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2108 - val_loss: 0.2715\n",
            "Epoch 456/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2108 - val_loss: 0.2715\n",
            "Epoch 457/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2107 - val_loss: 0.2715\n",
            "Epoch 458/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2105 - val_loss: 0.2714\n",
            "Epoch 459/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2105 - val_loss: 0.2714\n",
            "Epoch 460/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2104 - val_loss: 0.2714\n",
            "Epoch 461/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2102 - val_loss: 0.2714\n",
            "Epoch 462/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2102 - val_loss: 0.2714\n",
            "Epoch 463/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2101 - val_loss: 0.2715\n",
            "Epoch 464/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2100 - val_loss: 0.2715\n",
            "Epoch 465/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2099 - val_loss: 0.2714\n",
            "Epoch 466/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2097 - val_loss: 0.2713\n",
            "Epoch 467/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2097 - val_loss: 0.2713\n",
            "Epoch 468/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2096 - val_loss: 0.2713\n",
            "Epoch 469/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2094 - val_loss: 0.2713\n",
            "Epoch 470/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2094 - val_loss: 0.2713\n",
            "Epoch 471/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2092 - val_loss: 0.2715\n",
            "Epoch 472/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2092 - val_loss: 0.2713\n",
            "Epoch 473/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2090 - val_loss: 0.2712\n",
            "Epoch 474/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2089 - val_loss: 0.2712\n",
            "Epoch 475/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2089 - val_loss: 0.2712\n",
            "Epoch 476/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2087 - val_loss: 0.2712\n",
            "Epoch 477/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2086 - val_loss: 0.2713\n",
            "Epoch 478/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2085 - val_loss: 0.2712\n",
            "Epoch 479/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2085 - val_loss: 0.2711\n",
            "Epoch 480/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2083 - val_loss: 0.2711\n",
            "Epoch 481/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2082 - val_loss: 0.2711\n",
            "Epoch 482/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2081 - val_loss: 0.2711\n",
            "Epoch 483/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2081 - val_loss: 0.2710\n",
            "Epoch 484/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2081 - val_loss: 0.2711\n",
            "Epoch 485/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2079 - val_loss: 0.2711\n",
            "Epoch 486/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2077 - val_loss: 0.2711\n",
            "Epoch 487/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2077 - val_loss: 0.2710\n",
            "Epoch 488/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2075 - val_loss: 0.2711\n",
            "Epoch 489/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2075 - val_loss: 0.2711\n",
            "Epoch 490/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2073 - val_loss: 0.2710\n",
            "Epoch 491/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2073 - val_loss: 0.2710\n",
            "Epoch 492/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2072 - val_loss: 0.2710\n",
            "Epoch 493/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2071 - val_loss: 0.2710\n",
            "Epoch 494/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2070 - val_loss: 0.2710\n",
            "Epoch 495/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2069 - val_loss: 0.2710\n",
            "Epoch 496/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2068 - val_loss: 0.2714\n",
            "Epoch 497/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2068 - val_loss: 0.2710\n",
            "Epoch 498/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2066 - val_loss: 0.2709\n",
            "Epoch 499/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2065 - val_loss: 0.2709\n",
            "Epoch 500/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2064 - val_loss: 0.2710\n",
            "Epoch 501/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2064 - val_loss: 0.2709\n",
            "Epoch 502/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2063 - val_loss: 0.2710\n",
            "Epoch 503/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2061 - val_loss: 0.2709\n",
            "Epoch 504/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2061 - val_loss: 0.2709\n",
            "Epoch 505/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2060 - val_loss: 0.2710\n",
            "Epoch 506/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2059 - val_loss: 0.2708\n",
            "Epoch 507/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2058 - val_loss: 0.2708\n",
            "Epoch 508/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2057 - val_loss: 0.2708\n",
            "Epoch 509/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2056 - val_loss: 0.2708\n",
            "Epoch 510/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2055 - val_loss: 0.2708\n",
            "Epoch 511/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2055 - val_loss: 0.2709\n",
            "Epoch 512/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2053 - val_loss: 0.2710\n",
            "Epoch 513/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2052 - val_loss: 0.2709\n",
            "Epoch 514/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2051 - val_loss: 0.2708\n",
            "Epoch 515/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2051 - val_loss: 0.2708\n",
            "Epoch 516/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2051 - val_loss: 0.2708\n",
            "Epoch 517/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2049 - val_loss: 0.2708\n",
            "Epoch 518/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2048 - val_loss: 0.2707\n",
            "Epoch 519/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2047 - val_loss: 0.2708\n",
            "Epoch 520/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2048 - val_loss: 0.2708\n",
            "Epoch 521/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2045 - val_loss: 0.2707\n",
            "Epoch 522/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2045 - val_loss: 0.2707\n",
            "Epoch 523/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2044 - val_loss: 0.2707\n",
            "Epoch 524/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2042 - val_loss: 0.2706\n",
            "Epoch 525/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2041 - val_loss: 0.2707\n",
            "Epoch 526/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2041 - val_loss: 0.2707\n",
            "Epoch 527/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2040 - val_loss: 0.2707\n",
            "Epoch 528/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2039 - val_loss: 0.2707\n",
            "Epoch 529/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2037 - val_loss: 0.2707\n",
            "Epoch 530/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2038 - val_loss: 0.2706\n",
            "Epoch 531/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2036 - val_loss: 0.2706\n",
            "Epoch 532/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2035 - val_loss: 0.2706\n",
            "Epoch 533/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2035 - val_loss: 0.2707\n",
            "Epoch 534/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2034 - val_loss: 0.2706\n",
            "Epoch 535/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2033 - val_loss: 0.2706\n",
            "Epoch 536/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2031 - val_loss: 0.2707\n",
            "Epoch 537/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2031 - val_loss: 0.2706\n",
            "Epoch 538/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2030 - val_loss: 0.2706\n",
            "Epoch 539/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2030 - val_loss: 0.2706\n",
            "Epoch 540/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2029 - val_loss: 0.2708\n",
            "Epoch 541/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2027 - val_loss: 0.2706\n",
            "Epoch 542/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2026 - val_loss: 0.2707\n",
            "Epoch 543/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2025 - val_loss: 0.2705\n",
            "Epoch 544/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2025 - val_loss: 0.2706\n",
            "Epoch 545/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2024 - val_loss: 0.2706\n",
            "Epoch 546/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2023 - val_loss: 0.2706\n",
            "Epoch 547/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2023 - val_loss: 0.2705\n",
            "Epoch 548/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2021 - val_loss: 0.2706\n",
            "Epoch 549/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2021 - val_loss: 0.2706\n",
            "Epoch 550/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2020 - val_loss: 0.2706\n",
            "Epoch 551/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2019 - val_loss: 0.2705\n",
            "Epoch 552/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2018 - val_loss: 0.2705\n",
            "Epoch 553/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2018 - val_loss: 0.2705\n",
            "Epoch 554/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2016 - val_loss: 0.2706\n",
            "Epoch 555/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2015 - val_loss: 0.2704\n",
            "Epoch 556/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2015 - val_loss: 0.2704\n",
            "Epoch 557/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2014 - val_loss: 0.2706\n",
            "Epoch 558/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2013 - val_loss: 0.2707\n",
            "Epoch 559/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2013 - val_loss: 0.2704\n",
            "Epoch 560/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2011 - val_loss: 0.2704\n",
            "Epoch 561/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2010 - val_loss: 0.2705\n",
            "Epoch 562/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2009 - val_loss: 0.2704\n",
            "Epoch 563/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2010 - val_loss: 0.2705\n",
            "Epoch 564/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2008 - val_loss: 0.2704\n",
            "Epoch 565/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.2007 - val_loss: 0.2704\n",
            "Epoch 566/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2006 - val_loss: 0.2705\n",
            "Epoch 567/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2006 - val_loss: 0.2704\n",
            "Epoch 568/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2005 - val_loss: 0.2706\n",
            "Epoch 569/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.2004 - val_loss: 0.2705\n",
            "Epoch 570/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.2003 - val_loss: 0.2707\n",
            "Epoch 571/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2003 - val_loss: 0.2704\n",
            "Epoch 572/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2001 - val_loss: 0.2705\n",
            "Epoch 573/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2001 - val_loss: 0.2705\n",
            "Epoch 574/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.2000 - val_loss: 0.2704\n",
            "Epoch 575/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1999 - val_loss: 0.2704\n",
            "Epoch 576/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1999 - val_loss: 0.2706\n",
            "Epoch 577/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1997 - val_loss: 0.2705\n",
            "Epoch 578/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1997 - val_loss: 0.2703\n",
            "Epoch 579/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1996 - val_loss: 0.2704\n",
            "Epoch 580/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1995 - val_loss: 0.2704\n",
            "Epoch 581/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1995 - val_loss: 0.2704\n",
            "Epoch 582/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1994 - val_loss: 0.2704\n",
            "Epoch 583/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1994 - val_loss: 0.2703\n",
            "Epoch 584/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1992 - val_loss: 0.2704\n",
            "Epoch 585/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1992 - val_loss: 0.2704\n",
            "Epoch 586/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1992 - val_loss: 0.2704\n",
            "Epoch 587/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1989 - val_loss: 0.2703\n",
            "Epoch 588/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1988 - val_loss: 0.2704\n",
            "Epoch 589/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1988 - val_loss: 0.2704\n",
            "Epoch 590/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1987 - val_loss: 0.2703\n",
            "Epoch 591/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1987 - val_loss: 0.2704\n",
            "Epoch 592/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1986 - val_loss: 0.2704\n",
            "Epoch 593/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1985 - val_loss: 0.2703\n",
            "Epoch 594/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1984 - val_loss: 0.2706\n",
            "Epoch 595/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1983 - val_loss: 0.2703\n",
            "Epoch 596/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1982 - val_loss: 0.2703\n",
            "Epoch 597/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1982 - val_loss: 0.2704\n",
            "Epoch 598/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1982 - val_loss: 0.2705\n",
            "Epoch 599/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1981 - val_loss: 0.2705\n",
            "Epoch 600/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1979 - val_loss: 0.2706\n",
            "Epoch 601/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1979 - val_loss: 0.2707\n",
            "Epoch 602/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1979 - val_loss: 0.2703\n",
            "Epoch 603/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1976 - val_loss: 0.2703\n",
            "Epoch 604/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1977 - val_loss: 0.2706\n",
            "Epoch 605/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1976 - val_loss: 0.2702\n",
            "Epoch 606/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1975 - val_loss: 0.2705\n",
            "Epoch 607/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1974 - val_loss: 0.2703\n",
            "Epoch 608/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1974 - val_loss: 0.2703\n",
            "Epoch 609/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1973 - val_loss: 0.2702\n",
            "Epoch 610/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1972 - val_loss: 0.2702\n",
            "Epoch 611/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1972 - val_loss: 0.2705\n",
            "Epoch 612/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1970 - val_loss: 0.2703\n",
            "Epoch 613/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1969 - val_loss: 0.2703\n",
            "Epoch 614/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1969 - val_loss: 0.2702\n",
            "Epoch 615/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1968 - val_loss: 0.2702\n",
            "Epoch 616/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1968 - val_loss: 0.2706\n",
            "Epoch 617/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1967 - val_loss: 0.2704\n",
            "Epoch 618/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1966 - val_loss: 0.2702\n",
            "Epoch 619/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1965 - val_loss: 0.2703\n",
            "Epoch 620/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1965 - val_loss: 0.2702\n",
            "Epoch 621/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1964 - val_loss: 0.2702\n",
            "Epoch 622/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1963 - val_loss: 0.2703\n",
            "Epoch 623/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1962 - val_loss: 0.2702\n",
            "Epoch 624/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1961 - val_loss: 0.2702\n",
            "Epoch 625/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1960 - val_loss: 0.2703\n",
            "Epoch 626/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1961 - val_loss: 0.2702\n",
            "Epoch 627/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1960 - val_loss: 0.2702\n",
            "Epoch 628/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1959 - val_loss: 0.2703\n",
            "Epoch 629/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1957 - val_loss: 0.2703\n",
            "Epoch 630/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1958 - val_loss: 0.2704\n",
            "Epoch 631/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1956 - val_loss: 0.2703\n",
            "Epoch 632/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1955 - val_loss: 0.2704\n",
            "Epoch 633/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1956 - val_loss: 0.2703\n",
            "Epoch 634/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1955 - val_loss: 0.2702\n",
            "Epoch 635/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1954 - val_loss: 0.2704\n",
            "Epoch 636/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1953 - val_loss: 0.2703\n",
            "Epoch 637/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1953 - val_loss: 0.2705\n",
            "Epoch 638/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1952 - val_loss: 0.2707\n",
            "Epoch 639/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1950 - val_loss: 0.2703\n",
            "Epoch 640/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1950 - val_loss: 0.2703\n",
            "Epoch 641/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1949 - val_loss: 0.2704\n",
            "Epoch 642/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1947 - val_loss: 0.2702\n",
            "Epoch 643/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1947 - val_loss: 0.2702\n",
            "Epoch 644/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1947 - val_loss: 0.2702\n",
            "Epoch 645/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1946 - val_loss: 0.2704\n",
            "Epoch 646/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1945 - val_loss: 0.2702\n",
            "Epoch 647/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1945 - val_loss: 0.2707\n",
            "Epoch 648/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1944 - val_loss: 0.2704\n",
            "Epoch 649/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1943 - val_loss: 0.2703\n",
            "Epoch 650/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1942 - val_loss: 0.2702\n",
            "Epoch 651/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1941 - val_loss: 0.2704\n",
            "Epoch 652/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1941 - val_loss: 0.2703\n",
            "Epoch 653/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1940 - val_loss: 0.2702\n",
            "Epoch 654/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1941 - val_loss: 0.2703\n",
            "Epoch 655/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1940 - val_loss: 0.2705\n",
            "Epoch 656/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1938 - val_loss: 0.2704\n",
            "Epoch 657/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1938 - val_loss: 0.2704\n",
            "Epoch 658/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1938 - val_loss: 0.2702\n",
            "Epoch 659/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1936 - val_loss: 0.2706\n",
            "Epoch 660/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1936 - val_loss: 0.2703\n",
            "Epoch 661/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1935 - val_loss: 0.2706\n",
            "Epoch 662/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1935 - val_loss: 0.2704\n",
            "Epoch 663/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1933 - val_loss: 0.2702\n",
            "Epoch 664/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1934 - val_loss: 0.2702\n",
            "Epoch 665/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1934 - val_loss: 0.2702\n",
            "Epoch 666/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1932 - val_loss: 0.2703\n",
            "Epoch 667/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1931 - val_loss: 0.2703\n",
            "Epoch 668/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1931 - val_loss: 0.2703\n",
            "Epoch 669/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1929 - val_loss: 0.2702\n",
            "Epoch 670/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1929 - val_loss: 0.2703\n",
            "Epoch 671/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1928 - val_loss: 0.2703\n",
            "Epoch 672/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1927 - val_loss: 0.2703\n",
            "Epoch 673/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1927 - val_loss: 0.2704\n",
            "Epoch 674/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1927 - val_loss: 0.2702\n",
            "Epoch 675/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1926 - val_loss: 0.2703\n",
            "Epoch 676/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1925 - val_loss: 0.2705\n",
            "Epoch 677/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1924 - val_loss: 0.2703\n",
            "Epoch 678/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1924 - val_loss: 0.2703\n",
            "Epoch 679/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1923 - val_loss: 0.2703\n",
            "Epoch 680/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1922 - val_loss: 0.2703\n",
            "Epoch 681/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1922 - val_loss: 0.2703\n",
            "Epoch 682/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1921 - val_loss: 0.2703\n",
            "Epoch 683/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1920 - val_loss: 0.2703\n",
            "Epoch 684/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1919 - val_loss: 0.2704\n",
            "Epoch 685/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1921 - val_loss: 0.2702\n",
            "Epoch 686/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1919 - val_loss: 0.2703\n",
            "Epoch 687/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1919 - val_loss: 0.2704\n",
            "Epoch 688/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1916 - val_loss: 0.2703\n",
            "Epoch 689/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1917 - val_loss: 0.2703\n",
            "Epoch 690/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1916 - val_loss: 0.2708\n",
            "Epoch 691/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1915 - val_loss: 0.2702\n",
            "Epoch 692/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1914 - val_loss: 0.2704\n",
            "Epoch 693/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1914 - val_loss: 0.2703\n",
            "Epoch 694/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1913 - val_loss: 0.2705\n",
            "Epoch 695/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1913 - val_loss: 0.2703\n",
            "Epoch 696/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1913 - val_loss: 0.2703\n",
            "Epoch 697/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1911 - val_loss: 0.2704\n",
            "Epoch 698/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1912 - val_loss: 0.2704\n",
            "Epoch 699/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1909 - val_loss: 0.2705\n",
            "Epoch 700/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1909 - val_loss: 0.2703\n",
            "Epoch 701/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1908 - val_loss: 0.2703\n",
            "Epoch 702/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1909 - val_loss: 0.2704\n",
            "Epoch 703/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1907 - val_loss: 0.2704\n",
            "Epoch 704/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1908 - val_loss: 0.2704\n",
            "Epoch 705/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1906 - val_loss: 0.2706\n",
            "Epoch 706/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1905 - val_loss: 0.2703\n",
            "Epoch 707/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1905 - val_loss: 0.2706\n",
            "Epoch 708/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1905 - val_loss: 0.2703\n",
            "Epoch 709/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1905 - val_loss: 0.2704\n",
            "Epoch 710/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1903 - val_loss: 0.2703\n",
            "Epoch 711/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1904 - val_loss: 0.2705\n",
            "Epoch 712/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1902 - val_loss: 0.2705\n",
            "Epoch 713/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1902 - val_loss: 0.2703\n",
            "Epoch 714/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1900 - val_loss: 0.2704\n",
            "Epoch 715/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1901 - val_loss: 0.2704\n",
            "Epoch 716/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1900 - val_loss: 0.2704\n",
            "Epoch 717/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1899 - val_loss: 0.2705\n",
            "Epoch 718/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1900 - val_loss: 0.2703\n",
            "Epoch 719/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1897 - val_loss: 0.2704\n",
            "Epoch 720/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1897 - val_loss: 0.2705\n",
            "Epoch 721/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1896 - val_loss: 0.2703\n",
            "Epoch 722/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1895 - val_loss: 0.2704\n",
            "Epoch 723/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1895 - val_loss: 0.2704\n",
            "Epoch 724/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1895 - val_loss: 0.2704\n",
            "Epoch 725/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1894 - val_loss: 0.2706\n",
            "Epoch 726/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1896 - val_loss: 0.2705\n",
            "Epoch 727/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1894 - val_loss: 0.2705\n",
            "Epoch 728/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1893 - val_loss: 0.2704\n",
            "Epoch 729/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1891 - val_loss: 0.2705\n",
            "Epoch 730/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1891 - val_loss: 0.2708\n",
            "Epoch 731/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1891 - val_loss: 0.2707\n",
            "Epoch 732/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1890 - val_loss: 0.2704\n",
            "Epoch 733/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1890 - val_loss: 0.2708\n",
            "Epoch 734/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1889 - val_loss: 0.2706\n",
            "Epoch 735/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1889 - val_loss: 0.2704\n",
            "Epoch 736/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1888 - val_loss: 0.2704\n",
            "Epoch 737/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1887 - val_loss: 0.2704\n",
            "Epoch 738/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1886 - val_loss: 0.2704\n",
            "Epoch 739/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1885 - val_loss: 0.2703\n",
            "Epoch 740/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1885 - val_loss: 0.2704\n",
            "Epoch 741/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1884 - val_loss: 0.2704\n",
            "Epoch 742/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1885 - val_loss: 0.2705\n",
            "Epoch 743/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1884 - val_loss: 0.2705\n",
            "Epoch 744/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1882 - val_loss: 0.2708\n",
            "Epoch 745/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1883 - val_loss: 0.2710\n",
            "Epoch 746/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1882 - val_loss: 0.2705\n",
            "Epoch 747/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1880 - val_loss: 0.2704\n",
            "Epoch 748/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1880 - val_loss: 0.2705\n",
            "Epoch 749/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1880 - val_loss: 0.2705\n",
            "Epoch 750/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1880 - val_loss: 0.2706\n",
            "Epoch 751/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1880 - val_loss: 0.2705\n",
            "Epoch 752/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1877 - val_loss: 0.2705\n",
            "Epoch 753/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1878 - val_loss: 0.2705\n",
            "Epoch 754/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1878 - val_loss: 0.2704\n",
            "Epoch 755/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1877 - val_loss: 0.2705\n",
            "Epoch 756/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1876 - val_loss: 0.2704\n",
            "Epoch 757/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1874 - val_loss: 0.2704\n",
            "Epoch 758/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1875 - val_loss: 0.2711\n",
            "Epoch 759/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1874 - val_loss: 0.2705\n",
            "Epoch 760/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1873 - val_loss: 0.2705\n",
            "Epoch 761/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1873 - val_loss: 0.2705\n",
            "Epoch 762/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1873 - val_loss: 0.2706\n",
            "Epoch 763/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1872 - val_loss: 0.2706\n",
            "Epoch 764/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1871 - val_loss: 0.2706\n",
            "Epoch 765/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1870 - val_loss: 0.2705\n",
            "Epoch 766/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1869 - val_loss: 0.2706\n",
            "Epoch 767/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1870 - val_loss: 0.2706\n",
            "Epoch 768/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1868 - val_loss: 0.2708\n",
            "Epoch 769/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1869 - val_loss: 0.2706\n",
            "Epoch 770/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1868 - val_loss: 0.2705\n",
            "Epoch 771/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1867 - val_loss: 0.2706\n",
            "Epoch 772/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1866 - val_loss: 0.2705\n",
            "Epoch 773/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1867 - val_loss: 0.2711\n",
            "Epoch 774/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1865 - val_loss: 0.2706\n",
            "Epoch 775/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1866 - val_loss: 0.2706\n",
            "Epoch 776/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1865 - val_loss: 0.2706\n",
            "Epoch 777/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1863 - val_loss: 0.2706\n",
            "Epoch 778/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1863 - val_loss: 0.2706\n",
            "Epoch 779/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1865 - val_loss: 0.2708\n",
            "Epoch 780/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1863 - val_loss: 0.2705\n",
            "Epoch 781/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1861 - val_loss: 0.2707\n",
            "Epoch 782/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1862 - val_loss: 0.2707\n",
            "Epoch 783/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1860 - val_loss: 0.2711\n",
            "Epoch 784/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1861 - val_loss: 0.2707\n",
            "Epoch 785/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1860 - val_loss: 0.2706\n",
            "Epoch 786/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1859 - val_loss: 0.2709\n",
            "Epoch 787/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1858 - val_loss: 0.2706\n",
            "Epoch 788/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1858 - val_loss: 0.2706\n",
            "Epoch 789/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1858 - val_loss: 0.2709\n",
            "Epoch 790/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1858 - val_loss: 0.2713\n",
            "Epoch 791/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1857 - val_loss: 0.2707\n",
            "Epoch 792/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1856 - val_loss: 0.2706\n",
            "Epoch 793/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1856 - val_loss: 0.2706\n",
            "Epoch 794/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1855 - val_loss: 0.2709\n",
            "Epoch 795/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1855 - val_loss: 0.2707\n",
            "Epoch 796/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1854 - val_loss: 0.2706\n",
            "Epoch 797/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1853 - val_loss: 0.2707\n",
            "Epoch 798/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1853 - val_loss: 0.2706\n",
            "Epoch 799/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1853 - val_loss: 0.2707\n",
            "Epoch 800/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1851 - val_loss: 0.2708\n",
            "Epoch 801/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1851 - val_loss: 0.2707\n",
            "Epoch 802/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1850 - val_loss: 0.2712\n",
            "Epoch 803/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1850 - val_loss: 0.2707\n",
            "Epoch 804/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1850 - val_loss: 0.2707\n",
            "Epoch 805/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1850 - val_loss: 0.2709\n",
            "Epoch 806/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1848 - val_loss: 0.2707\n",
            "Epoch 807/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1847 - val_loss: 0.2707\n",
            "Epoch 808/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1848 - val_loss: 0.2711\n",
            "Epoch 809/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1848 - val_loss: 0.2707\n",
            "Epoch 810/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1846 - val_loss: 0.2708\n",
            "Epoch 811/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1845 - val_loss: 0.2707\n",
            "Epoch 812/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1845 - val_loss: 0.2709\n",
            "Epoch 813/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1845 - val_loss: 0.2711\n",
            "Epoch 814/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1845 - val_loss: 0.2707\n",
            "Epoch 815/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1843 - val_loss: 0.2712\n",
            "Epoch 816/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1844 - val_loss: 0.2708\n",
            "Epoch 817/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1844 - val_loss: 0.2707\n",
            "Epoch 818/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1841 - val_loss: 0.2709\n",
            "Epoch 819/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1843 - val_loss: 0.2709\n",
            "Epoch 820/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1841 - val_loss: 0.2710\n",
            "Epoch 821/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1841 - val_loss: 0.2708\n",
            "Epoch 822/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1840 - val_loss: 0.2707\n",
            "Epoch 823/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1839 - val_loss: 0.2707\n",
            "Epoch 824/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1840 - val_loss: 0.2708\n",
            "Epoch 825/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1839 - val_loss: 0.2715\n",
            "Epoch 826/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1839 - val_loss: 0.2708\n",
            "Epoch 827/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1838 - val_loss: 0.2707\n",
            "Epoch 828/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1837 - val_loss: 0.2708\n",
            "Epoch 829/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1836 - val_loss: 0.2714\n",
            "Epoch 830/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1836 - val_loss: 0.2710\n",
            "Epoch 831/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1836 - val_loss: 0.2709\n",
            "Epoch 832/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1836 - val_loss: 0.2709\n",
            "Epoch 833/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1835 - val_loss: 0.2708\n",
            "Epoch 834/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1834 - val_loss: 0.2709\n",
            "Epoch 835/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1834 - val_loss: 0.2711\n",
            "Epoch 836/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1834 - val_loss: 0.2708\n",
            "Epoch 837/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1833 - val_loss: 0.2708\n",
            "Epoch 838/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1832 - val_loss: 0.2712\n",
            "Epoch 839/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1831 - val_loss: 0.2710\n",
            "Epoch 840/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1831 - val_loss: 0.2709\n",
            "Epoch 841/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1830 - val_loss: 0.2708\n",
            "Epoch 842/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1830 - val_loss: 0.2710\n",
            "Epoch 843/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1830 - val_loss: 0.2709\n",
            "Epoch 844/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1830 - val_loss: 0.2714\n",
            "Epoch 845/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1830 - val_loss: 0.2710\n",
            "Epoch 846/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1828 - val_loss: 0.2708\n",
            "Epoch 847/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1829 - val_loss: 0.2710\n",
            "Epoch 848/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1828 - val_loss: 0.2714\n",
            "Epoch 849/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1827 - val_loss: 0.2710\n",
            "Epoch 850/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1827 - val_loss: 0.2715\n",
            "Epoch 851/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1826 - val_loss: 0.2709\n",
            "Epoch 852/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1826 - val_loss: 0.2710\n",
            "Epoch 853/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1825 - val_loss: 0.2709\n",
            "Epoch 854/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1824 - val_loss: 0.2710\n",
            "Epoch 855/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1824 - val_loss: 0.2710\n",
            "Epoch 856/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1823 - val_loss: 0.2711\n",
            "Epoch 857/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1823 - val_loss: 0.2709\n",
            "Epoch 858/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1822 - val_loss: 0.2709\n",
            "Epoch 859/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1821 - val_loss: 0.2709\n",
            "Epoch 860/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1823 - val_loss: 0.2713\n",
            "Epoch 861/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1821 - val_loss: 0.2712\n",
            "Epoch 862/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1821 - val_loss: 0.2713\n",
            "Epoch 863/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1820 - val_loss: 0.2709\n",
            "Epoch 864/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1820 - val_loss: 0.2710\n",
            "Epoch 865/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1820 - val_loss: 0.2717\n",
            "Epoch 866/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1819 - val_loss: 0.2711\n",
            "Epoch 867/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1818 - val_loss: 0.2710\n",
            "Epoch 868/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1819 - val_loss: 0.2713\n",
            "Epoch 869/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1818 - val_loss: 0.2711\n",
            "Epoch 870/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1817 - val_loss: 0.2711\n",
            "Epoch 871/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1817 - val_loss: 0.2711\n",
            "Epoch 872/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1816 - val_loss: 0.2711\n",
            "Epoch 873/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1816 - val_loss: 0.2712\n",
            "Epoch 874/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1814 - val_loss: 0.2711\n",
            "Epoch 875/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1815 - val_loss: 0.2710\n",
            "Epoch 876/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1813 - val_loss: 0.2711\n",
            "Epoch 877/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1813 - val_loss: 0.2712\n",
            "Epoch 878/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1813 - val_loss: 0.2711\n",
            "Epoch 879/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1812 - val_loss: 0.2711\n",
            "Epoch 880/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1812 - val_loss: 0.2710\n",
            "Epoch 881/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1812 - val_loss: 0.2711\n",
            "Epoch 882/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1811 - val_loss: 0.2713\n",
            "Epoch 883/1000\n",
            "34/34 [==============================] - 1s 26ms/step - loss: 0.1810 - val_loss: 0.2711\n",
            "Epoch 884/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1810 - val_loss: 0.2712\n",
            "Epoch 885/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1811 - val_loss: 0.2714\n",
            "Epoch 886/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1810 - val_loss: 0.2713\n",
            "Epoch 887/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1808 - val_loss: 0.2711\n",
            "Epoch 888/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1810 - val_loss: 0.2719\n",
            "Epoch 889/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1808 - val_loss: 0.2712\n",
            "Epoch 890/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1808 - val_loss: 0.2712\n",
            "Epoch 891/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1807 - val_loss: 0.2714\n",
            "Epoch 892/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1806 - val_loss: 0.2712\n",
            "Epoch 893/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1806 - val_loss: 0.2712\n",
            "Epoch 894/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1805 - val_loss: 0.2713\n",
            "Epoch 895/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1806 - val_loss: 0.2714\n",
            "Epoch 896/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1804 - val_loss: 0.2716\n",
            "Epoch 897/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1804 - val_loss: 0.2712\n",
            "Epoch 898/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1805 - val_loss: 0.2712\n",
            "Epoch 899/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1804 - val_loss: 0.2712\n",
            "Epoch 900/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1804 - val_loss: 0.2716\n",
            "Epoch 901/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1803 - val_loss: 0.2714\n",
            "Epoch 902/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1802 - val_loss: 0.2712\n",
            "Epoch 903/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1802 - val_loss: 0.2713\n",
            "Epoch 904/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1801 - val_loss: 0.2714\n",
            "Epoch 905/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1800 - val_loss: 0.2712\n",
            "Epoch 906/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1800 - val_loss: 0.2713\n",
            "Epoch 907/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1800 - val_loss: 0.2714\n",
            "Epoch 908/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1800 - val_loss: 0.2717\n",
            "Epoch 909/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1799 - val_loss: 0.2713\n",
            "Epoch 910/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1799 - val_loss: 0.2713\n",
            "Epoch 911/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1799 - val_loss: 0.2713\n",
            "Epoch 912/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1798 - val_loss: 0.2714\n",
            "Epoch 913/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1797 - val_loss: 0.2714\n",
            "Epoch 914/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1798 - val_loss: 0.2713\n",
            "Epoch 915/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1796 - val_loss: 0.2713\n",
            "Epoch 916/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1796 - val_loss: 0.2714\n",
            "Epoch 917/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1795 - val_loss: 0.2714\n",
            "Epoch 918/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1795 - val_loss: 0.2718\n",
            "Epoch 919/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1795 - val_loss: 0.2716\n",
            "Epoch 920/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1794 - val_loss: 0.2716\n",
            "Epoch 921/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1794 - val_loss: 0.2716\n",
            "Epoch 922/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1794 - val_loss: 0.2719\n",
            "Epoch 923/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1794 - val_loss: 0.2715\n",
            "Epoch 924/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1793 - val_loss: 0.2715\n",
            "Epoch 925/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1792 - val_loss: 0.2714\n",
            "Epoch 926/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1792 - val_loss: 0.2714\n",
            "Epoch 927/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1792 - val_loss: 0.2714\n",
            "Epoch 928/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1790 - val_loss: 0.2715\n",
            "Epoch 929/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1790 - val_loss: 0.2715\n",
            "Epoch 930/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1791 - val_loss: 0.2716\n",
            "Epoch 931/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1790 - val_loss: 0.2715\n",
            "Epoch 932/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1790 - val_loss: 0.2721\n",
            "Epoch 933/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1789 - val_loss: 0.2717\n",
            "Epoch 934/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1788 - val_loss: 0.2718\n",
            "Epoch 935/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1788 - val_loss: 0.2715\n",
            "Epoch 936/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1788 - val_loss: 0.2722\n",
            "Epoch 937/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1790 - val_loss: 0.2715\n",
            "Epoch 938/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1787 - val_loss: 0.2715\n",
            "Epoch 939/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1787 - val_loss: 0.2716\n",
            "Epoch 940/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1785 - val_loss: 0.2716\n",
            "Epoch 941/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1785 - val_loss: 0.2715\n",
            "Epoch 942/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1785 - val_loss: 0.2719\n",
            "Epoch 943/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1784 - val_loss: 0.2716\n",
            "Epoch 944/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1784 - val_loss: 0.2718\n",
            "Epoch 945/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1784 - val_loss: 0.2717\n",
            "Epoch 946/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1784 - val_loss: 0.2716\n",
            "Epoch 947/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1783 - val_loss: 0.2719\n",
            "Epoch 948/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1783 - val_loss: 0.2717\n",
            "Epoch 949/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1784 - val_loss: 0.2716\n",
            "Epoch 950/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1782 - val_loss: 0.2716\n",
            "Epoch 951/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1781 - val_loss: 0.2717\n",
            "Epoch 952/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1781 - val_loss: 0.2716\n",
            "Epoch 953/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1781 - val_loss: 0.2718\n",
            "Epoch 954/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1781 - val_loss: 0.2718\n",
            "Epoch 955/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1781 - val_loss: 0.2725\n",
            "Epoch 956/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1781 - val_loss: 0.2725\n",
            "Epoch 957/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1780 - val_loss: 0.2720\n",
            "Epoch 958/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1778 - val_loss: 0.2718\n",
            "Epoch 959/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1778 - val_loss: 0.2717\n",
            "Epoch 960/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1778 - val_loss: 0.2719\n",
            "Epoch 961/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1777 - val_loss: 0.2718\n",
            "Epoch 962/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1777 - val_loss: 0.2718\n",
            "Epoch 963/1000\n",
            "34/34 [==============================] - 1s 25ms/step - loss: 0.1777 - val_loss: 0.2717\n",
            "Epoch 964/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1776 - val_loss: 0.2718\n",
            "Epoch 965/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1774 - val_loss: 0.2717\n",
            "Epoch 966/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1775 - val_loss: 0.2717\n",
            "Epoch 967/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1776 - val_loss: 0.2718\n",
            "Epoch 968/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1775 - val_loss: 0.2722\n",
            "Epoch 969/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1774 - val_loss: 0.2717\n",
            "Epoch 970/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1773 - val_loss: 0.2717\n",
            "Epoch 971/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1774 - val_loss: 0.2720\n",
            "Epoch 972/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1773 - val_loss: 0.2718\n",
            "Epoch 973/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1773 - val_loss: 0.2718\n",
            "Epoch 974/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1771 - val_loss: 0.2718\n",
            "Epoch 975/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1772 - val_loss: 0.2719\n",
            "Epoch 976/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1772 - val_loss: 0.2725\n",
            "Epoch 977/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1772 - val_loss: 0.2718\n",
            "Epoch 978/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1771 - val_loss: 0.2718\n",
            "Epoch 979/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1770 - val_loss: 0.2718\n",
            "Epoch 980/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1770 - val_loss: 0.2718\n",
            "Epoch 981/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1769 - val_loss: 0.2720\n",
            "Epoch 982/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1768 - val_loss: 0.2719\n",
            "Epoch 983/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1769 - val_loss: 0.2720\n",
            "Epoch 984/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1768 - val_loss: 0.2719\n",
            "Epoch 985/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1768 - val_loss: 0.2718\n",
            "Epoch 986/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1767 - val_loss: 0.2719\n",
            "Epoch 987/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1767 - val_loss: 0.2720\n",
            "Epoch 988/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1767 - val_loss: 0.2719\n",
            "Epoch 989/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1767 - val_loss: 0.2719\n",
            "Epoch 990/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1766 - val_loss: 0.2721\n",
            "Epoch 991/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1765 - val_loss: 0.2723\n",
            "Epoch 992/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1767 - val_loss: 0.2719\n",
            "Epoch 993/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1765 - val_loss: 0.2720\n",
            "Epoch 994/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1764 - val_loss: 0.2720\n",
            "Epoch 995/1000\n",
            "34/34 [==============================] - 1s 23ms/step - loss: 0.1765 - val_loss: 0.2719\n",
            "Epoch 996/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1763 - val_loss: 0.2721\n",
            "Epoch 997/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1763 - val_loss: 0.2722\n",
            "Epoch 998/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1762 - val_loss: 0.2722\n",
            "Epoch 999/1000\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.1763 - val_loss: 0.2720\n",
            "Epoch 1000/1000\n",
            "34/34 [==============================] - 1s 22ms/step - loss: 0.1762 - val_loss: 0.2721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3a0b9f58b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0.034874942, 0.2212694204456747, 3.6797975968037435), (0.032329243, 0.23580026189427816, 3.7484075950663445), (0.030779244, 0.2276339840277261, 3.6787310024161917), (0.033040255, 0.23473775004729208, 3.7397849988172176), (0.032491542, 0.21413388410496287, 3.70264879547916)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ae_df = pd.DataFrame.from_records(scores_ae, columns=[\"Silhouette\", \"ARI\", \"DB score\"])\n",
        "ae_df[\"Method\"] = \"AE\"\n",
        "ae_df.describe().loc[[\"mean\", \"std\"],]\n",
        "ae_df.to_csv(\"AE.csv\", index=True)"
      ],
      "metadata": {
        "id": "yFo55AhvdFN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combined_df = compr_df.append(pca_df).append(sigmoid_ipca_df).append(poly_ipca_df).append(ae_df).append(umap_df)#\n",
        "#combined_df.to_csv(f\"{path}../Baseline_models_combined.csv.zip\", compression=\"zip\", sep=\";\")"
      ],
      "metadata": {
        "id": "-H6ELnEwvYqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PHCvOdLfvdBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining all result from baseline and autoencoder-based models"
      ],
      "metadata": {
        "id": "BlSxsqpBMItc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = pd.read_csv(f\"{path}/../../Baseline_models_combined.csv.zip\", compression=\"zip\", sep=\";\")\n",
        "combined_df.head()"
      ],
      "metadata": {
        "id": "ca_NQfStPILu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "700108d3-d5a9-4bdb-feac-5b5852a84479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Silhuette    ARI  Davies-Bouldin          Method\n",
              "0     -0.014  0.136           5.280  No compression\n",
              "1     -0.012  0.141           5.300  No compression\n",
              "2     -0.009  0.129           5.309  No compression\n",
              "3     -0.011  0.132           5.405  No compression\n",
              "4     -0.011  0.119           5.111  No compression"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a50ef6d-f751-4008-9a40-7acf838304f7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Silhuette</th>\n",
              "      <th>ARI</th>\n",
              "      <th>Davies-Bouldin</th>\n",
              "      <th>Method</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.014</td>\n",
              "      <td>0.136</td>\n",
              "      <td>5.280</td>\n",
              "      <td>No compression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.012</td>\n",
              "      <td>0.141</td>\n",
              "      <td>5.300</td>\n",
              "      <td>No compression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.009</td>\n",
              "      <td>0.129</td>\n",
              "      <td>5.309</td>\n",
              "      <td>No compression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.011</td>\n",
              "      <td>0.132</td>\n",
              "      <td>5.405</td>\n",
              "      <td>No compression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.011</td>\n",
              "      <td>0.119</td>\n",
              "      <td>5.111</td>\n",
              "      <td>No compression</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a50ef6d-f751-4008-9a40-7acf838304f7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2a50ef6d-f751-4008-9a40-7acf838304f7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2a50ef6d-f751-4008-9a40-7acf838304f7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data=combined_df, x=\"Method\", y=\"Davies-Bouldin\")\n",
        "plt.title(\"The Davies-Bouldin score over all models\",fontsize=14)\n",
        "plt.ylabel(\"The Davies-Bouldin score [-]\", fontsize=14)\n",
        "plt.xlabel(\"\", fontsize=14)\n",
        "plt.xticks(fontsize=14, rotation=45, ha=\"right\")\n",
        "plt.yticks(fontsize=14)\n",
        "\n",
        "x1, x2 = 0, 4 \n",
        "y, h, col = 5.8, 0.05, 'k'\n",
        "plt.plot([x1, x1, x2, x2], [y, y+h, y+h, y], lw=1, c=col)\n",
        "plt.text((x1+x2)*.5, y+h, \"**\", ha='center', va='bottom')\n",
        "x1, x2 = 4, 5 \n",
        "y, h, col = 6.31, 0.05, 'k'\n",
        "plt.plot([x1, x1, x2, x2], [y, y+h, y+h, y], lw=1, c=col)\n",
        "plt.text((x1+x2)*.5, y+h, \"**\", ha='center', va='bottom')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "EJaXXnnur3pr",
        "outputId": "312b35fa-3f8e-4001-97fa-ac8b8459cbd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data=combined_df, x=\"Method\", y=\"ARI\")\n",
        "plt.title(\"The Adjusted Rand Index over all models\", fontsize=14)\n",
        "plt.ylabel(\"The Adjusted Rand Index [-]\", fontsize=14)\n",
        "plt.xlabel(\"\", fontsize=14)\n",
        "plt.xticks(fontsize=14, rotation=45, ha=\"right\")\n",
        "plt.yticks(fontsize=14)\n",
        "\n",
        "x1, x2 = 3, 4 \n",
        "y, h, col = 0.25, 0.005, 'k'\n",
        "plt.plot([x1, x1, x2, x2], [y, y+h, y+h, y], lw=1, c=col)\n",
        "plt.text((x1+x2)*.5, y+h, \"**\", ha='center', va='bottom')\n",
        "x1, x2 = 1, 4 \n",
        "y, h, col = 0.27, 0.005, 'k'\n",
        "plt.plot([x1, x1, x2, x2], [y, y+h, y+h, y], lw=1, c=col)\n",
        "plt.text((x1+x2)*.5, y+h, \"**\", ha='center', va='bottom')\n",
        "\n",
        "x1, x2 = 1, 5 \n",
        "y, h, col = 0.29, 0.005, 'k'\n",
        "plt.plot([x1, x1, x2, x2], [y, y+h, y+h, y], lw=1, c=col)\n",
        "plt.text((x1+x2)*.5, y+h, \"*\", ha='center', va='bottom')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "-Fi4v7gcZcqV",
        "outputId": "459bd5e3-44cd-47dc-cd09-ac9ceb2ea357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(3.0, 0.295, '*')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data=combined_df, x=\"Method\", y=\"Silhuette\")\n",
        "plt.title(\"The Silhuette score over all models\", fontsize=14)\n",
        "plt.ylabel(\"Silhuette score [-]\", fontsize=14)\n",
        "plt.xlabel(\"\", fontsize=14)\n",
        "plt.xticks(fontsize=14, rotation=45, ha=\"right\")\n",
        "plt.yticks(fontsize=14)\n",
        "x1, x2 = 4, 5 \n",
        "y, h, col = 0.040, 0.001, 'k'\n",
        "plt.plot([x1, x1, x2, x2], [y, y+h, y+h, y], lw=1, c=col)\n",
        "plt.text((x1+x2)*.5, y+h, \"*\", ha='center', va='bottom')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "-JwWblWqZgC9",
        "outputId": "21e964be-29dc-40c3-fb18-681a37096667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(4.5, 0.041, '*')"
            ]
          },
          "metadata": {},
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stastical comparison of baseline and autoencoder-based models"
      ],
      "metadata": {
        "id": "Ijzd9HWeJ5vs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing normality of the data "
      ],
      "metadata": {
        "id": "iuCayyoPV6Gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normality of the data could be checked visually via QQ plot or histagram. However there is a statisticial test that allows to find if data comes from normal distibution - Shapiro-Wilk Test."
      ],
      "metadata": {
        "id": "_pi-c4SybQa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "methods = [\"No compression\", \"PCA\", \"kernel PCA (sigmoid)\", \"kernel PCA (poly)\", \"AE\",  \"UMAP\"]\n",
        "\n",
        "bouldin, ari, silh, labels = [], [], [], []\n",
        "\n",
        "for method in methods:\n",
        "  labels.append(method)\n",
        "  df = combined_df[combined_df[\"Method\"] == method]\n",
        "  bouldin.append(df[\"Davies-Bouldin\"].tolist())\n",
        "  ari.append(df[\"ARI\"].tolist())\n",
        "  silh.append(df[\"Silhuette\"].tolist())"
      ],
      "metadata": {
        "id": "mGzxQxnbV3JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shapiro(bouldin)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6TOq_S3b31U",
        "outputId": "42246fa5-09a1-49ec-b9e4-b91fba378525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ShapiroResult(statistic=0.909479558467865, pvalue=0.014437713660299778)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shapiro(ari)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68HUPHoocKuh",
        "outputId": "d8fbd973-a6d0-4d0b-aab7-bc5288db8dd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ShapiroResult(statistic=0.7785063982009888, pvalue=2.7253468942944892e-05)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shapiro(silh)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P8hIRSucNpe",
        "outputId": "c4696339-ced5-4c26-f078-1fe53ac5ed5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ShapiroResult(statistic=0.8237898349761963, pvalue=0.0001836761221056804)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the p-value is less than .05, we reject the null hypothesis of the Shapiro-Wilk test. This means we have sufficient evidence to say that the sample data does not come from a normal distribution."
      ],
      "metadata": {
        "id": "BukL_hlycqAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Friedman (non-prametric) test for paired samples"
      ],
      "metadata": {
        "id": "62Y2GFaGV3pG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the statistics above we can conclude that the data are not normally distributed ans we should use non-parametric model. Setting random state in scikit-learn KFold function generate the same data partitions for cross-validation each time when the code is running. This means that the performance metrics from different models will be based on the same data splits, and the samples as paired. The Friedman test is used for comparison of paired samples (non-parametric).\n",
        "\n"
      ],
      "metadata": {
        "id": "-BOyzO6HXldO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing on the Davies-Bouldin score\n",
        "f, p_value = friedmanchisquare(*bouldin)\n",
        "print('F: %.3f' % f)\n",
        "print('p-value: %.5f' % p_value)\n",
        "\n",
        "print(round(p_value*10000, 3), \"*10^4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-dmKccLBmPr",
        "outputId": "f82d4ec7-045c-4e75-cbf8-383c65695943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F: 20.543\n",
            "p-value: 0.00099\n",
            "9.88 *10^4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing on the Silhuette score\n",
        "f, p_value = friedmanchisquare(*silh)\n",
        "print('F: %.3f' % f)\n",
        "print('p-value: %.5f' % p_value)\n",
        "\n",
        "print(round(p_value*10000,3), \"*10^4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOzwGkhpCWlU",
        "outputId": "ed9658f5-c94a-4331-a084-0d3cbbe66f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F: 23.953\n",
            "p-value: 0.00022\n",
            "2.216 *10^4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing on the ARI score\n",
        "f, p_value = friedmanchisquare(*ari)\n",
        "print('F: %.3f' % f)\n",
        "print('p-value: %.5f' % p_value)\n",
        "\n",
        "print(round(p_value*10000, 3), \"*10^4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtL3KtXQCaNc",
        "outputId": "a6f83720-577f-436d-a889-a1c023a8ff84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F: 21.782\n",
            "p-value: 0.00058\n",
            "5.761 *10^4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data above show that there is a significant difference between models, however we do not know which models are different from each other. To identify these pairs of models, Nemenyi post-hoc test is performed."
      ],
      "metadata": {
        "id": "OsmaDZbTeo9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nemenyi post-hoc test"
      ],
      "metadata": {
        "id": "3xsBfjQ_hd_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bonferroni correction was not used to adjust for multiple comparison if Nemenyi test as the Nemenyi test already accounts for multiple comparisons by using the Studentized range distribution (q-distribution) to compute the critical difference. This test is designed to control the family-wise error rate for multiple comparisons in a manner similar to the Bonferroni correction, but with a less conservative approach."
      ],
      "metadata": {
        "id": "avteb7_cvTfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nemenyi_results = sp.posthoc_nemenyi_friedman(np.array(bouldin).T)\n",
        "nemenyi_results.columns = nemenyi_results.index = labels\n",
        "nemenyi_results = round(nemenyi_results, 3)\n",
        "pd.DataFrame(nemenyi_results).to_csv(f\"{path}/nemenyi_results_bouldin.csv\", index=True)\n",
        "nemenyi_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "sUmD3UzWhnz5",
        "outputId": "47b93772-cac2-4c5f-a8e9-2075ae052938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      No compression   PCA  kernel PCA (sigmoid)  \\\n",
              "No compression                 1.000  0.43                 0.900   \n",
              "PCA                            0.430  1.00                 0.900   \n",
              "kernel PCA (sigmoid)           0.900  0.90                 1.000   \n",
              "kernel PCA (poly)              0.075  0.90                 0.533   \n",
              "AE                             0.003  0.43                 0.075   \n",
              "UMAP                           0.900  0.63                 0.900   \n",
              "\n",
              "                      kernel PCA (poly)     AE   UMAP  \n",
              "No compression                    0.075  0.003  0.900  \n",
              "PCA                               0.900  0.430  0.630  \n",
              "kernel PCA (sigmoid)              0.533  0.075  0.900  \n",
              "kernel PCA (poly)                 1.000  0.900  0.168  \n",
              "AE                                0.900  1.000  0.009  \n",
              "UMAP                              0.168  0.009  1.000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6913a1de-ebb2-4251-b5e2-5456f6beb7e1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No compression</th>\n",
              "      <th>PCA</th>\n",
              "      <th>kernel PCA (sigmoid)</th>\n",
              "      <th>kernel PCA (poly)</th>\n",
              "      <th>AE</th>\n",
              "      <th>UMAP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>No compression</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PCA</th>\n",
              "      <td>0.430</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kernel PCA (sigmoid)</th>\n",
              "      <td>0.900</td>\n",
              "      <td>0.90</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.533</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kernel PCA (poly)</th>\n",
              "      <td>0.075</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.533</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AE</th>\n",
              "      <td>0.003</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.900</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>UMAP</th>\n",
              "      <td>0.900</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.009</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6913a1de-ebb2-4251-b5e2-5456f6beb7e1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6913a1de-ebb2-4251-b5e2-5456f6beb7e1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6913a1de-ebb2-4251-b5e2-5456f6beb7e1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nemenyi_results = sp.posthoc_nemenyi_friedman(np.array(silh).T)\n",
        "nemenyi_results.columns = nemenyi_results.index = labels\n",
        "nemenyi_results = round(nemenyi_results, 3)\n",
        "pd.DataFrame(nemenyi_results).to_csv(f\"{path}/nemenyi_results_silh.csv\", index=True)\n",
        "nemenyi_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "gzQize2PpTeD",
        "outputId": "f21f0506-0f53-4d03-ff85-9c7665c26ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      No compression    PCA  kernel PCA (sigmoid)  \\\n",
              "No compression                 1.000  0.900                 0.900   \n",
              "PCA                            0.900  1.000                 0.874   \n",
              "kernel PCA (sigmoid)           0.900  0.874                 1.000   \n",
              "kernel PCA (poly)              0.679  0.777                 0.168   \n",
              "AE                             0.201  0.280                 0.017   \n",
              "UMAP                           0.376  0.280                 0.900   \n",
              "\n",
              "                      kernel PCA (poly)     AE   UMAP  \n",
              "No compression                    0.679  0.201  0.376  \n",
              "PCA                               0.777  0.280  0.280  \n",
              "kernel PCA (sigmoid)              0.168  0.017  0.900  \n",
              "kernel PCA (poly)                 1.000  0.900  0.009  \n",
              "AE                                0.900  1.000  0.001  \n",
              "UMAP                              0.009  0.001  1.000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ecd2ff8a-d3d6-47bb-9207-2ebc1aefd7af\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No compression</th>\n",
              "      <th>PCA</th>\n",
              "      <th>kernel PCA (sigmoid)</th>\n",
              "      <th>kernel PCA (poly)</th>\n",
              "      <th>AE</th>\n",
              "      <th>UMAP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>No compression</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.679</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PCA</th>\n",
              "      <td>0.900</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.874</td>\n",
              "      <td>0.777</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kernel PCA (sigmoid)</th>\n",
              "      <td>0.900</td>\n",
              "      <td>0.874</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kernel PCA (poly)</th>\n",
              "      <td>0.679</td>\n",
              "      <td>0.777</td>\n",
              "      <td>0.168</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AE</th>\n",
              "      <td>0.201</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.900</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>UMAP</th>\n",
              "      <td>0.376</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.001</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ecd2ff8a-d3d6-47bb-9207-2ebc1aefd7af')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ecd2ff8a-d3d6-47bb-9207-2ebc1aefd7af button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ecd2ff8a-d3d6-47bb-9207-2ebc1aefd7af');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nemenyi_results = sp.posthoc_nemenyi_friedman(np.array(ari).T)\n",
        "nemenyi_results = round(nemenyi_results, 3)\n",
        "nemenyi_results.columns = nemenyi_results.index = labels\n",
        "pd.DataFrame(nemenyi_results).to_csv(f\"{path}/nemenyi_results_ari.csv\", index=True)\n",
        "nemenyi_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "IMGH3lyMpKRo",
        "outputId": "386e9d56-3322-4e18-cbaf-44ed1b82487f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      No compression    PCA  kernel PCA (sigmoid)  \\\n",
              "No compression                 1.000  0.777                 0.900   \n",
              "PCA                            0.777  1.000                 0.582   \n",
              "kernel PCA (sigmoid)           0.900  0.582                 1.000   \n",
              "kernel PCA (poly)              0.874  0.900                 0.679   \n",
              "AE                             0.139  0.003                 0.280   \n",
              "UMAP                           0.582  0.047                 0.777   \n",
              "\n",
              "                      kernel PCA (poly)     AE   UMAP  \n",
              "No compression                    0.874  0.139  0.582  \n",
              "PCA                               0.900  0.003  0.047  \n",
              "kernel PCA (sigmoid)              0.679  0.280  0.777  \n",
              "kernel PCA (poly)                 1.000  0.005  0.075  \n",
              "AE                                0.005  1.000  0.900  \n",
              "UMAP                              0.075  0.900  1.000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-191eb526-dc84-4d04-bc15-93ed523e0471\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No compression</th>\n",
              "      <th>PCA</th>\n",
              "      <th>kernel PCA (sigmoid)</th>\n",
              "      <th>kernel PCA (poly)</th>\n",
              "      <th>AE</th>\n",
              "      <th>UMAP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>No compression</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.777</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.874</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PCA</th>\n",
              "      <td>0.777</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.582</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kernel PCA (sigmoid)</th>\n",
              "      <td>0.900</td>\n",
              "      <td>0.582</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.679</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kernel PCA (poly)</th>\n",
              "      <td>0.874</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.679</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AE</th>\n",
              "      <td>0.139</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.005</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>UMAP</th>\n",
              "      <td>0.582</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.777</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.900</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-191eb526-dc84-4d04-bc15-93ed523e0471')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-191eb526-dc84-4d04-bc15-93ed523e0471 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-191eb526-dc84-4d04-bc15-93ed523e0471');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplication of the autoencoder-based and baseline models to testing data"
      ],
      "metadata": {
        "id": "Lj1Z-KrObrb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = pd.read_csv(f\"{path}/y_test_A_merged_CRC_BRCA.csv.tar.gz\", compression = \"gzip\", index_col=0)\n",
        "#y_test = y_test.reset_index(drop=True)\n",
        "#y_test.index = test_df.index\n",
        "y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdA7-uEMb1yW",
        "outputId": "1a98472d-8953-46a6-efff-98f785831c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2156, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(f\"{path}/X_test_A_merged_CRC_BRCA.csv.tar.gz\", compression = \"gzip\", index_col=0).T\n",
        "test_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlBF5ZMDcEsF",
        "outputId": "d1402246-01f0-4bfc-e983-cf0edb3932d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2156, 22596)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No compression"
      ],
      "metadata": {
        "id": "cyM88EmvJRJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all = []\n",
        "km = KMeans(n_clusters=10, random_state=42)\n",
        "y_pred = km.fit_predict(test_df)\n",
        "ari = adjusted_rand_score(y_pred, y_test[\"Label\"])\n",
        "\n",
        "silhouetteScore = silhouette_score(test_df, y_test[\"Label\"], metric=\"euclidean\")\n",
        "davies_bouldinScore = davies_bouldin_score(test_df, y_test[\"Label\"])\n",
        "\n",
        "print(f\"Silhuette: {round(silhouetteScore,3)}\", f\"ARI {round(ari,3)}\", f\"Davies-Bouldin {round(davies_bouldinScore,3)}\")\n",
        "\n",
        "all.append((round(silhouetteScore,3), round(ari,3), round(davies_bouldinScore,3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6m3OOtgHsS1",
        "outputId": "fa3cac52-4259-420b-91c0-dd21d24d4db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhuette: -0.077 ARI 0.175 Davies-Bouldin 4.727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PCA compression"
      ],
      "metadata": {
        "id": "1DMtT4RsJT2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2408).fit(train_df)\n",
        "X_test = pca.transform(test_df)"
      ],
      "metadata": {
        "id": "eDEOEwHPJbcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km = KMeans(n_clusters=10, random_state=42)\n",
        "y_pred = km.fit_predict(X_test)\n",
        "ari = adjusted_rand_score(y_pred, y_test[\"Label\"])\n",
        "\n",
        "silhouetteScore = silhouette_score(X_test, y_test[\"Label\"], metric=\"euclidean\")\n",
        "davies_bouldinScore = davies_bouldin_score(X_test, y_test[\"Label\"])\n",
        "\n",
        "print(f\"Silhuette: {round(silhouetteScore,3)}\", f\"ARI {round(ari,3)}\", f\"Davies-Bouldin {round(davies_bouldinScore,3)}\")\n",
        "\n",
        "all.append((round(silhouetteScore,3), round(ari,3), round(davies_bouldinScore,3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfSyXO63I27W",
        "outputId": "96861ca5-215f-4ae0-9b13-64e02dc42e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhuette: -0.087 ARI 0.177 Davies-Bouldin 4.388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### kPCA compression (poly)"
      ],
      "metadata": {
        "id": "h1wG0RMmJyxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = KernelPCA(n_components=2408, kernel=\"poly\").fit(train_df)\n",
        "X_test = pca.transform(test_df)"
      ],
      "metadata": {
        "id": "jQ_pmV-UKe-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km = KMeans(n_clusters=10, random_state=42)\n",
        "y_pred = km.fit_predict(X_test)\n",
        "ari = adjusted_rand_score(y_pred, y_test[\"Label\"])\n",
        "\n",
        "silhouetteScore = silhouette_score(X_test, y_test[\"Label\"], metric=\"euclidean\")\n",
        "davies_bouldinScore = davies_bouldin_score(X_test, y_test[\"Label\"])\n",
        "\n",
        "print(f\"Silhuette: {round(silhouetteScore,3)}\", f\"ARI {round(ari,3)}\", f\"Davies-Bouldin {round(davies_bouldinScore,3)}\")\n",
        "\n",
        "all.append((round(silhouetteScore,3), round(ari,3), round(davies_bouldinScore,3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8lDtR-vKfIi",
        "outputId": "e134d934-4ee0-4dca-efd7-4481cb30330f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhuette: -0.08 ARI 0.176 Davies-Bouldin 4.237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### kPCA compression (sigmoid)"
      ],
      "metadata": {
        "id": "Iwj6ZpBCKzmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = KernelPCA(n_components=2408, kernel=\"sigmoid\").fit(train_df)\n",
        "X_test = pca.transform(test_df)"
      ],
      "metadata": {
        "id": "FlUTDPvFK1j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km = KMeans(n_clusters=10, random_state=42)\n",
        "y_pred = km.fit_predict(X_test)\n",
        "ari = adjusted_rand_score(y_pred, y_test[\"Label\"])\n",
        "\n",
        "silhouetteScore = silhouette_score(X_test, y_test[\"Label\"], metric=\"euclidean\")\n",
        "davies_bouldinScore = davies_bouldin_score(X_test, y_test[\"Label\"])\n",
        "\n",
        "print(f\"Silhuette: {round(silhouetteScore,3)}\", f\"ARI {round(ari,3)}\", f\"Davies-Bouldin {round(davies_bouldinScore,3)}\")\n",
        "\n",
        "all.append((round(silhouetteScore,3), round(ari,3), round(davies_bouldinScore,3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sphuZzmK104",
        "outputId": "b2c87bf2-4630-44cf-811a-43503b9326c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhuette: -0.087 ARI 0.171 Davies-Bouldin 4.415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UMAP compression"
      ],
      "metadata": {
        "id": "nY3Dfx1OOG7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ump = UMAP(n_components=2408, min_dist=0.1, n_neighbors=150).fit(train_df)\n",
        "X_test = ump.transform(X_test)"
      ],
      "metadata": {
        "id": "mukI2Y4rOF2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km = KMeans(n_clusters=10, random_state=42)\n",
        "y_pred = km.fit_predict(X_test)\n",
        "ari = adjusted_rand_score(y_pred, y_test[\"Label\"])\n",
        "\n",
        "silhouetteScore = silhouette_score(X_test, y_test[\"Label\"], metric=\"euclidean\")\n",
        "davies_bouldinScore = davies_bouldin_score(X_test, y_test[\"Label\"])\n",
        "\n",
        "print(f\"Silhuette: {round(silhouetteScore,3)}\", f\"ARI {round(ari,3)}\", f\"Davies-Bouldin {round(davies_bouldinScore,3)}\")\n",
        "\n",
        "all.append((round(silhouetteScore,3), round(ari,3), round(davies_bouldinScore,3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63urkkHROMai",
        "outputId": "3853d091-6fcc-4c8c-810b-306605a2e40f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhuette: -0.0689999982714653 ARI -0.001 Davies-Bouldin 24.143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compr_df = pd.DataFrame.from_records(all, columns=[\"Silhouette\", \"ARI\", \"DB score\"])\n",
        "compr_df.index = [\"No compression\", \"PCA\", \"kPCA (poly)\", \"kPCA (sigmoid)\", \"UMAP\"]\n",
        "compr_df.to_csv(f\"{path}/testing_data_clustering_metrics.csv\", index=True)\n",
        "compr_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "cul8IarLe7eO",
        "outputId": "3853fa65-963d-41c5-9c9e-aa07890f7e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                Silhouette    ARI  DB score\n",
              "No compression      -0.077  0.175     4.727\n",
              "PCA                 -0.087  0.177     4.388\n",
              "kPCA (poly)         -0.080  0.176     4.237\n",
              "kPCA (sigmoid)      -0.087  0.171     4.415\n",
              "UMAP                -0.069 -0.001    24.143"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Silhouette</th>\n",
              "      <th>ARI</th>\n",
              "      <th>DB score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>No compression</th>\n",
              "      <td>-0.077</td>\n",
              "      <td>0.175</td>\n",
              "      <td>4.727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PCA</th>\n",
              "      <td>-0.087</td>\n",
              "      <td>0.177</td>\n",
              "      <td>4.388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kPCA (poly)</th>\n",
              "      <td>-0.080</td>\n",
              "      <td>0.176</td>\n",
              "      <td>4.237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kPCA (sigmoid)</th>\n",
              "      <td>-0.087</td>\n",
              "      <td>0.171</td>\n",
              "      <td>4.415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>UMAP</th>\n",
              "      <td>-0.069</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>24.143</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AE compression"
      ],
      "metadata": {
        "id": "Ile6t-_4ONnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Load the encoder model\n",
        "loaded_encoder = load_model(f'{path}/ae_encoder.h5', compile=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny4sBQ0ooKTC",
        "outputId": "74c93676-d878-47b0-cd9f-a247a753f4ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-17 21:31:16.432969: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/ngs/ngs-sdk/lib64:/usr/local/ncbi/ncbi-vdb/lib64:\n",
            "2023-04-17 21:31:16.433005: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2023-04-17 21:31:16.433030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (koios): /proc/driver/nvidia/version does not exist\n",
            "2023-04-17 21:31:16.457859: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=4557).fit(train_df)\n",
        "X_test = pca.transform(test_df)"
      ],
      "metadata": {
        "id": "R9lP2kZxCx8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_pred = loaded_encoder.predict(X_test, batch_size=1024)\n",
        "  \n",
        "km = KMeans(n_clusters=10, random_state=42)\n",
        "y_pred = km.fit_predict(X_val_pred)\n",
        "ari = adjusted_rand_score(y_pred, y_test[\"Label\"])\n",
        "\n",
        "silhouetteScore = silhouette_score(X_val_pred, y_test[\"Label\"], metric=\"euclidean\")\n",
        "davies_bouldinScore = davies_bouldin_score(X_val_pred, y_test[\"Label\"])\n",
        "\n",
        "print(f\"Silhuette: {round(silhouetteScore,3)}\", f\"ARI {round(ari,3)}\", f\"Davies-Bouldin {round(davies_bouldinScore,3)}\")\n",
        "\n",
        "#all.append((round(silhouetteScore,3), round(ari,3), round(davies_bouldinScore,3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb27tONpcJiJ",
        "outputId": "1fedf671-5bc4-4ebf-bf88-0e2aa23c9e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 11ms/step\n",
            "Silhuette: -0.039000000804662704 ARI 0.2 Davies-Bouldin 3.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Session info"
      ],
      "metadata": {
        "id": "hRbN1Uk0uabC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import sklearn\n",
        "import google.colab\n",
        "\n",
        "print(umap.__version__)\n",
        "print(sp.__version__)\n",
        "print(\"Pandas\", pd.__version__)\n",
        "print(\"Numpy\", np.__version__)\n",
        "print(\"Matplotlib\", matplotlib.__version__)\n",
        "print(\"Seaborn\", sns.__version__)\n",
        "print(\"colab\", google.colab.__version__)\n",
        "print(\"sklearn\", sklearn.__version__)"
      ],
      "metadata": {
        "id": "cHj8SPGbuZxh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN/7ikrIkuGN0KpWl6L0HsQ",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}